{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6c38b",
   "metadata": {},
   "source": [
    "### 1. TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35d5c2",
   "metadata": {},
   "source": [
    "#### _1.1 Load text file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bbc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab034f1",
   "metadata": {},
   "source": [
    "#### _1.2 RE tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c072e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', '', ' ', 'what', \"'\", 's', ' ', 'good', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample = \"Hey, what's good?\"\n",
    "result = re.split(r'([,.:;?!\"()\\'/]|--|\\s)', sample)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d13985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'what', \"'\", 's', 'good', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # retunrs false for whitespaces / no spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138869",
   "metadata": {},
   "source": [
    "Now, apply RE tokenizer to main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5277b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4654\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([.,:;?!\"()\\'/]|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.split()]\n",
    "\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51066469",
   "metadata": {},
   "source": [
    "#### _1.3 Token ID creation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 1139\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"Length of vocabulary: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6806f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for t, i in vocab.items():\n",
    "    print(t, i)\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41849",
   "metadata": {},
   "source": [
    "#### _1.4 Tokenizer class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "s2i = tokenizer.encode(\"I HAD always thought.\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15042de9",
   "metadata": {},
   "source": [
    "#### _1.5 Special Context Tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5422a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary with special context tokens: 1141\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(f\"Length of vocabulary with special context tokens: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8ca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for k, v in enumerate(vocab.keys()):\n",
    "    keys.append(v)\n",
    "print(keys[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c6a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        # if item not in vocab, replace it with <|unk|> token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        preprocessed.append(\"<|endoftext|>\")\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b72c372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, how are you doing? <|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "s2i = tokenizer.encode(\"Hello, how are you doing?\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fbc60",
   "metadata": {},
   "source": [
    "#### _1.6 Byte Pair Encoding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87e9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84994a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2bf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 13778, 2763, 13, 220, 50256, 10928, 345, 588, 257, 6508, 1659, 660, 64, 30]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, Ilham. <|endoftext|> Would you like a cupoftea?\"\n",
    "integers = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b08a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Ilham. <|endoftext|> Would you like a cupoftea?\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe5f90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151db66",
   "metadata": {},
   "source": [
    "### 2. INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826950f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens from byte pair encoding: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens from byte pair encoding:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b564426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [40, 367, 2885, 1464, 1807]\n",
      "y:     [367, 2885, 1464, 1807, 3619]\n"
     ]
    }
   ],
   "source": [
    "context_size = 5 # input will have 5 tokens\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size + 1]\n",
    "print(f\"X: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991f59e",
   "metadata": {},
   "source": [
    "#### _2.1 Using Dataloader_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c2795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride): # max_len is context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - max_len, stride):\n",
    "            input_chunk = token_ids[i:i + max_len]\n",
    "            target_chunk = token_ids[i + 1:i + max_len + 1]\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "    \n",
    "    # the below 2 methods is required for Dataloader to be used\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # we are basically saying that if the input is the 50th tensor, then the output is the 50th tensor\n",
    "        return (\n",
    "            torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            torch.tensor(self.target_ids[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291c8d7",
   "metadata": {},
   "source": [
    "The idea is to form something like as follows:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[5, 6, 7, 8],<br>\n",
    "[9, 10, 11, 12]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[6, 7, 8, 9],<br>\n",
    "[10, 11, 12, 13]]<br><br>\n",
    "...where the first matrix is X and the second matrix is y. Note that in the above example, the stride as well as the max length is 4. If the stride was 2:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[3, 4, 5, 6],<br>\n",
    "[5, 6, 7, 8]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[4, 5, 6, 7],<br>\n",
    "[6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b7d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    # drop last if last tensor is shorter than max_len\n",
    "    # batch size is the number of training ip-op data pairs to be used for training by whcih the parameters are updated\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91f4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464, 12556,  4062,   602]]), tensor([[12556,  4062,   602,   290]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_len = 4, stride = 1, shuffle = False) # looking into how the function will work\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00fb31",
   "metadata": {},
   "source": [
    "Using a batch size of 1 is not preferred as this leads to noisy updates, even though good for memory.<br>\n",
    "Note that a higher overlap (lower stride) can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147d962",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51c802",
   "metadata": {},
   "source": [
    "### 3. VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60811e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3 # embedding dimention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # intialize mebedding matrix randomly\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2974b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e240f8",
   "metadata": {},
   "source": [
    "The embedding weight matrix is basically used for lookup operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f791e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids)) # looking up vector embeddings for the sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb66cb",
   "metadata": {},
   "source": [
    "Note that this is essencially a one hot encoded represenation of the input IDs passed into a linear layer to get the output embeddings where the weights of the neural net are randomly initialized. But we dom't use this because it's not efficient due to the sparsity of the one hot encoded input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample two\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f16f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size = 8, max_len = max_len,\n",
    "    stride = max_len, shuffle = False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ee1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  464, 12556,  4062,   602],\n",
      "        [  290, 31421, 15120,   198],\n",
      "        [  198,  2215,   530,  3073],\n",
      "        [  379,   262, 22942,   286],\n",
      "        [19773,  1081,   361, 22982],\n",
      "        [24411,    11,   340,  4329],\n",
      "        [ 1598,   326,   465,  3108],\n",
      "        [  656,   262,   995,   286]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a3873",
   "metadata": {},
   "source": [
    "This is basically a batch of 8 with 4 tokens each, and each token is converted to a vector of dimention 256. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b54ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967bad5",
   "metadata": {},
   "source": [
    "### 4. POSITIONAL EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16957c",
   "metadata": {},
   "source": [
    "Now we create positional embedding the same way as we did for the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80d41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_len = max_len # 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_len, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_len)) # arange creates ids from 0 to max_len - 1 and pos_embedding_layer converts them to embedding matrix where each row corresponds to the positional embedding for that position id\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36c780",
   "metadata": {},
   "source": [
    "So the first vector embedding of a 4 token sentence will always be added by the vector [1.7375, -0.5620, ..., 1.0345]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9485424",
   "metadata": {},
   "source": [
    "It must also be noted that each row will have the same set of positional embedding values. In other words, the PE value repeats for each row. So the final embedding matrix will only be 4x256 and not 8x4x256. We only care about the position in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a685f7",
   "metadata": {},
   "source": [
    "We can directly add the token and position embeddings, even though the dimentions don't match exactly via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83f6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22a91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8b1b2",
   "metadata": {},
   "source": [
    "### 5. SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99401b4a",
   "metadata": {},
   "source": [
    "We know that attention scores are calculated by taking the dot product between the query token and all the other input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c4b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # let the query token be journey\n",
    "\n",
    "attention_scores_x_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_x_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attention_scores_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b35b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# now normalize the scores\n",
    "\n",
    "attention_weights_x_2 = attention_scores_x_2 / attention_scores_x_2.sum()\n",
    "print(attention_weights_x_2)\n",
    "print(attention_weights_x_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f000ce",
   "metadata": {},
   "source": [
    "Note that attention __scores__ are not normalized, but attention __weights__ are, and they sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff9c1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# softmax normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
    "\n",
    "attention_weights_x_2_naive_sm = softmax_naive(attention_scores_x_2) # sm: softmax\n",
    "print(attention_weights_x_2_naive_sm)\n",
    "print(attention_weights_x_2_naive_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6e463",
   "metadata": {},
   "source": [
    "PyTorch implementation of Softmax is preffered to control instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433ce8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# pytorch softmax operation\n",
    "\n",
    "attention_weights_x_2_pt_sm = torch.softmax(attention_scores_x_2, dim = 0) # pt: pytorch\n",
    "print(attention_weights_x_2_pt_sm)\n",
    "print(attention_weights_x_2_pt_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8713a1",
   "metadata": {},
   "source": [
    "#### _5.1 Context vector calculation for 'journey'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95e352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "context_vector_x2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_x2 += attention_weights_x_2_pt_sm[i] * x_i\n",
    "\n",
    "print(context_vector_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af65b2",
   "metadata": {},
   "source": [
    "#### _5.2 Calculate attention matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09b7c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44d1f3",
   "metadata": {},
   "source": [
    "This can be done using 2 for loops but that's computationally very expensive. Rather, we can do the above transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd949977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim = -1) \n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080d6e",
   "metadata": {},
   "source": [
    "Setting dimention to -1 means it will normalize accross the columns. This is because the matrix dimention is n_row x n_col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53a8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vectors calculation (z_i)\n",
    "\n",
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb33fa7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a780a1",
   "metadata": {},
   "source": [
    "### 6. SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27e2b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb2fa1",
   "metadata": {},
   "source": [
    "Now we randomly initialize W_q, W_k & W_v. Each of them will have dimentiones were the number of row count will be eqaul to the input vector dimention (column count of input matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f666e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be working with the sample word 'journey' again\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # this will be the number of columns in the key, quey and value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c259d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# set requires_grad to True later for model training\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "print(W_q)\n",
    "print(W_k)\n",
    "print(W_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff8bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the query, key and value for the sample input word 'journey'\n",
    "\n",
    "q_2 = x_2 @ W_q\n",
    "k_2 = x_2 @ W_k\n",
    "v_2 = x_2 @ W_v\n",
    "\n",
    "print(q_2)\n",
    "print(k_2)\n",
    "print(v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d166cc5",
   "metadata": {},
   "source": [
    "Note that conventionally, just like how things were implemented in section 5, the output from these dot product operations must have the same dimention as the input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e18afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]])\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "# get the overall query, key and value\n",
    "\n",
    "query = inputs @ W_q\n",
    "key = inputs @ W_k\n",
    "value = inputs @ W_v\n",
    "\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38158409",
   "metadata": {},
   "source": [
    "Now we compute the attention scores. In self attention, this is essencially the dot product between the query and the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2b0bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# for 'journey':\n",
    "\n",
    "query_2 = query[1]\n",
    "key_2 = key[1]\n",
    "\n",
    "attention_scores_2 = query_2 @ key.T\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1ce3e",
   "metadata": {},
   "source": [
    "This is basically saying how much the word __journey__ attends to all the other words. Obviously, this will be highest for the second word (itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7406e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "# overall attention\n",
    "\n",
    "attention_scores = query @ key.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ea712",
   "metadata": {},
   "source": [
    "For now, these don't mean anything because they are not trained. Next, we normalize these scores. We normalize by first scaling the scores by square root of d_out or embedding dimention of each word of the key matrix (number of columns). Next, we apply softmax over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77eeecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# normalize to get attention weights (this is just for 'journey')\n",
    "\n",
    "d_k = key.shape[1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61831e12",
   "metadata": {},
   "source": [
    "Why take square root? Multiply any 2 numbers (here, we are multiplying the key and query) increases the variance. So to stabilize it back, we take the root. Another reason is for bringing stability to the softmax outputs and to have an even distribution. If not, the scores can get overly confident for a single input word. (Refer lecture 15, 46th minute for more detail).<br><br>\n",
    "This is why self attention is also called __sclaed dot product attention__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae49dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "# get attention weights for enitre sentence\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42196d7a",
   "metadata": {},
   "source": [
    "These attentions weights are now multiplied with the _value_ matrix to get the __context vectors__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4251f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "context = attention_weights @ value\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068759",
   "metadata": {},
   "source": [
    "Now we make a self attention calss for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d68f1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_k\n",
    "        queries = x @ self.W_q\n",
    "        values = x @ self.W_v\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80c39c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "378ea687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, which is more optimized due to the Linear class from PyTorch\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    \n",
    "    # change here compared to v1\n",
    "    def forward(self, x): \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c61440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29b258",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec3580",
   "metadata": {},
   "source": [
    "### 7. CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8e047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55681081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f16ae",
   "metadata": {},
   "source": [
    "Now we apply causal masking using tril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "001d9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_len = attention_weights.shape[0]\n",
    "mask_sample = torch.tril(torch.ones(context_len, context_len))\n",
    "print(mask_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d705d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attention = attention_weights * mask_sample\n",
    "print(masked_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788ce1",
   "metadata": {},
   "source": [
    "Now perform renormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bfb33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_attention.sum(dim = 1, keepdim = True)\n",
    "masked_attention_norm = masked_attention / row_sums\n",
    "print(masked_attention_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886285d",
   "metadata": {},
   "source": [
    "However, there is a problem with this method. Even though we have performed masking, the attention scores after applying softmax leads to data leakag due to data redistribution which occurs based on future values as well. A simple solution is to perform masking over the attention scores, and then perform softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9660fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf) # this basically takes the attention scores matrix, looks at positions where the value is True, and gves it -ve inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61290adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aae77",
   "metadata": {},
   "source": [
    "_(not exactly correct it seems)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412bba0",
   "metadata": {},
   "source": [
    "Dropout is applied here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d13e1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f273f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cba8",
   "metadata": {},
   "source": [
    "Rescaling based on droupout percentage also occurs.<br>\n",
    "Next, we also introduce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c2196b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch)\n",
    "print()\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72632a48",
   "metadata": {},
   "source": [
    "Think of it as 2 input matrices. One sentence can be \"your journey starts with one step\" and the other can be \"my name is mohammed asif sahadh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1eca4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # new\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal = 1)) # new (register buffer i think ensures that the non trainable stuff will be moved to appropriate device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # new batch dim b\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # one coma two because we don't need to transpose the batch dimention (idx 0)\n",
    "        attention_scores.masked_fill_( # _ ops makes it inplace\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # :num_tokens is to ensure if the sequence is less than the context length\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights) # new      \n",
    " \n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "422bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_len, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print()\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f200a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54ab2",
   "metadata": {},
   "source": [
    "### 8. MULTI HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1552de7",
   "metadata": {},
   "source": [
    "For MHA, we simply have to create a wrapper for causal attention that stacks the outputs of multiple of thier outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eed8cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( # creates an instance of causal attention class\n",
    "            [CausalAttention(d_in, d_out, context_len, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f655078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "391277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0cbd3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = inputs.shape[0]\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_len, 0.0, 3)\n",
    "mha.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b17cb",
   "metadata": {},
   "source": [
    "_(output matches here though)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b194d7",
   "metadata": {},
   "source": [
    "To solve the inefficiency casued by performing matrix multiplations over multiple head, we implement weight splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e319612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_out = x.shape # s1\n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # s5\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights) # s8\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2) # s9 & s10\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # s11\n",
    "        context_vec = self.out_proj(context_vec) # optional\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a43c13",
   "metadata": {},
   "source": [
    "s1 to s11 are steps to implement MHA, in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87274ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]],\n",
      "\n",
      "        [[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.90, 0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)\n",
    "\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02e987",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39e8b5",
   "metadata": {},
   "source": [
    "### 9. IMPLEMENTING GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c5f2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2425",
   "metadata": {},
   "source": [
    "#### _9.1 Dummy class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f92aa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # this is the entire lookup matrix to get the embedding for a token\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # this depends on context length of course (also a lookup table)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # make a placeholder for transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )        \n",
    "        \n",
    "        # make a placeholder for layernorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx): # in_idx is a batch of input tokens\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device)) # arange creates ids from 0 to max_len - 1 \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) # lots of things happen here\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af49492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token embeddings:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Logits:\n",
      "tensor([[[-5.4645e-03, -7.1054e-01, -1.0149e+00,  ..., -4.1823e-01,\n",
      "           2.2412e-02, -1.0233e+00],\n",
      "         [ 1.7904e+00, -3.2025e-03, -1.1346e-01,  ..., -6.4449e-01,\n",
      "           6.3170e-01,  3.2810e+00],\n",
      "         [ 4.1122e-01, -2.4297e-01, -3.9615e-01,  ...,  3.1840e-01,\n",
      "           5.1082e-01, -7.7559e-02],\n",
      "         [-8.1073e-01, -4.5008e-01, -8.1223e-01,  ...,  1.2990e+00,\n",
      "           3.7404e-01,  9.0462e-02]],\n",
      "\n",
      "        [[ 2.6268e-01, -7.7104e-01, -1.4651e+00,  ..., -6.4443e-01,\n",
      "          -4.4353e-01, -9.2719e-01],\n",
      "         [ 1.1406e+00, -3.4269e-01, -6.9491e-01,  ..., -1.7102e-01,\n",
      "           6.8366e-01,  2.0607e+00],\n",
      "         [ 1.1106e+00,  1.1755e+00, -6.2576e-01,  ..., -4.4802e-01,\n",
      "           2.6767e-02, -9.2523e-01],\n",
      "         [-3.8614e-01,  3.0415e-01, -9.1265e-01,  ...,  1.8638e+00,\n",
      "           6.4222e-01,  2.9281e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dummy sample\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim = 0)\n",
    "print(\"Input token embeddings:\")\n",
    "print(batch)\n",
    "print()\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Logits:\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39f60",
   "metadata": {},
   "source": [
    "#### _9.2 Layernorm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f850829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[-1.4771,  0.3162, -0.1837, -0.8028, -1.2379],\n",
      "        [-1.2232,  0.3065, -0.4733, -0.1332, -0.5370]])\n",
      "\n",
      "Output:\n",
      "tensor([[0.0000, 0.2986, 0.2986, 0.5947, 1.2578, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3003, 0.4595, 0.9096, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple feedforward\n",
    "batch_sample = torch.randn(2, 5)\n",
    "print(\"Input:\")\n",
    "print(batch_sample)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_sample)\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "181ab561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4083],\n",
      "        [0.2782]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.2228],\n",
      "        [0.1328]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim = -1, keepdim = True)\n",
    "var = out.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e1ffc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8650, -0.2323, -0.2324,  0.3950,  1.7998, -0.8650],\n",
      "        [-0.7634, -0.7634,  0.0605,  0.4974,  1.7322, -0.7634]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layernorm = (out - mean) / (var ** 0.5)\n",
    "print(layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c6e7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4769e-08],\n",
      "        [ 4.4703e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = layernorm.mean(dim = -1, keepdim = True)\n",
    "var = layernorm.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a38ab2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "011ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2219e-07],\n",
      "        [-9.5367e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_sample)\n",
    "mean = out_ln.mean(dim = -1, keepdim = True)\n",
    "var = out_ln.var(dim = -1, keepdim = True, unbiased = False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f45fe",
   "metadata": {},
   "source": [
    "#### _9.3 GELU_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d37162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a23c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b0ce1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afb9f8",
   "metadata": {},
   "source": [
    "#### _9.4 Skip Connections_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30c63ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut): \n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), \n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x) # output of linear layer x\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87eab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_wo_shortut = ExampleDNN(layer_sizes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59b36e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grads(model, x):\n",
    "    output = model(x) # normal output from nn\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "794b7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_wo_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cace1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_w_shortut = ExampleDNN(layer_sizes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1291623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_w_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def0a06",
   "metadata": {},
   "source": [
    "Gradient vanishing reduced..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a05c5",
   "metadata": {},
   "source": [
    "#### _9.5 Coding Attention & Linear Layers in a Transformer Block_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5cf83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733201",
   "metadata": {},
   "source": [
    "Requirements for building the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50ef554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ee8ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # MHA\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # shape: [batch size, num tokens, emb size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        # FCL\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cadd4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[0.2961, 0.5166, 0.2517,  ..., 0.9541, 0.8567, 0.4604],\n",
      "         [0.2238, 0.3047, 0.3019,  ..., 0.5465, 0.4532, 0.7598],\n",
      "         [0.6945, 0.2478, 0.4111,  ..., 0.8838, 0.4898, 0.5963],\n",
      "         [0.0890, 0.7804, 0.9223,  ..., 0.4507, 0.6357, 0.5833]],\n",
      "\n",
      "        [[0.5716, 0.9297, 0.3396,  ..., 0.0477, 0.4564, 0.2797],\n",
      "         [0.0936, 0.2211, 0.3806,  ..., 0.3948, 0.4545, 0.4536],\n",
      "         [0.6788, 0.1741, 0.2084,  ..., 0.5557, 0.5930, 0.0959],\n",
      "         [0.3894, 0.4083, 0.0662,  ..., 0.9861, 0.9341, 0.1319]]])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Output:\n",
      "tensor([[[-0.0055,  0.0972, -0.1122,  ...,  1.2889,  0.2623,  0.6685],\n",
      "         [ 0.0023, -0.2369,  0.1720,  ...,  0.5952,  0.2497,  0.7447],\n",
      "         [ 0.4673,  0.4472,  0.1791,  ...,  1.2525,  0.3045,  0.7750],\n",
      "         [ 0.0662,  0.7224,  0.9206,  ...,  0.4790,  0.7428,  0.7015]],\n",
      "\n",
      "        [[ 0.3622,  1.2144,  0.5221,  ...,  0.1854,  0.0111, -0.5034],\n",
      "         [-0.0225,  0.7789,  0.2770,  ...,  0.1734,  0.5419,  0.1143],\n",
      "         [ 0.7425,  0.4013,  0.3211,  ...,  0.3268,  0.7523, -0.1642],\n",
      "         [ 0.5745,  0.6241,  0.4410,  ...,  1.1963,  1.2650,  0.2243]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24eb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896df3d9",
   "metadata": {},
   "source": [
    "### 10. CODING GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "548d0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b2055",
   "metadata": {},
   "source": [
    "#### _10.1 The GPT Class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74dea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_len\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx): # input batch\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33845b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output batch:\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\")\n",
    "print(batch)\n",
    "print(\"Output batch:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71656dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e817112",
   "metadata": {},
   "source": [
    "_Note: We are not using weight tying, which was performed in the original GPT-2 model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c9e73",
   "metadata": {},
   "source": [
    "#### _10.2 Generate Text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9f72ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx is the input batch\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop current context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # get predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # batch_size x tokens_num x vocab_size\n",
    "        # get the last time step (last set of logits)\n",
    "        logits = logits[:, -1, :]\n",
    "        # apply softmax\n",
    "        probs = torch.softmax(logits, dim = -1)\n",
    "        # get id of max\n",
    "        idx_next = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "        # append id to running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c37518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 314, 716, 407, 1016, 284, 4483, 11311]\n",
      "Encoded tensor: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311]])\n",
      "Encoded tensor shape: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am not going to eat chocolate\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded tensor: {encoded_tensor}\")\n",
    "print(f\"Encoded tensor shape: {encoded_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50371a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311, 20656,\n",
      "         30719, 44035, 23338, 24151, 10835, 42731, 35799, 46215,   371]])\n",
      "Output shape: 19\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Output shape: {len(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9b986d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am not going to eat chocolate AmbassadorOptional touredنGirl malesGradットorea R\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4bf9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68ec82",
   "metadata": {},
   "source": [
    "### 11. The LLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17e66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 512, # can be reduced for training simplicity\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88e7cef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d000af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "Every effort moves youublishedstein municipal sushi abnormal Payment contemporthsα politely\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "\n",
    "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41ebb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feet wet\n",
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\"]\n",
    "                       [40, 1107, 588]])     # [\"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],   # [\" effort moves you\"]\n",
    "                        [1107, 588, 11311]]) # [\" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50fea205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[2.9209e-05, 1.0527e-05, 1.6121e-05,  ..., 2.6603e-05,\n",
      "          2.6384e-05, 4.3995e-05],\n",
      "         [2.6438e-05, 9.8570e-06, 1.3430e-05,  ..., 7.6308e-06,\n",
      "          1.2208e-05, 4.2206e-05],\n",
      "         [2.0652e-05, 3.0963e-05, 7.3546e-05,  ..., 3.5792e-05,\n",
      "          1.5129e-05, 1.7849e-05]],\n",
      "\n",
      "        [[9.4693e-06, 2.0345e-05, 1.8425e-05,  ..., 8.6429e-06,\n",
      "          1.5247e-05, 3.9934e-05],\n",
      "         [2.3645e-05, 8.8373e-06, 2.1052e-05,  ..., 7.2422e-06,\n",
      "          1.4043e-05, 1.4942e-05],\n",
      "         [1.0545e-05, 1.4763e-05, 2.7852e-05,  ..., 2.5073e-05,\n",
      "          1.3072e-05, 3.0611e-05]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim = -1)\n",
    "print(probs.shape)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e751eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[40822],\n",
      "         [27614],\n",
      "         [20921]],\n",
      "\n",
      "        [[ 8807],\n",
      "         [34094],\n",
      "         [45958]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7781e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  effort moves you\n",
      "Predicted: ovychrequisite Buddh\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Predicted: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe710892",
   "metadata": {},
   "source": [
    "#### _11.1 Cross Entropy Loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2790cdf",
   "metadata": {},
   "source": [
    "Basically find the target probabilities of the target ids. We need to get these as close to 1 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a599343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3003e-05, 1.7775e-05, 1.4705e-05])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e579b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1463e-05, 1.5585e-05, 8.4811e-06])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5a962f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.6723, -10.9377, -11.1273, -11.3764, -11.0692, -11.6777])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be67a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.9768)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e02985cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9768)\n"
     ]
    }
   ],
   "source": [
    "neg_log_likelihood = -1 * avg_log_probs\n",
    "print(neg_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6eaf5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler method\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "# above, we have merged the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6aab6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9768)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91714488",
   "metadata": {},
   "source": [
    "#### _11.2 Perplexity_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cea0b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58498.9844)\n"
     ]
    }
   ],
   "source": [
    "perp = torch.exp(loss)\n",
    "print(perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ece60",
   "metadata": {},
   "source": [
    "That's horrible...for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d066a05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dec17",
   "metadata": {},
   "source": [
    "### 12. INITIAL ERROR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c34ddffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 443230\n",
      "The Early Foundations and Academic Journey\n",
      "\n",
      "When one looks at the trajectory of Mohammed Asif Sahad\n"
     ]
    }
   ],
   "source": [
    "with open(\"me.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b38052cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 88270\n"
     ]
    }
   ],
   "source": [
    "total_tokens = len(tokenizer.encode(raw_text))\n",
    "print(f\"Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4f4810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = True,\n",
    "    shuffle = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "865bda38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens * (1 - train_ratio) < GPT_CONFIG_124M[\"context_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c291830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "\n",
      "Test Loader:\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([1, 512]) torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print()\n",
    "print(\"Test Loader:\")\n",
    "for x, y in test_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b12ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # this does all the softmax & everything\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None): # this will show the loss of the LM\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches # mean loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "081e60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cac25d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.976940873381379\n",
      "Testing loss: 10.980136553446451\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): # disable for now\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Testing loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eff95",
   "metadata": {},
   "source": [
    "### 13. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0fdb1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function\n",
    "def evaluate_model(model, train_loader, test_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches = eval_iter)\n",
    "        test_loss = calc_loss_loader(test_loader, model, device, num_batches = eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ed39a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see text generation during training\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model, idx = encoded, max_new_tokens = 50, context_size = context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    print()\n",
    "    model.train() # set it back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2aa2c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, test_loader, optimizer, device, \n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients from previous batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel() # return number of tokens seen\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation (optional)\n",
    "            if global_step % eval_freq == 0: # only after a set of batches is used for training\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_loader, test_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch + 1} (Step {global_step:04d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Test loss {test_loss:.3f}\")\n",
    "            \n",
    "        # print sample text from each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        \n",
    "    return train_losses, test_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db83d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 0000): Train loss 9.666, Test loss 9.692\n",
      "Epoch 1 (Step 0005): Train loss 7.817, Test loss 7.801\n",
      "Epoch 1 (Step 0010): Train loss 6.447, Test loss 6.432\n",
      "Epoch 1 (Step 0015): Train loss 6.032, Test loss 5.993\n",
      "Epoch 1 (Step 0020): Train loss 5.756, Test loss 5.706\n",
      "Epoch 1 (Step 0025): Train loss 5.619, Test loss 5.566\n",
      "Epoch 1 (Step 0030): Train loss 5.246, Test loss 5.242\n",
      "Epoch 1 (Step 0035): Train loss 5.055, Test loss 4.982\n",
      "Epoch 1 (Step 0040): Train loss 4.544, Test loss 4.642\n",
      "Epoch 1 (Step 0045): Train loss 4.518, Test loss 4.387\n",
      "Epoch 1 (Step 0050): Train loss 4.146, Test loss 4.116\n",
      "Epoch 1 (Step 0055): Train loss 3.922, Test loss 3.857\n",
      "Epoch 1 (Step 0060): Train loss 3.582, Test loss 3.610\n",
      "Epoch 1 (Step 0065): Train loss 3.137, Test loss 3.423\n",
      "Epoch 1 (Step 0070): Train loss 2.944, Test loss 3.210\n",
      "Epoch 1 (Step 0075): Train loss 2.821, Test loss 3.004\n",
      "Asif likes to the gym is not only participating in the gym, and the gym, and the gym, and the gym is not only of his professional.                      \n",
      "\n",
      "Epoch 2 (Step 0080): Train loss 2.612, Test loss 2.832\n",
      "Epoch 2 (Step 0085): Train loss 2.537, Test loss 2.671\n",
      "Epoch 2 (Step 0090): Train loss 2.351, Test loss 2.551\n",
      "Epoch 2 (Step 0095): Train loss 2.282, Test loss 2.469\n",
      "Epoch 2 (Step 0100): Train loss 2.302, Test loss 2.385\n",
      "Epoch 2 (Step 0105): Train loss 1.916, Test loss 2.338\n",
      "Epoch 2 (Step 0110): Train loss 1.981, Test loss 2.257\n",
      "Epoch 2 (Step 0115): Train loss 1.899, Test loss 2.218\n",
      "Epoch 2 (Step 0120): Train loss 1.940, Test loss 2.183\n",
      "Epoch 2 (Step 0125): Train loss 1.934, Test loss 2.131\n",
      "Epoch 2 (Step 0130): Train loss 1.790, Test loss 2.092\n",
      "Epoch 2 (Step 0135): Train loss 1.857, Test loss 2.106\n",
      "Epoch 2 (Step 0140): Train loss 1.616, Test loss 2.062\n",
      "Epoch 2 (Step 0145): Train loss 1.662, Test loss 2.087\n",
      "Epoch 2 (Step 0150): Train loss 1.880, Test loss 2.148\n",
      "Asif likes to the nuanced differences in the nuanced trade-offs between automation with Cybeorg Education Technology Pvt. This cross-in-generated applications. He has consistently demonstrated his ability to the Machine Learning Team Co-in-Language Models (VLMs) for\n",
      "\n",
      "Epoch 3 (Step 0155): Train loss 1.553, Test loss 2.143\n",
      "Epoch 3 (Step 0160): Train loss 1.552, Test loss 2.114\n",
      "Epoch 3 (Step 0165): Train loss 1.481, Test loss 2.070\n",
      "Epoch 3 (Step 0170): Train loss 1.416, Test loss 2.043\n",
      "Epoch 3 (Step 0175): Train loss 1.351, Test loss 2.079\n",
      "Epoch 3 (Step 0180): Train loss 1.303, Test loss 2.128\n",
      "Epoch 3 (Step 0185): Train loss 1.292, Test loss 2.133\n",
      "Epoch 3 (Step 0190): Train loss 1.204, Test loss 2.123\n",
      "Epoch 3 (Step 0195): Train loss 1.108, Test loss 2.123\n",
      "Epoch 3 (Step 0200): Train loss 1.015, Test loss 2.137\n",
      "Epoch 3 (Step 0205): Train loss 1.154, Test loss 2.175\n",
      "Epoch 3 (Step 0210): Train loss 0.950, Test loss 2.211\n",
      "Epoch 3 (Step 0215): Train loss 0.871, Test loss 2.237\n",
      "Epoch 3 (Step 0220): Train loss 0.930, Test loss 2.227\n",
      "Epoch 3 (Step 0225): Train loss 0.791, Test loss 2.206\n",
      "Epoch 3 (Step 0230): Train loss 0.713, Test loss 2.235\n",
      "Asif likes to a more well-hour hackathon, Noa AI emerged as an agentic, he has consistently built it to the classroom.  The technical competence. At its core was a stellar CGPA of 9. At its core was a focus on\n",
      "\n",
      "Epoch 4 (Step 0235): Train loss 0.636, Test loss 2.316\n",
      "Epoch 4 (Step 0240): Train loss 0.612, Test loss 2.362\n",
      "Epoch 4 (Step 0245): Train loss 0.544, Test loss 2.420\n",
      "Epoch 4 (Step 0250): Train loss 0.453, Test loss 2.432\n",
      "Epoch 4 (Step 0255): Train loss 0.643, Test loss 2.439\n",
      "Epoch 4 (Step 0260): Train loss 0.594, Test loss 2.448\n",
      "Epoch 4 (Step 0265): Train loss 0.460, Test loss 2.383\n",
      "Epoch 4 (Step 0270): Train loss 0.428, Test loss 2.397\n",
      "Epoch 4 (Step 0275): Train loss 0.433, Test loss 2.425\n",
      "Epoch 4 (Step 0280): Train loss 0.470, Test loss 2.406\n",
      "Epoch 4 (Step 0285): Train loss 0.403, Test loss 2.411\n",
      "Epoch 4 (Step 0290): Train loss 0.359, Test loss 2.447\n",
      "Epoch 4 (Step 0295): Train loss 0.368, Test loss 2.523\n",
      "Epoch 4 (Step 0300): Train loss 0.327, Test loss 2.469\n",
      "Epoch 4 (Step 0305): Train loss 0.280, Test loss 2.475\n",
      "Asif likes to development, and how to an engineer and a researcher. For example, during his undergraduate studies had given him breadth, each of which expanded his Bachelor’s degree—with a respectable CGPA of 3.18 out of 4. This mindset allowed\n",
      "\n",
      "Epoch 5 (Step 0310): Train loss 0.259, Test loss 2.463\n",
      "Epoch 5 (Step 0315): Train loss 0.261, Test loss 2.458\n",
      "Epoch 5 (Step 0320): Train loss 0.238, Test loss 2.493\n",
      "Epoch 5 (Step 0325): Train loss 0.208, Test loss 2.495\n",
      "Epoch 5 (Step 0330): Train loss 0.172, Test loss 2.472\n",
      "Epoch 5 (Step 0335): Train loss 0.174, Test loss 2.469\n",
      "Epoch 5 (Step 0340): Train loss 0.207, Test loss 2.439\n",
      "Epoch 5 (Step 0345): Train loss 0.141, Test loss 2.467\n",
      "Epoch 5 (Step 0350): Train loss 0.142, Test loss 2.438\n",
      "Epoch 5 (Step 0355): Train loss 0.107, Test loss 2.388\n",
      "Epoch 5 (Step 0360): Train loss 0.103, Test loss 2.416\n",
      "Epoch 5 (Step 0365): Train loss 0.102, Test loss 2.443\n",
      "Epoch 5 (Step 0370): Train loss 0.104, Test loss 2.442\n",
      "Epoch 5 (Step 0375): Train loss 0.093, Test loss 2.440\n",
      "Epoch 5 (Step 0380): Train loss 0.078, Test loss 2.455\n",
      "Asif likes to development, he also ensured his team members grew in their own technical competence. This speaks to his belief in collective progress—an understanding that a team’s output is not merely the sum of what he knows.       \n",
      "\n",
      "Epoch 6 (Step 0385): Train loss 0.085, Test loss 2.467\n",
      "Epoch 6 (Step 0390): Train loss 0.069, Test loss 2.469\n",
      "Epoch 6 (Step 0395): Train loss 0.086, Test loss 2.478\n",
      "Epoch 6 (Step 0400): Train loss 0.072, Test loss 2.480\n",
      "Epoch 6 (Step 0405): Train loss 0.070, Test loss 2.482\n",
      "Epoch 6 (Step 0410): Train loss 0.044, Test loss 2.475\n",
      "Epoch 6 (Step 0415): Train loss 0.055, Test loss 2.449\n",
      "Epoch 6 (Step 0420): Train loss 0.059, Test loss 2.451\n",
      "Epoch 6 (Step 0425): Train loss 0.048, Test loss 2.495\n",
      "Epoch 6 (Step 0430): Train loss 0.050, Test loss 2.456\n",
      "Epoch 6 (Step 0435): Train loss 0.032, Test loss 2.454\n",
      "Epoch 6 (Step 0440): Train loss 0.042, Test loss 2.465\n",
      "Epoch 6 (Step 0445): Train loss 0.036, Test loss 2.467\n",
      "Epoch 6 (Step 0450): Train loss 0.031, Test loss 2.477\n",
      "Epoch 6 (Step 0455): Train loss 0.023, Test loss 2.482\n",
      "Epoch 6 (Step 0460): Train loss 0.034, Test loss 2.453\n",
      "Asif likes to development, he also ensured his team members grew in their own technical competence. The project’s ambition was not to simply replicate existing models but also his ability to communicate effectively—skills that yield faster and more resource-efficient results.  \n",
      "\n",
      "Epoch 7 (Step 0465): Train loss 0.023, Test loss 2.433\n",
      "Epoch 7 (Step 0470): Train loss 0.023, Test loss 2.433\n",
      "Epoch 7 (Step 0475): Train loss 0.020, Test loss 2.434\n",
      "Epoch 7 (Step 0480): Train loss 0.027, Test loss 2.432\n",
      "Epoch 7 (Step 0485): Train loss 0.018, Test loss 2.425\n",
      "Epoch 7 (Step 0490): Train loss 0.018, Test loss 2.447\n",
      "Epoch 7 (Step 0495): Train loss 0.016, Test loss 2.453\n",
      "Epoch 7 (Step 0500): Train loss 0.014, Test loss 2.440\n",
      "Epoch 7 (Step 0505): Train loss 0.021, Test loss 2.443\n",
      "Epoch 7 (Step 0510): Train loss 0.016, Test loss 2.433\n",
      "Epoch 7 (Step 0515): Train loss 0.023, Test loss 2.423\n",
      "Epoch 7 (Step 0520): Train loss 0.017, Test loss 2.445\n",
      "Epoch 7 (Step 0525): Train loss 0.012, Test loss 2.458\n",
      "Epoch 7 (Step 0530): Train loss 0.015, Test loss 2.461\n",
      "Epoch 7 (Step 0535): Train loss 0.013, Test loss 2.488\n",
      "Asif likes to development, he also ensured his team members grew in their own technical competence. This speaks to his belief in collective progress—an understanding that a team’s output is not merely the sum of individual contributions but the product of shared learning, dialogue,\n",
      "\n",
      "Epoch 8 (Step 0540): Train loss 0.013, Test loss 2.508\n",
      "Epoch 8 (Step 0545): Train loss 0.018, Test loss 2.521\n",
      "Epoch 8 (Step 0550): Train loss 0.012, Test loss 2.507\n",
      "Epoch 8 (Step 0555): Train loss 0.009, Test loss 2.483\n",
      "Epoch 8 (Step 0560): Train loss 0.013, Test loss 2.465\n",
      "Epoch 8 (Step 0565): Train loss 0.012, Test loss 2.473\n",
      "Epoch 8 (Step 0570): Train loss 0.011, Test loss 2.499\n",
      "Epoch 8 (Step 0575): Train loss 0.011, Test loss 2.524\n",
      "Epoch 8 (Step 0580): Train loss 0.008, Test loss 2.520\n",
      "Epoch 8 (Step 0585): Train loss 0.010, Test loss 2.487\n",
      "Epoch 8 (Step 0590): Train loss 0.012, Test loss 2.447\n",
      "Epoch 8 (Step 0595): Train loss 0.012, Test loss 2.447\n",
      "Epoch 8 (Step 0600): Train loss 0.011, Test loss 2.474\n",
      "Epoch 8 (Step 0605): Train loss 0.010, Test loss 2.498\n",
      "Epoch 8 (Step 0610): Train loss 0.009, Test loss 2.476\n",
      "Epoch 8 (Step 0615): Train loss 0.010, Test loss 2.438\n",
      "Asif likes to development, he also ensured his team members grew in their own technical competence. This speaks to his belief in collective progress—an understanding that a team’s output is not merely the sum of individual contributions but the product of shared learning, dialogue,\n",
      "\n",
      "Epoch 9 (Step 0620): Train loss 0.016, Test loss 2.445\n",
      "Epoch 9 (Step 0625): Train loss 0.013, Test loss 2.464\n",
      "Epoch 9 (Step 0630): Train loss 0.008, Test loss 2.495\n",
      "Epoch 9 (Step 0635): Train loss 0.007, Test loss 2.490\n",
      "Epoch 9 (Step 0640): Train loss 0.009, Test loss 2.496\n",
      "Epoch 9 (Step 0645): Train loss 0.007, Test loss 2.506\n",
      "Epoch 9 (Step 0650): Train loss 0.007, Test loss 2.500\n",
      "Epoch 9 (Step 0655): Train loss 0.006, Test loss 2.501\n",
      "Epoch 9 (Step 0660): Train loss 0.006, Test loss 2.509\n",
      "Epoch 9 (Step 0665): Train loss 0.009, Test loss 2.497\n",
      "Epoch 9 (Step 0670): Train loss 0.006, Test loss 2.500\n",
      "Epoch 9 (Step 0675): Train loss 0.007, Test loss 2.498\n",
      "Epoch 9 (Step 0680): Train loss 0.006, Test loss 2.510\n",
      "Epoch 9 (Step 0685): Train loss 0.007, Test loss 2.502\n",
      "Epoch 9 (Step 0690): Train loss 0.009, Test loss 2.528\n",
      "Asif likes to development, he also ensured his team members grew in their own technical competence. This speaks to his belief in collective progress—an understanding that a team’s output is not merely the sum of individual contributions but the product of shared learning, dialogue,\n",
      "\n",
      "Epoch 10 (Step 0695): Train loss 0.005, Test loss 2.546\n",
      "Epoch 10 (Step 0700): Train loss 0.007, Test loss 2.570\n",
      "Epoch 10 (Step 0705): Train loss 0.006, Test loss 2.534\n",
      "Epoch 10 (Step 0710): Train loss 0.006, Test loss 2.517\n",
      "Epoch 10 (Step 0715): Train loss 0.006, Test loss 2.496\n",
      "Epoch 10 (Step 0720): Train loss 0.006, Test loss 2.506\n",
      "Epoch 10 (Step 0725): Train loss 0.004, Test loss 2.516\n",
      "Epoch 10 (Step 0730): Train loss 0.006, Test loss 2.524\n",
      "Epoch 10 (Step 0735): Train loss 0.006, Test loss 2.533\n",
      "Epoch 10 (Step 0740): Train loss 0.007, Test loss 2.544\n",
      "Epoch 10 (Step 0745): Train loss 0.005, Test loss 2.551\n",
      "Epoch 10 (Step 0750): Train loss 0.004, Test loss 2.556\n",
      "Epoch 10 (Step 0755): Train loss 0.005, Test loss 2.547\n",
      "Epoch 10 (Step 0760): Train loss 0.005, Test loss 2.530\n",
      "Epoch 10 (Step 0765): Train loss 0.007, Test loss 2.520\n",
      "Asif likes to development, he also ensured his team members grew in their own technical competence. This speaks to his belief in collective progress—an understanding that a team’s output is not merely the sum of individual contributions but the product of shared learning, dialogue,\n",
      "\n",
      "Training completed in 5.65 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, test_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, test_loader, optimizer, device, \n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5, # after every 5 batches, training and validation loss will be printed\n",
    "    start_context = \"Asif likes to\", tokenizer = tokenizer\n",
    ") \n",
    "\n",
    "end = time.time()\n",
    "training_time = (end - start) / 60\n",
    "print(f\"Training completed in {training_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a9c87ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 6144, 11264, 16384, 21504, 26624, 31744, 36864, 41984, 47104, 52224, 57344, 62464, 67584, 72704, 77824, 82944, 88064, 93184, 98304, 103424, 108544, 113664, 118784, 123904, 129024, 134144, 139264, 144384, 149504, 154624, 159744, 164864, 169984, 175104, 180224, 185344, 190464, 195584, 200704, 205824, 210944, 216064, 221184, 226304, 231424, 236544, 241664, 246784, 251904, 257024, 262144, 267264, 272384, 277504, 282624, 287744, 292864, 297984, 303104, 308224, 313344, 318464, 323584, 328704, 333824, 338944, 344064, 349184, 354304, 359424, 364544, 369664, 374784, 379904, 385024, 390144, 395264, 400384, 405504, 410624, 415744, 420864, 425984, 431104, 436224, 441344, 446464, 451584, 456704, 461824, 466944, 472064, 477184, 482304, 487424, 492544, 497664, 502784, 507904, 513024, 518144, 523264, 528384, 533504, 538624, 543744, 548864, 553984, 559104, 564224, 569344, 574464, 579584, 584704, 589824, 594944, 600064, 605184, 610304, 615424, 620544, 625664, 630784, 635904, 641024, 646144, 651264, 656384, 661504, 666624, 671744, 676864, 681984, 687104, 692224, 697344, 702464, 707584, 712704, 717824, 722944, 728064, 733184, 738304, 743424, 748544, 753664, 758784, 763904, 769024, 774144, 779264, 784384]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_seen) # number of tokens seen by the model at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e03c1a",
   "metadata": {},
   "source": [
    "Awesome, but there is some overfitting. This means the model is taking text directly from the book...word to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e4c17d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (7, 4))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, label = 'Training Loss', color='#2E86AB', linewidth = 1)\n",
    "plt.plot(epochs, test_losses, label = 'Testing Loss', color='#F24236', linewidth = 1, linestyle='--')\n",
    "\n",
    "plt.title(\"Training & Testing Loss\", fontsize = 14, pad = 10)\n",
    "plt.xlabel(\"Epoch\", fontsize = 10)\n",
    "plt.ylabel(\"Loss\", fontsize = 10)\n",
    "\n",
    "plt.legend(frameon = True, fancybox = True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef895f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d367a73",
   "metadata": {},
   "source": [
    "### 14. LLM DECODING STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ba413d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ae7d8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling + top-k sampling\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature = 0.0, top_k = None, eos_id = None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "        \n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "        \n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c3196",
   "metadata": {},
   "source": [
    "Time to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7a7c2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Asif likes to see AI as an engineering discipline deeply intertwined with society, requiring both technical brilliance and ethical responsibility.\n",
      "\n",
      "By the time Asif completed his Bachelor’s degree—with a respectable CGPA of 3.18 out of 4.00—he was not simply a graduate in Computer Science. He was a budding technologist\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model, \n",
    "    idx = text_to_token_ids(\"Asif likes to\", tokenizer).to(device),\n",
    "    max_new_tokens = 65,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"],\n",
    "    top_k = 25,\n",
    "    temperature = 1.4\n",
    ")\n",
    "\n",
    "print(\"Output:\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a535d277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
