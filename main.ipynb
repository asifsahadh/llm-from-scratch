{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6c38b",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35d5c2",
   "metadata": {},
   "source": [
    "#### 1.1 Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bbc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab034f1",
   "metadata": {},
   "source": [
    "#### 1.2 RE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c072e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', '', ' ', 'what', \"'\", 's', ' ', 'good', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample = \"Hey, what's good?\"\n",
    "result = re.split(r'([,.:;?!\"()\\'/]|--|\\s)', sample)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d13985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'what', \"'\", 's', 'good', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # retunrs false for whitespaces / no spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138869",
   "metadata": {},
   "source": [
    "Now, apply RE tokenizer to main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5277b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4654\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([.,:;?!\"()\\'/]|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.split()]\n",
    "\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51066469",
   "metadata": {},
   "source": [
    "#### 1.3 Token ID creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 1139\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"Length of vocabulary: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6806f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for t, i in vocab.items():\n",
    "    print(t, i)\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41849",
   "metadata": {},
   "source": [
    "#### 1.4 Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "s2i = tokenizer.encode(\"I HAD always thought.\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15042de9",
   "metadata": {},
   "source": [
    "#### 1.5 Special Context Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5422a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary with special context tokens: 1141\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(f\"Length of vocabulary with special context tokens: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8ca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for k, v in enumerate(vocab.keys()):\n",
    "    keys.append(v)\n",
    "print(keys[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c6a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        # if item not in vocab, replace it with <|unk|> token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        preprocessed.append(\"<|endoftext|>\")\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b72c372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, how are you doing? <|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "s2i = tokenizer.encode(\"Hello, how are you doing?\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fbc60",
   "metadata": {},
   "source": [
    "#### 1.6 Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87e9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84994a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2bf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 13778, 2763, 13, 220, 50256, 10928, 345, 588, 257, 6508, 1659, 660, 64, 30]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, Ilham. <|endoftext|> Would you like a cupoftea?\"\n",
    "integers = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b08a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Ilham. <|endoftext|> Would you like a cupoftea?\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe5f90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151db66",
   "metadata": {},
   "source": [
    "### 2. Input-Target Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826950f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens from byte pair encoding: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens from byte pair encoding:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b564426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [40, 367, 2885, 1464, 1807]\n",
      "y:     [367, 2885, 1464, 1807, 3619]\n"
     ]
    }
   ],
   "source": [
    "context_size = 5 # input will have 5 tokens\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size + 1]\n",
    "print(f\"X: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991f59e",
   "metadata": {},
   "source": [
    "#### 2.1 Using Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c2795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride): # max_len is context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - max_len, stride):\n",
    "            input_chunk = token_ids[i:i + max_len]\n",
    "            target_chunk = token_ids[i + 1:i + max_len + 1]\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "    \n",
    "    # the below 2 methods is required for Dataloader to be used\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # we are basically saying that if the input is the 50th tensor, then the output is the 50th tensor\n",
    "        return (\n",
    "            torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            torch.tensor(self.target_ids[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291c8d7",
   "metadata": {},
   "source": [
    "The idea is to form something like as follows:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[5, 6, 7, 8],<br>\n",
    "[9, 10, 11, 12]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[6, 7, 8, 9],<br>\n",
    "[10, 11, 12, 13]]<br><br>\n",
    "...where the first matrix is X and the second matrix is y. Note that in the above example, the stride as well as the max length is 4. If the stride was 2:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[3, 4, 5, 6],<br>\n",
    "[5, 6, 7, 8]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[4, 5, 6, 7],<br>\n",
    "[6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b7d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    # drop last if last tensor is shorter than max_len\n",
    "    # batch size is the number of training ip-op data pairs to be used for training by whcih the parameters are updated\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91f4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_len = 4, stride = 1, shuffle = False) # looking into how the function will work\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00fb31",
   "metadata": {},
   "source": [
    "Using a batch size of 1 is not preferred as this leads to noisy updates, even though good for memory.<br>\n",
    "Note that a higher overlap (lower stride) can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147d962",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51c802",
   "metadata": {},
   "source": [
    "### 3. Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60811e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3 # embedding dimention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # intialize mebedding matrix randomly\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2974b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e240f8",
   "metadata": {},
   "source": [
    "The embedding weight matrix is basically used for lookup operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f791e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids)) # looking up vector embeddings for the sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb66cb",
   "metadata": {},
   "source": [
    "Note that this is essencially a one hot encoded represenation of the input IDs passed into a linear layer to get the output embeddings where the weights of the neural net are randomly initialized. But we dom't use this because it's not efficient due to the sparsity of the one hot encoded input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample two\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f16f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size = 8, max_len = max_len,\n",
    "    stride = max_len, shuffle = False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ee1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a3873",
   "metadata": {},
   "source": [
    "This is basically a batch of 8 with 4 tokens each, and each token is converted to a vector of dimention 256. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b54ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967bad5",
   "metadata": {},
   "source": [
    "### 4. Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16957c",
   "metadata": {},
   "source": [
    "Now we create positional embedding the same way as we did for the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80d41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_len = max_len\n",
    "pos_embedding_layer = torch.nn.Embedding(context_len, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_len)) # arange creates ids from 0 to max_len - 1 and pos_embedding_layer converts them to embedding matrix where each row corresponds to the positional embedding for that position id\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9485424",
   "metadata": {},
   "source": [
    "It must also be noted that each row will have the same set of positional embedding values. In other words, the PE value repeats for each row. So the final embedding matrix will only be 4x256 and not 8x4x256. We only care about the position in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a685f7",
   "metadata": {},
   "source": [
    "We can directly add the token and position embeddings, even though the dimentions don't match exactly via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83f6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22a91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8b1b2",
   "metadata": {},
   "source": [
    "### 5. Simplified Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99401b4a",
   "metadata": {},
   "source": [
    "We know that attention scores are calculated by taking the dot product between the query token and all the other input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c4b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # let the query token be journey\n",
    "\n",
    "attention_scores_x_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_x_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attention_scores_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b35b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# now normalize the scores\n",
    "\n",
    "attention_weights_x_2 = attention_scores_x_2 / attention_scores_x_2.sum()\n",
    "print(attention_weights_x_2)\n",
    "print(attention_weights_x_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f000ce",
   "metadata": {},
   "source": [
    "Note that attention __scores__ are not normalized, but attention __weights__ are, and they sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff9c1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# softmax normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
    "\n",
    "attention_weights_x_2_naive_sm = softmax_naive(attention_scores_x_2) # sm: softmax\n",
    "print(attention_weights_x_2_naive_sm)\n",
    "print(attention_weights_x_2_naive_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6e463",
   "metadata": {},
   "source": [
    "PyTorch implementation of Softmax is preffered to control instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433ce8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# pytorch softmax operation\n",
    "\n",
    "attention_weights_x_2_pt_sm = torch.softmax(attention_scores_x_2, dim = 0) # pt: pytorch\n",
    "print(attention_weights_x_2_pt_sm)\n",
    "print(attention_weights_x_2_pt_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8713a1",
   "metadata": {},
   "source": [
    "#### 5.1 Context vector calculation for 'journey'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95e352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "context_vector_x2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_x2 += attention_weights_x_2_pt_sm[i] * x_i\n",
    "\n",
    "print(context_vector_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af65b2",
   "metadata": {},
   "source": [
    "#### 5.2 Calculate attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09b7c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44d1f3",
   "metadata": {},
   "source": [
    "This can be done using 2 for loops but that's computationally very expensive. Rather, we can do the above transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd949977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim = -1) \n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080d6e",
   "metadata": {},
   "source": [
    "Setting dimention to -1 means it will normalize accross the columns. This is because the matrix dimention is n_row x n_col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53a8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vectors calculation (z_i)\n",
    "\n",
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb33fa7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a780a1",
   "metadata": {},
   "source": [
    "### 6. Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27e2b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb2fa1",
   "metadata": {},
   "source": [
    "Now we randomly initialize W_q, W_k & W_v. Each of them will have dimentiones were the number of row count will be eqaul to the input vector dimention (column count of input matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f666e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be working with the sample word 'journey' again\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # this will be the number of columns in the key, quey and value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c259d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# set requires_grad to True later for model training\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "print(W_q)\n",
    "print(W_k)\n",
    "print(W_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff8bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the query, key and value for the sample input word 'journey'\n",
    "\n",
    "q_2 = x_2 @ W_q\n",
    "k_2 = x_2 @ W_k\n",
    "v_2 = x_2 @ W_v\n",
    "\n",
    "print(q_2)\n",
    "print(k_2)\n",
    "print(v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d166cc5",
   "metadata": {},
   "source": [
    "Note that conventionally, just like how things were implemented in section 5, the output from these dot product operations must have the same dimention as the input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e18afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]])\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "# get the overall query, key and value\n",
    "\n",
    "query = inputs @ W_q\n",
    "key = inputs @ W_k\n",
    "value = inputs @ W_v\n",
    "\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38158409",
   "metadata": {},
   "source": [
    "Now we compute the attention scores. In self attention, this is essencially the dot product between the query and the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2b0bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# for 'journey':\n",
    "\n",
    "query_2 = query[1]\n",
    "key_2 = key[1]\n",
    "\n",
    "attention_scores_2 = query_2 @ key.T\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1ce3e",
   "metadata": {},
   "source": [
    "This is basically saying how much the word __journey__ attends to all the other words. Obviously, this will be highest for the second word (itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7406e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "# overall attention\n",
    "\n",
    "attention_scores = query @ key.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ea712",
   "metadata": {},
   "source": [
    "For now, these don't mean anything because they are not trained. Next, we normalize these scores. We normalize by first scaling the scores by square root of d_out or embedding dimention of each word of the key matrix (number of columns). Next, we apply softmax over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77eeecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# normalize to get attention weights (this is just for 'journey')\n",
    "\n",
    "d_k = key.shape[1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61831e12",
   "metadata": {},
   "source": [
    "Why take square root? Multiply any 2 numbers (here, we are multiplying the key and query) increases the variance. So to stabilize it back, we take the root. Another reason is for bringing stability to the softmax outputs and to have an even distribution. If not, the scores can get overly confident for a single input word. (Refer lecture 15, 46th minute for more detail).<br><br>\n",
    "This is why self attention is also called __sclaed dot product attention__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae49dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "# get attention weights for enitre sentence\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42196d7a",
   "metadata": {},
   "source": [
    "These attentions weights are now multiplied with the _value_ matrix to get the __context vectors__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4251f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "context = attention_weights @ value\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068759",
   "metadata": {},
   "source": [
    "Now we make a self attention calss for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d68f1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_k\n",
    "        queries = x @ self.W_q\n",
    "        values = x @ self.W_v\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80c39c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "378ea687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, which is more optimized due to the Linear class from PyTorch\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    \n",
    "    # change here compared to v1\n",
    "    def forward(self, x): \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c61440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29b258",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec3580",
   "metadata": {},
   "source": [
    "### 7. Causal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8e047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55681081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f16ae",
   "metadata": {},
   "source": [
    "Now we apply causal masking using tril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "001d9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_len = attention_weights.shape[0]\n",
    "mask_sample = torch.tril(torch.ones(context_len, context_len))\n",
    "print(mask_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d705d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attention = attention_weights * mask_sample\n",
    "print(masked_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788ce1",
   "metadata": {},
   "source": [
    "Now perform renormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bfb33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_attention.sum(dim = 1, keepdim = True)\n",
    "masked_attention_norm = masked_attention / row_sums\n",
    "print(masked_attention_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886285d",
   "metadata": {},
   "source": [
    "However, there is a problem with this method. Even though we have performed masking, the attention scores after applying softmax leads to data leakag due to data redistribution which occurs based on future values as well. A simple solution is to perform masking over the attention scores, and then perform softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9660fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf) # this basically takes the attention scores matrix, looks at positions where the value is True, and gves it -ve inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61290adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aae77",
   "metadata": {},
   "source": [
    "_(not exactly correct it seems)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412bba0",
   "metadata": {},
   "source": [
    "Dropout is applied here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d13e1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f273f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cba8",
   "metadata": {},
   "source": [
    "Rescaling based on droupout percentage also occurs.<br>\n",
    "Next, we also introduce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c2196b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch)\n",
    "print()\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72632a48",
   "metadata": {},
   "source": [
    "Think of it as 2 input matrices. One sentence can be \"your journey starts with one step\" and the other can be \"my name is mohammed asif sahadh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1eca4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # new\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal = 1)) # new (register buffer i think ensures that the non trainable stuff will be moved to appropriate device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # new batch dim b\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # one coma two because we don't need to transpose the batch dimention (idx 0)\n",
    "        attention_scores.masked_fill_( # _ ops makes it inplace\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # :num_tokens is to ensure if the sequence is less than the context length\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights) # new      \n",
    " \n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "422bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_len, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print()\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f200a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54ab2",
   "metadata": {},
   "source": [
    "### 8. Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1552de7",
   "metadata": {},
   "source": [
    "For MHA, we simply have to create a wrapper for causal attention that stacks the outputs of multiple of thier outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "eed8cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( # creates an instance of causal attention class\n",
    "            [CausalAttention(d_in, d_out, context_len, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f655078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "391277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0cbd3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = inputs.shape[0]\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_len, 0.0, 3)\n",
    "mha.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b17cb",
   "metadata": {},
   "source": [
    "_(output matches here though)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
