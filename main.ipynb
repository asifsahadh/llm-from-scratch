{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6c38b",
   "metadata": {},
   "source": [
    "### 1. TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35d5c2",
   "metadata": {},
   "source": [
    "#### _1.1 Load text file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bbc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab034f1",
   "metadata": {},
   "source": [
    "#### _1.2 RE tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c072e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', '', ' ', 'what', \"'\", 's', ' ', 'good', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample = \"Hey, what's good?\"\n",
    "result = re.split(r'([,.:;?!\"()\\'/]|--|\\s)', sample)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d13985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'what', \"'\", 's', 'good', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # retunrs false for whitespaces / no spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138869",
   "metadata": {},
   "source": [
    "Now, apply RE tokenizer to main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5277b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4654\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([.,:;?!\"()\\'/]|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.split()]\n",
    "\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51066469",
   "metadata": {},
   "source": [
    "#### _1.3 Token ID creation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 1139\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"Length of vocabulary: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6806f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for t, i in vocab.items():\n",
    "    print(t, i)\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41849",
   "metadata": {},
   "source": [
    "#### _1.4 Tokenizer class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "s2i = tokenizer.encode(\"I HAD always thought.\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15042de9",
   "metadata": {},
   "source": [
    "#### _1.5 Special Context Tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5422a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary with special context tokens: 1141\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(f\"Length of vocabulary with special context tokens: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8ca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for k, v in enumerate(vocab.keys()):\n",
    "    keys.append(v)\n",
    "print(keys[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c6a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        # if item not in vocab, replace it with <|unk|> token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        preprocessed.append(\"<|endoftext|>\")\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b72c372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, how are you doing? <|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "s2i = tokenizer.encode(\"Hello, how are you doing?\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fbc60",
   "metadata": {},
   "source": [
    "#### _1.6 Byte Pair Encoding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87e9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84994a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2bf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 13778, 2763, 13, 220, 50256, 10928, 345, 588, 257, 6508, 1659, 660, 64, 30]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, Ilham. <|endoftext|> Would you like a cupoftea?\"\n",
    "integers = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b08a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Ilham. <|endoftext|> Would you like a cupoftea?\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe5f90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151db66",
   "metadata": {},
   "source": [
    "### 2. INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826950f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens from byte pair encoding: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens from byte pair encoding:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b564426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [40, 367, 2885, 1464, 1807]\n",
      "y:     [367, 2885, 1464, 1807, 3619]\n"
     ]
    }
   ],
   "source": [
    "context_size = 5 # input will have 5 tokens\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size + 1]\n",
    "print(f\"X: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991f59e",
   "metadata": {},
   "source": [
    "#### _2.1 Using Dataloader_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c2795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride): # max_len is context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - max_len, stride):\n",
    "            input_chunk = token_ids[i:i + max_len]\n",
    "            target_chunk = token_ids[i + 1:i + max_len + 1]\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "    \n",
    "    # the below 2 methods is required for Dataloader to be used\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # we are basically saying that if the input is the 50th tensor, then the output is the 50th tensor\n",
    "        return (\n",
    "            torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            torch.tensor(self.target_ids[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291c8d7",
   "metadata": {},
   "source": [
    "The idea is to form something like as follows:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[5, 6, 7, 8],<br>\n",
    "[9, 10, 11, 12]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[6, 7, 8, 9],<br>\n",
    "[10, 11, 12, 13]]<br><br>\n",
    "...where the first matrix is X and the second matrix is y. Note that in the above example, the stride as well as the max length is 4. If the stride was 2:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[3, 4, 5, 6],<br>\n",
    "[5, 6, 7, 8]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[4, 5, 6, 7],<br>\n",
    "[6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b7d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    # drop last if last tensor is shorter than max_len\n",
    "    # batch size is the number of training ip-op data pairs to be used for training by whcih the parameters are updated\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91f4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_len = 4, stride = 1, shuffle = False) # looking into how the function will work\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00fb31",
   "metadata": {},
   "source": [
    "Using a batch size of 1 is not preferred as this leads to noisy updates, even though good for memory.<br>\n",
    "Note that a higher overlap (lower stride) can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147d962",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51c802",
   "metadata": {},
   "source": [
    "### 3. VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60811e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3 # embedding dimention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # intialize mebedding matrix randomly\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2974b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e240f8",
   "metadata": {},
   "source": [
    "The embedding weight matrix is basically used for lookup operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f791e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids)) # looking up vector embeddings for the sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb66cb",
   "metadata": {},
   "source": [
    "Note that this is essencially a one hot encoded represenation of the input IDs passed into a linear layer to get the output embeddings where the weights of the neural net are randomly initialized. But we dom't use this because it's not efficient due to the sparsity of the one hot encoded input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample two\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f16f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size = 8, max_len = max_len,\n",
    "    stride = max_len, shuffle = False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ee1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a3873",
   "metadata": {},
   "source": [
    "This is basically a batch of 8 with 4 tokens each, and each token is converted to a vector of dimention 256. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b54ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967bad5",
   "metadata": {},
   "source": [
    "### 4. POSITIONAL EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16957c",
   "metadata": {},
   "source": [
    "Now we create positional embedding the same way as we did for the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80d41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_len = max_len # 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_len, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_len)) # arange creates ids from 0 to max_len - 1 and pos_embedding_layer converts them to embedding matrix where each row corresponds to the positional embedding for that position id\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36c780",
   "metadata": {},
   "source": [
    "So the first vector embedding of a 4 token sentence will always be added by the vector [1.7375, -0.5620, ..., 1.0345]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9485424",
   "metadata": {},
   "source": [
    "It must also be noted that each row will have the same set of positional embedding values. In other words, the PE value repeats for each row. So the final embedding matrix will only be 4x256 and not 8x4x256. We only care about the position in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a685f7",
   "metadata": {},
   "source": [
    "We can directly add the token and position embeddings, even though the dimentions don't match exactly via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83f6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22a91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8b1b2",
   "metadata": {},
   "source": [
    "### 5. SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99401b4a",
   "metadata": {},
   "source": [
    "We know that attention scores are calculated by taking the dot product between the query token and all the other input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c4b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # let the query token be journey\n",
    "\n",
    "attention_scores_x_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_x_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attention_scores_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b35b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# now normalize the scores\n",
    "\n",
    "attention_weights_x_2 = attention_scores_x_2 / attention_scores_x_2.sum()\n",
    "print(attention_weights_x_2)\n",
    "print(attention_weights_x_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f000ce",
   "metadata": {},
   "source": [
    "Note that attention __scores__ are not normalized, but attention __weights__ are, and they sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff9c1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# softmax normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
    "\n",
    "attention_weights_x_2_naive_sm = softmax_naive(attention_scores_x_2) # sm: softmax\n",
    "print(attention_weights_x_2_naive_sm)\n",
    "print(attention_weights_x_2_naive_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6e463",
   "metadata": {},
   "source": [
    "PyTorch implementation of Softmax is preffered to control instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433ce8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# pytorch softmax operation\n",
    "\n",
    "attention_weights_x_2_pt_sm = torch.softmax(attention_scores_x_2, dim = 0) # pt: pytorch\n",
    "print(attention_weights_x_2_pt_sm)\n",
    "print(attention_weights_x_2_pt_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8713a1",
   "metadata": {},
   "source": [
    "#### _5.1 Context vector calculation for 'journey'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95e352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "context_vector_x2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_x2 += attention_weights_x_2_pt_sm[i] * x_i\n",
    "\n",
    "print(context_vector_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af65b2",
   "metadata": {},
   "source": [
    "#### _5.2 Calculate attention matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09b7c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44d1f3",
   "metadata": {},
   "source": [
    "This can be done using 2 for loops but that's computationally very expensive. Rather, we can do the above transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd949977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim = -1) \n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080d6e",
   "metadata": {},
   "source": [
    "Setting dimention to -1 means it will normalize accross the columns. This is because the matrix dimention is n_row x n_col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53a8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vectors calculation (z_i)\n",
    "\n",
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb33fa7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a780a1",
   "metadata": {},
   "source": [
    "### 6. SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27e2b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb2fa1",
   "metadata": {},
   "source": [
    "Now we randomly initialize W_q, W_k & W_v. Each of them will have dimentiones were the number of row count will be eqaul to the input vector dimention (column count of input matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f666e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be working with the sample word 'journey' again\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # this will be the number of columns in the key, quey and value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c259d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# set requires_grad to True later for model training\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "print(W_q)\n",
    "print(W_k)\n",
    "print(W_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff8bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the query, key and value for the sample input word 'journey'\n",
    "\n",
    "q_2 = x_2 @ W_q\n",
    "k_2 = x_2 @ W_k\n",
    "v_2 = x_2 @ W_v\n",
    "\n",
    "print(q_2)\n",
    "print(k_2)\n",
    "print(v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d166cc5",
   "metadata": {},
   "source": [
    "Note that conventionally, just like how things were implemented in section 5, the output from these dot product operations must have the same dimention as the input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e18afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]])\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "# get the overall query, key and value\n",
    "\n",
    "query = inputs @ W_q\n",
    "key = inputs @ W_k\n",
    "value = inputs @ W_v\n",
    "\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38158409",
   "metadata": {},
   "source": [
    "Now we compute the attention scores. In self attention, this is essencially the dot product between the query and the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2b0bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# for 'journey':\n",
    "\n",
    "query_2 = query[1]\n",
    "key_2 = key[1]\n",
    "\n",
    "attention_scores_2 = query_2 @ key.T\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1ce3e",
   "metadata": {},
   "source": [
    "This is basically saying how much the word __journey__ attends to all the other words. Obviously, this will be highest for the second word (itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7406e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "# overall attention\n",
    "\n",
    "attention_scores = query @ key.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ea712",
   "metadata": {},
   "source": [
    "For now, these don't mean anything because they are not trained. Next, we normalize these scores. We normalize by first scaling the scores by square root of d_out or embedding dimention of each word of the key matrix (number of columns). Next, we apply softmax over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77eeecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# normalize to get attention weights (this is just for 'journey')\n",
    "\n",
    "d_k = key.shape[1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61831e12",
   "metadata": {},
   "source": [
    "Why take square root? Multiply any 2 numbers (here, we are multiplying the key and query) increases the variance. So to stabilize it back, we take the root. Another reason is for bringing stability to the softmax outputs and to have an even distribution. If not, the scores can get overly confident for a single input word. (Refer lecture 15, 46th minute for more detail).<br><br>\n",
    "This is why self attention is also called __sclaed dot product attention__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae49dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "# get attention weights for enitre sentence\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42196d7a",
   "metadata": {},
   "source": [
    "These attentions weights are now multiplied with the _value_ matrix to get the __context vectors__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4251f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "context = attention_weights @ value\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068759",
   "metadata": {},
   "source": [
    "Now we make a self attention calss for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d68f1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_k\n",
    "        queries = x @ self.W_q\n",
    "        values = x @ self.W_v\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80c39c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "378ea687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, which is more optimized due to the Linear class from PyTorch\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    \n",
    "    # change here compared to v1\n",
    "    def forward(self, x): \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c61440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29b258",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec3580",
   "metadata": {},
   "source": [
    "### 7. CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8e047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55681081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f16ae",
   "metadata": {},
   "source": [
    "Now we apply causal masking using tril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "001d9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_len = attention_weights.shape[0]\n",
    "mask_sample = torch.tril(torch.ones(context_len, context_len))\n",
    "print(mask_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d705d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attention = attention_weights * mask_sample\n",
    "print(masked_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788ce1",
   "metadata": {},
   "source": [
    "Now perform renormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bfb33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_attention.sum(dim = 1, keepdim = True)\n",
    "masked_attention_norm = masked_attention / row_sums\n",
    "print(masked_attention_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886285d",
   "metadata": {},
   "source": [
    "However, there is a problem with this method. Even though we have performed masking, the attention scores after applying softmax leads to data leakag due to data redistribution which occurs based on future values as well. A simple solution is to perform masking over the attention scores, and then perform softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9660fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf) # this basically takes the attention scores matrix, looks at positions where the value is True, and gves it -ve inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61290adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aae77",
   "metadata": {},
   "source": [
    "_(not exactly correct it seems)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412bba0",
   "metadata": {},
   "source": [
    "Dropout is applied here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d13e1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f273f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cba8",
   "metadata": {},
   "source": [
    "Rescaling based on droupout percentage also occurs.<br>\n",
    "Next, we also introduce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c2196b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch)\n",
    "print()\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72632a48",
   "metadata": {},
   "source": [
    "Think of it as 2 input matrices. One sentence can be \"your journey starts with one step\" and the other can be \"my name is mohammed asif sahadh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1eca4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # new\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal = 1)) # new (register buffer i think ensures that the non trainable stuff will be moved to appropriate device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # new batch dim b\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # one coma two because we don't need to transpose the batch dimention (idx 0)\n",
    "        attention_scores.masked_fill_( # _ ops makes it inplace\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # :num_tokens is to ensure if the sequence is less than the context length\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights) # new      \n",
    " \n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "422bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_len, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print()\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f200a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54ab2",
   "metadata": {},
   "source": [
    "### 8. MULTI HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1552de7",
   "metadata": {},
   "source": [
    "For MHA, we simply have to create a wrapper for causal attention that stacks the outputs of multiple of thier outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eed8cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( # creates an instance of causal attention class\n",
    "            [CausalAttention(d_in, d_out, context_len, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f655078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "391277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0cbd3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = inputs.shape[0]\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_len, 0.0, 3)\n",
    "mha.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b17cb",
   "metadata": {},
   "source": [
    "_(output matches here though)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b194d7",
   "metadata": {},
   "source": [
    "To solve the inefficiency casued by performing matrix multiplations over multiple head, we implement weight splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e319612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_out = x.shape # s1\n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # s5\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights) # s8\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2) # s9 & s10\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # s11\n",
    "        context_vec = self.out_proj(context_vec) # optional\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a43c13",
   "metadata": {},
   "source": [
    "s1 to s11 are steps to implement MHA, in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87274ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]],\n",
      "\n",
      "        [[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.90, 0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)\n",
    "\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02e987",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39e8b5",
   "metadata": {},
   "source": [
    "### 9. IMPLEMENTING GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c5f2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2425",
   "metadata": {},
   "source": [
    "#### _9.1 Dummy class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f92aa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # this is the entire lookup matrix to get the embedding for a token\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # this depends on context length of course (also a lookup table)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # make a placeholder for transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )        \n",
    "        \n",
    "        # make a placeholder for layernorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx): # in_idx is a batch of input tokens\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device)) # arange creates ids from 0 to max_len - 1 \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) # lots of things happen here\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af49492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token embeddings:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Logits:\n",
      "tensor([[[-5.4645e-03, -7.1054e-01, -1.0149e+00,  ..., -4.1823e-01,\n",
      "           2.2412e-02, -1.0233e+00],\n",
      "         [ 1.7904e+00, -3.2025e-03, -1.1346e-01,  ..., -6.4449e-01,\n",
      "           6.3170e-01,  3.2810e+00],\n",
      "         [ 4.1122e-01, -2.4297e-01, -3.9615e-01,  ...,  3.1840e-01,\n",
      "           5.1082e-01, -7.7559e-02],\n",
      "         [-8.1073e-01, -4.5008e-01, -8.1223e-01,  ...,  1.2990e+00,\n",
      "           3.7404e-01,  9.0462e-02]],\n",
      "\n",
      "        [[ 2.6268e-01, -7.7104e-01, -1.4651e+00,  ..., -6.4443e-01,\n",
      "          -4.4353e-01, -9.2719e-01],\n",
      "         [ 1.1406e+00, -3.4269e-01, -6.9491e-01,  ..., -1.7102e-01,\n",
      "           6.8366e-01,  2.0607e+00],\n",
      "         [ 1.1106e+00,  1.1755e+00, -6.2576e-01,  ..., -4.4802e-01,\n",
      "           2.6767e-02, -9.2523e-01],\n",
      "         [-3.8614e-01,  3.0415e-01, -9.1265e-01,  ...,  1.8638e+00,\n",
      "           6.4222e-01,  2.9281e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dummy sample\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim = 0)\n",
    "print(\"Input token embeddings:\")\n",
    "print(batch)\n",
    "print()\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Logits:\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39f60",
   "metadata": {},
   "source": [
    "#### _9.2 Layernorm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f850829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[-1.4771,  0.3162, -0.1837, -0.8028, -1.2379],\n",
      "        [-1.2232,  0.3065, -0.4733, -0.1332, -0.5370]])\n",
      "\n",
      "Output:\n",
      "tensor([[0.0000, 0.2986, 0.2986, 0.5947, 1.2578, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3003, 0.4595, 0.9096, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple feedforward\n",
    "batch_sample = torch.randn(2, 5)\n",
    "print(\"Input:\")\n",
    "print(batch_sample)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_sample)\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "181ab561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4083],\n",
      "        [0.2782]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.2228],\n",
      "        [0.1328]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim = -1, keepdim = True)\n",
    "var = out.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e1ffc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8650, -0.2323, -0.2324,  0.3950,  1.7998, -0.8650],\n",
      "        [-0.7634, -0.7634,  0.0605,  0.4974,  1.7322, -0.7634]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layernorm = (out - mean) / (var ** 0.5)\n",
    "print(layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c6e7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4769e-08],\n",
      "        [ 4.4703e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = layernorm.mean(dim = -1, keepdim = True)\n",
    "var = layernorm.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a38ab2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "011ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2219e-07],\n",
      "        [-9.5367e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_sample)\n",
    "mean = out_ln.mean(dim = -1, keepdim = True)\n",
    "var = out_ln.var(dim = -1, keepdim = True, unbiased = False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f45fe",
   "metadata": {},
   "source": [
    "#### _9.3 GELU_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d37162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a23c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b0ce1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afb9f8",
   "metadata": {},
   "source": [
    "#### _9.4 Skip Connections_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30c63ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut): \n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), \n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x) # output of linear layer x\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87eab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_wo_shortut = ExampleDNN(layer_sizes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59b36e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grads(model, x):\n",
    "    output = model(x) # normal output from nn\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "794b7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_wo_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cace1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_w_shortut = ExampleDNN(layer_sizes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1291623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_w_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def0a06",
   "metadata": {},
   "source": [
    "Gradient vanishing reduced..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a05c5",
   "metadata": {},
   "source": [
    "#### _9.5 Coding Attention & Linear Layers in a Transformer Block_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5cf83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733201",
   "metadata": {},
   "source": [
    "Requirements for building the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50ef554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ee8ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # MHA\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # shape: [batch size, num tokens, emb size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        # FCL\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cadd4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[0.2961, 0.5166, 0.2517,  ..., 0.9541, 0.8567, 0.4604],\n",
      "         [0.2238, 0.3047, 0.3019,  ..., 0.5465, 0.4532, 0.7598],\n",
      "         [0.6945, 0.2478, 0.4111,  ..., 0.8838, 0.4898, 0.5963],\n",
      "         [0.0890, 0.7804, 0.9223,  ..., 0.4507, 0.6357, 0.5833]],\n",
      "\n",
      "        [[0.5716, 0.9297, 0.3396,  ..., 0.0477, 0.4564, 0.2797],\n",
      "         [0.0936, 0.2211, 0.3806,  ..., 0.3948, 0.4545, 0.4536],\n",
      "         [0.6788, 0.1741, 0.2084,  ..., 0.5557, 0.5930, 0.0959],\n",
      "         [0.3894, 0.4083, 0.0662,  ..., 0.9861, 0.9341, 0.1319]]])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Output:\n",
      "tensor([[[-0.0055,  0.0972, -0.1122,  ...,  1.2889,  0.2623,  0.6685],\n",
      "         [ 0.0023, -0.2369,  0.1720,  ...,  0.5952,  0.2497,  0.7447],\n",
      "         [ 0.4673,  0.4472,  0.1791,  ...,  1.2525,  0.3045,  0.7750],\n",
      "         [ 0.0662,  0.7224,  0.9206,  ...,  0.4790,  0.7428,  0.7015]],\n",
      "\n",
      "        [[ 0.3622,  1.2144,  0.5221,  ...,  0.1854,  0.0111, -0.5034],\n",
      "         [-0.0225,  0.7789,  0.2770,  ...,  0.1734,  0.5419,  0.1143],\n",
      "         [ 0.7425,  0.4013,  0.3211,  ...,  0.3268,  0.7523, -0.1642],\n",
      "         [ 0.5745,  0.6241,  0.4410,  ...,  1.1963,  1.2650,  0.2243]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24eb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896df3d9",
   "metadata": {},
   "source": [
    "### 10. CODING GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "548d0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b2055",
   "metadata": {},
   "source": [
    "#### _10.1 The GPT Class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74dea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_len\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx): # input batch\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33845b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output batch:\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\")\n",
    "print(batch)\n",
    "print(\"Output batch:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71656dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e817112",
   "metadata": {},
   "source": [
    "_Note: We are not using weight tying, which was performed in the original GPT-2 model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c9e73",
   "metadata": {},
   "source": [
    "#### _10.2 Generate Text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9f72ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx is the input batch\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop current context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # get predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # batch_size x tokens_num x vocab_size\n",
    "        # get the last time step (last set of logits)\n",
    "        logits = logits[:, -1, :]\n",
    "        # apply softmax\n",
    "        probs = torch.softmax(logits, dim = -1)\n",
    "        # get id of max\n",
    "        idx_next = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "        # append id to running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c37518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 314, 716, 407, 1016, 284, 4483, 11311]\n",
      "Encoded tensor: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311]])\n",
      "Encoded tensor shape: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am not going to eat chocolate\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded tensor: {encoded_tensor}\")\n",
    "print(f\"Encoded tensor shape: {encoded_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50371a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311, 20656,\n",
      "         30719, 44035, 23338, 24151, 10835, 42731, 35799, 46215,   371]])\n",
      "Output shape: 19\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Output shape: {len(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9b986d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am not going to eat chocolate AmbassadorOptional touredنGirl malesGradットorea R\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4bf9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68ec82",
   "metadata": {},
   "source": [
    "### 11. The LLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17e66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 256, # can be reduced for training simplicity\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88e7cef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d000af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "\n",
    "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41ebb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feet wet\n",
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\"]\n",
    "                       [40, 1107, 588]])     # [\"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],   # [\" effort moves you\"]\n",
    "                        [1107, 588, 11311]]) # [\" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50fea205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., 2.2409e-05,\n",
      "          6.9776e-06, 1.8776e-05],\n",
      "         [9.1569e-06, 1.0062e-05, 7.8786e-06,  ..., 2.9090e-05,\n",
      "          6.0103e-06, 1.3571e-05],\n",
      "         [2.9877e-05, 8.8507e-06, 1.5741e-05,  ..., 3.5456e-05,\n",
      "          1.4094e-05, 1.3526e-05]],\n",
      "\n",
      "        [[1.2561e-05, 2.0538e-05, 1.4332e-05,  ..., 1.0389e-05,\n",
      "          3.4784e-05, 1.4239e-05],\n",
      "         [7.2731e-06, 1.7864e-05, 1.0565e-05,  ..., 2.1206e-05,\n",
      "          1.1390e-05, 1.5559e-05],\n",
      "         [2.9496e-05, 3.3605e-05, 4.1029e-05,  ..., 6.5249e-06,\n",
      "          5.8203e-05, 1.3698e-05]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim = -1)\n",
    "print(probs.shape)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e751eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7781e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  effort moves you\n",
      "Predicted:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Predicted: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe710892",
   "metadata": {},
   "source": [
    "#### _11.1 Cross Entropy Loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2790cdf",
   "metadata": {},
   "source": [
    "Basically find the target probabilities of the target ids. We need to get these as close to 1 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a599343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e579b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5a962f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be67a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e02985cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_log_likelihood = -1 * avg_log_probs\n",
    "print(neg_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6eaf5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler method\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "# above, we have merged the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6aab6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91714488",
   "metadata": {},
   "source": [
    "#### _11.2 Perplexity_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cea0b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perp = torch.exp(loss)\n",
    "print(perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ece60",
   "metadata": {},
   "source": [
    "That's horrible...for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d066a05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dec17",
   "metadata": {},
   "source": [
    "### 12. INITIAL ERROR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c34ddffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b38052cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_tokens = len(tokenizer.encode(raw_text))\n",
    "print(f\"Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4f4810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = True,\n",
    "    shuffle = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "865bda38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens * (1 - train_ratio) < GPT_CONFIG_124M[\"context_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c291830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Test Loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print()\n",
    "print(\"Test Loader:\")\n",
    "for x, y in test_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b12ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # this does all the softmax & everything\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None): # this will show the loss of the LM\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches # mean loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "081e60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cac25d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987583266364204\n",
      "Testing loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): # disable for now\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Testing loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eff95",
   "metadata": {},
   "source": [
    "### 13. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0fdb1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function\n",
    "def evaluate_model(model, train_loader, test_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches = eval_iter)\n",
    "        test_loss = calc_loss_loader(test_loader, model, device, num_batches = eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4ed39a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see text generation during training\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model, idx = encoded, max_new_tokens = 50, context_size = context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    print()\n",
    "    model.train() # set it back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2aa2c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, test_loader, optimizer, device, \n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients from previous batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel() # return number of tokens seen\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation (optional)\n",
    "            if global_step % eval_freq == 0: # only after a set of batches is used for training\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_loader, test_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch + 1} (Step {global_step:02d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Test loss {test_loss:.3f}\")\n",
    "            \n",
    "        # print sample text from each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        \n",
    "    return train_losses, test_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "db83d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 00): Train loss 9.821, Test loss 9.931\n",
      "Epoch 1 (Step 05): Train loss 8.069, Test loss 8.334\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "\n",
      "Epoch 2 (Step 10): Train loss 6.623, Test loss 7.049\n",
      "Epoch 2 (Step 15): Train loss 6.046, Test loss 6.598\n",
      "Every effort moves you, and,, and,,,,,,,,,.                                   \n",
      "\n",
      "Epoch 3 (Step 20): Train loss 5.558, Test loss 6.503\n",
      "Epoch 3 (Step 25): Train loss 5.471, Test loss 6.392\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and I had, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "\n",
      "Epoch 4 (Step 30): Train loss 4.995, Test loss 6.275\n",
      "Epoch 4 (Step 35): Train loss 4.756, Test loss 6.289\n",
      "Every effort moves you, and I had been the picture.                    \"I\"I the the donkey of the donkey the donkey of the picture and I had been a\"I\n",
      "\n",
      "Epoch 5 (Step 40): Train loss 4.113, Test loss 6.182\n",
      "Every effort moves you know the \"Oh, and he had to me--I me. \"Oh, I felt--and it's the  \"Oh, and I had been the donkey--and it to me, and down the \"Oh,\n",
      "\n",
      "Epoch 6 (Step 45): Train loss 3.721, Test loss 6.143\n",
      "Epoch 6 (Step 50): Train loss 3.170, Test loss 6.147\n",
      "Every effort moves you know the fact, and I felt.  \"I had the last word.     \"I didn't. \"I was his pictures--I looked.   \"I looked.    \"I\n",
      "\n",
      "Epoch 7 (Step 55): Train loss 3.109, Test loss 6.186\n",
      "Epoch 7 (Step 60): Train loss 2.370, Test loss 6.126\n",
      "Every effort moves you know the inevitable garlanded to have to have the fact with a little: \"Yes--and by me to me to have to see a smile behind his pictures--as I had been the honour of the donkey.      \n",
      "\n",
      "Epoch 8 (Step 65): Train loss 1.911, Test loss 6.151\n",
      "Epoch 8 (Step 70): Train loss 1.592, Test loss 6.223\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain. \"Oh, he was's an awful simpleton, and Mrs. Gisburn's head to look up at the sketch of the donkey. \"There were days when I\n",
      "\n",
      "Epoch 9 (Step 75): Train loss 1.239, Test loss 6.259\n",
      "Epoch 9 (Step 80): Train loss 0.957, Test loss 6.270\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. The last word.        He laughed again, and threw back the head to look up at the sketch of the donkey. \"There were days when I\n",
      "\n",
      "Epoch 10 (Step 85): Train loss 0.708, Test loss 6.387\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "\n",
      "Training completed in 0.46 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, test_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, test_loader, optimizer, device, \n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5, # after every 5 batches, training and validation loss will be printed\n",
    "    start_context = \"Every effort moves you\", tokenizer = tokenizer\n",
    ") \n",
    "\n",
    "end = time.time()\n",
    "training_time = (end - start) / 60\n",
    "print(f\"Training completed in {training_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a9c87ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512,\n",
       " 3072,\n",
       " 5632,\n",
       " 8192,\n",
       " 10752,\n",
       " 13312,\n",
       " 15872,\n",
       " 18432,\n",
       " 20992,\n",
       " 23552,\n",
       " 26112,\n",
       " 28672,\n",
       " 31232,\n",
       " 33792,\n",
       " 36352,\n",
       " 38912,\n",
       " 41472,\n",
       " 44032]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_seen # number of tokens seen by the model at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e03c1a",
   "metadata": {},
   "source": [
    "Awesome, but there is some overfitting. This means the model is taking text directly from the book...word to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e4c17d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (7, 4))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, label = 'Training Loss', color='#2E86AB', linewidth = 1)\n",
    "plt.plot(epochs, test_losses, label = 'Testing Loss', color='#F24236', linewidth = 1, linestyle='--')\n",
    "\n",
    "plt.title(\"Training & Testing Loss\", fontsize = 14, pad = 10)\n",
    "plt.xlabel(\"Epoch\", fontsize = 10)\n",
    "plt.ylabel(\"Loss\", fontsize = 10)\n",
    "\n",
    "plt.legend(frameon = True, fancybox = True, shadow = True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd5ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
