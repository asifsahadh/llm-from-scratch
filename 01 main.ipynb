{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6c38b",
   "metadata": {},
   "source": [
    "### 1. TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35d5c2",
   "metadata": {},
   "source": [
    "#### _1.1 Load text file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bbc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab034f1",
   "metadata": {},
   "source": [
    "#### _1.2 RE tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c072e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', '', ' ', 'what', \"'\", 's', ' ', 'good', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample = \"Hey, what's good?\"\n",
    "result = re.split(r'([,.:;?!\"()\\'/]|--|\\s)', sample)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d13985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'what', \"'\", 's', 'good', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # retunrs false for whitespaces / no spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138869",
   "metadata": {},
   "source": [
    "Now, apply RE tokenizer to main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5277b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4654\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([.,:;?!\"()\\'/]|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.split()]\n",
    "\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51066469",
   "metadata": {},
   "source": [
    "#### _1.3 Token ID creation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 1139\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"Length of vocabulary: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6806f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for t, i in vocab.items():\n",
    "    print(t, i)\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41849",
   "metadata": {},
   "source": [
    "#### _1.4 Tokenizer class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "s2i = tokenizer.encode(\"I HAD always thought.\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15042de9",
   "metadata": {},
   "source": [
    "#### _1.5 Special Context Tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5422a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary with special context tokens: 1141\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(f\"Length of vocabulary with special context tokens: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8ca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for k, v in enumerate(vocab.keys()):\n",
    "    keys.append(v)\n",
    "print(keys[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c6a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        # if item not in vocab, replace it with <|unk|> token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        preprocessed.append(\"<|endoftext|>\")\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b72c372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, how are you doing? <|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "s2i = tokenizer.encode(\"Hello, how are you doing?\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fbc60",
   "metadata": {},
   "source": [
    "#### _1.6 Byte Pair Encoding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87e9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84994a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2bf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 13778, 2763, 13, 220, 50256, 10928, 345, 588, 257, 6508, 1659, 660, 64, 30]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, Ilham. <|endoftext|> Would you like a cupoftea?\"\n",
    "integers = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b08a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Ilham. <|endoftext|> Would you like a cupoftea?\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe5f90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151db66",
   "metadata": {},
   "source": [
    "### 2. INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826950f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens from byte pair encoding: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens from byte pair encoding:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b564426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [40, 367, 2885, 1464, 1807]\n",
      "y:     [367, 2885, 1464, 1807, 3619]\n"
     ]
    }
   ],
   "source": [
    "context_size = 5 # input will have 5 tokens\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size + 1]\n",
    "print(f\"X: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991f59e",
   "metadata": {},
   "source": [
    "#### _2.1 Using Dataloader_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c2795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride): # max_len is context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - max_len, stride):\n",
    "            input_chunk = token_ids[i:i + max_len]\n",
    "            target_chunk = token_ids[i + 1:i + max_len + 1]\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "    \n",
    "    # the below 2 methods is required for Dataloader to be used\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # we are basically saying that if the input is the 50th tensor, then the output is the 50th tensor\n",
    "        return (\n",
    "            torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            torch.tensor(self.target_ids[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291c8d7",
   "metadata": {},
   "source": [
    "The idea is to form something like as follows:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[5, 6, 7, 8],<br>\n",
    "[9, 10, 11, 12]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[6, 7, 8, 9],<br>\n",
    "[10, 11, 12, 13]]<br><br>\n",
    "...where the first matrix is X and the second matrix is y. Note that in the above example, the stride as well as the max length is 4. If the stride was 2:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[3, 4, 5, 6],<br>\n",
    "[5, 6, 7, 8]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[4, 5, 6, 7],<br>\n",
    "[6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b7d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    # drop last if last tensor is shorter than max_len\n",
    "    # batch size is the number of training ip-op data pairs to be used for training by whcih the parameters are updated\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91f4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464, 12556,  4062,   602]]), tensor([[12556,  4062,   602,   290]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_len = 4, stride = 1, shuffle = False) # looking into how the function will work\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00fb31",
   "metadata": {},
   "source": [
    "Using a batch size of 1 is not preferred as this leads to noisy updates, even though good for memory.<br>\n",
    "Note that a higher overlap (lower stride) can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147d962",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51c802",
   "metadata": {},
   "source": [
    "### 3. VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60811e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3 # embedding dimention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # intialize mebedding matrix randomly\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2974b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e240f8",
   "metadata": {},
   "source": [
    "The embedding weight matrix is basically used for lookup operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f791e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids)) # looking up vector embeddings for the sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb66cb",
   "metadata": {},
   "source": [
    "Note that this is essencially a one hot encoded represenation of the input IDs passed into a linear layer to get the output embeddings where the weights of the neural net are randomly initialized. But we dom't use this because it's not efficient due to the sparsity of the one hot encoded input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample two\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f16f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size = 8, max_len = max_len,\n",
    "    stride = max_len, shuffle = False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ee1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  464, 12556,  4062,   602],\n",
      "        [  290, 31421, 15120,   198],\n",
      "        [  198,  2215,   530,  3073],\n",
      "        [  379,   262, 22942,   286],\n",
      "        [19773,  1081,   361, 22982],\n",
      "        [24411,    11,   340,  4329],\n",
      "        [ 1598,   326,   465,  3108],\n",
      "        [  656,   262,   995,   286]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a3873",
   "metadata": {},
   "source": [
    "This is basically a batch of 8 with 4 tokens each, and each token is converted to a vector of dimention 256. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b54ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967bad5",
   "metadata": {},
   "source": [
    "### 4. POSITIONAL EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16957c",
   "metadata": {},
   "source": [
    "Now we create positional embedding the same way as we did for the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80d41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_len = max_len # 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_len, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_len)) # arange creates ids from 0 to max_len - 1 and pos_embedding_layer converts them to embedding matrix where each row corresponds to the positional embedding for that position id\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36c780",
   "metadata": {},
   "source": [
    "So the first vector embedding of a 4 token sentence will always be added by the vector [1.7375, -0.5620, ..., 1.0345]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9485424",
   "metadata": {},
   "source": [
    "It must also be noted that each row will have the same set of positional embedding values. In other words, the PE value repeats for each row. So the final embedding matrix will only be 4x256 and not 8x4x256. We only care about the position in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a685f7",
   "metadata": {},
   "source": [
    "We can directly add the token and position embeddings, even though the dimentions don't match exactly via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83f6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22a91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8b1b2",
   "metadata": {},
   "source": [
    "### 5. SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99401b4a",
   "metadata": {},
   "source": [
    "We know that attention scores are calculated by taking the dot product between the query token and all the other input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4c4b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # let the query token be journey\n",
    "\n",
    "attention_scores_x_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_x_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attention_scores_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec621eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b35b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# now normalize the scores\n",
    "\n",
    "attention_weights_x_2 = attention_scores_x_2 / attention_scores_x_2.sum()\n",
    "print(attention_weights_x_2)\n",
    "print(attention_weights_x_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f000ce",
   "metadata": {},
   "source": [
    "Note that attention __scores__ are not normalized, but attention __weights__ are, and they sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff9c1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# softmax normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
    "\n",
    "attention_weights_x_2_naive_sm = softmax_naive(attention_scores_x_2) # sm: softmax\n",
    "print(attention_weights_x_2_naive_sm)\n",
    "print(attention_weights_x_2_naive_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6e463",
   "metadata": {},
   "source": [
    "PyTorch implementation of Softmax is preffered to control instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "433ce8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# pytorch softmax operation\n",
    "\n",
    "attention_weights_x_2_pt_sm = torch.softmax(attention_scores_x_2, dim = 0) # pt: pytorch\n",
    "print(attention_weights_x_2_pt_sm)\n",
    "print(attention_weights_x_2_pt_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8713a1",
   "metadata": {},
   "source": [
    "#### _5.1 Context vector calculation for 'journey'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c95e352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "context_vector_x2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_x2 += attention_weights_x_2_pt_sm[i] * x_i\n",
    "\n",
    "print(context_vector_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af65b2",
   "metadata": {},
   "source": [
    "#### _5.2 Calculate attention matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09b7c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44d1f3",
   "metadata": {},
   "source": [
    "This can be done using 2 for loops but that's computationally very expensive. Rather, we can do the above transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd949977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim = -1) \n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080d6e",
   "metadata": {},
   "source": [
    "Setting dimention to -1 means it will normalize accross the columns. This is because the matrix dimention is n_row x n_col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53a8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vectors calculation (z_i)\n",
    "\n",
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb33fa7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a780a1",
   "metadata": {},
   "source": [
    "### 6. SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27e2b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb2fa1",
   "metadata": {},
   "source": [
    "Now we randomly initialize W_q, W_k & W_v. Each of them will have dimentiones were the number of row count will be eqaul to the input vector dimention (column count of input matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44f666e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be working with the sample word 'journey' again\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # this will be the number of columns in the key, quey and value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c259d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# set requires_grad to True later for model training\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "print(W_q)\n",
    "print(W_k)\n",
    "print(W_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff8bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the query, key and value for the sample input word 'journey'\n",
    "\n",
    "q_2 = x_2 @ W_q\n",
    "k_2 = x_2 @ W_k\n",
    "v_2 = x_2 @ W_v\n",
    "\n",
    "print(q_2)\n",
    "print(k_2)\n",
    "print(v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d166cc5",
   "metadata": {},
   "source": [
    "Note that conventionally, just like how things were implemented in section 5, the output from these dot product operations must have the same dimention as the input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e18afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]])\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "# get the overall query, key and value\n",
    "\n",
    "query = inputs @ W_q\n",
    "key = inputs @ W_k\n",
    "value = inputs @ W_v\n",
    "\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38158409",
   "metadata": {},
   "source": [
    "Now we compute the attention scores. In self attention, this is essencially the dot product between the query and the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2b0bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# for 'journey':\n",
    "\n",
    "query_2 = query[1]\n",
    "key_2 = key[1]\n",
    "\n",
    "attention_scores_2 = query_2 @ key.T\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1ce3e",
   "metadata": {},
   "source": [
    "This is basically saying how much the word __journey__ attends to all the other words. Obviously, this will be highest for the second word (itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7406e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "# overall attention\n",
    "\n",
    "attention_scores = query @ key.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ea712",
   "metadata": {},
   "source": [
    "For now, these don't mean anything because they are not trained. Next, we normalize these scores. We normalize by first scaling the scores by square root of d_out or embedding dimention of each word of the key matrix (number of columns). Next, we apply softmax over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77eeecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# normalize to get attention weights (this is just for 'journey')\n",
    "\n",
    "d_k = key.shape[1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61831e12",
   "metadata": {},
   "source": [
    "Why take square root? Multiply any 2 numbers (here, we are multiplying the key and query) increases the variance. So to stabilize it back, we take the root. Another reason is for bringing stability to the softmax outputs and to have an even distribution. If not, the scores can get overly confident for a single input word. (Refer lecture 15, 46th minute for more detail).<br><br>\n",
    "This is why self attention is also called __sclaed dot product attention__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ae49dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "# get attention weights for enitre sentence\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42196d7a",
   "metadata": {},
   "source": [
    "These attentions weights are now multiplied with the _value_ matrix to get the __context vectors__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4251f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "context = attention_weights @ value\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068759",
   "metadata": {},
   "source": [
    "Now we make a self attention calss for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d68f1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_k\n",
    "        queries = x @ self.W_q\n",
    "        values = x @ self.W_v\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80c39c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "378ea687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, which is more optimized due to the Linear class from PyTorch\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    \n",
    "    # change here compared to v1\n",
    "    def forward(self, x): \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c61440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29b258",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec3580",
   "metadata": {},
   "source": [
    "### 7. CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8e047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55681081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f16ae",
   "metadata": {},
   "source": [
    "Now we apply causal masking using tril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "001d9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_len = attention_weights.shape[0]\n",
    "mask_sample = torch.tril(torch.ones(context_len, context_len))\n",
    "print(mask_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d705d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attention = attention_weights * mask_sample\n",
    "print(masked_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788ce1",
   "metadata": {},
   "source": [
    "Now perform renormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bfb33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_attention.sum(dim = 1, keepdim = True)\n",
    "masked_attention_norm = masked_attention / row_sums\n",
    "print(masked_attention_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886285d",
   "metadata": {},
   "source": [
    "However, there is a problem with this method. Even though we have performed masking, the attention scores after applying softmax leads to data leakag due to data redistribution which occurs based on future values as well. A simple solution is to perform masking over the attention scores, and then perform softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9660fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf) # this basically takes the attention scores matrix, looks at positions where the value is True, and gves it -ve inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61290adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aae77",
   "metadata": {},
   "source": [
    "_(not exactly correct it seems)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412bba0",
   "metadata": {},
   "source": [
    "Dropout is applied here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d13e1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f273f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cba8",
   "metadata": {},
   "source": [
    "Rescaling based on droupout percentage also occurs.<br>\n",
    "Next, we also introduce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c2196b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch)\n",
    "print()\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72632a48",
   "metadata": {},
   "source": [
    "Think of it as 2 input matrices. One sentence can be \"your journey starts with one step\" and the other can be \"my name is mohammed asif sahadh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1eca4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # new\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal = 1)) # new (register buffer i think ensures that the non trainable stuff will be moved to appropriate device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # new batch dim b\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # one coma two because we don't need to transpose the batch dimention (idx 0)\n",
    "        attention_scores.masked_fill_( # _ ops makes it inplace\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # :num_tokens is to ensure if the sequence is less than the context length\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights) # new      \n",
    " \n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "422bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_len, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print()\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f200a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54ab2",
   "metadata": {},
   "source": [
    "### 8. MULTI HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1552de7",
   "metadata": {},
   "source": [
    "For MHA, we simply have to create a wrapper for causal attention that stacks the outputs of multiple of thier outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eed8cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( # creates an instance of causal attention class\n",
    "            [CausalAttention(d_in, d_out, context_len, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f655078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "391277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e0cbd3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = inputs.shape[0]\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_len, 0.0, 3)\n",
    "mha.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b17cb",
   "metadata": {},
   "source": [
    "_(output matches here though)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b194d7",
   "metadata": {},
   "source": [
    "To solve the inefficiency casued by performing matrix multiplations over multiple head, we implement weight splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e319612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_out = x.shape # s1\n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # s5\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights) # s8\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2) # s9 & s10\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # s11\n",
    "        context_vec = self.out_proj(context_vec) # optional\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a43c13",
   "metadata": {},
   "source": [
    "s1 to s11 are steps to implement MHA, in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "87274ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]],\n",
      "\n",
      "        [[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.90, 0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)\n",
    "\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02e987",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39e8b5",
   "metadata": {},
   "source": [
    "### 9. IMPLEMENTING GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c5f2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2425",
   "metadata": {},
   "source": [
    "#### _9.1 Dummy class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f92aa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # this is the entire lookup matrix to get the embedding for a token\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # this depends on context length of course (also a lookup table)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # make a placeholder for transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )        \n",
    "        \n",
    "        # make a placeholder for layernorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx): # in_idx is a batch of input tokens\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device)) # arange creates ids from 0 to max_len - 1 \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) # lots of things happen here\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7af49492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token embeddings:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "tensor([[[-5.4645e-03, -7.1054e-01, -1.0149e+00,  ..., -4.1823e-01,\n",
      "           2.2412e-02, -1.0233e+00],\n",
      "         [ 1.7904e+00, -3.2025e-03, -1.1346e-01,  ..., -6.4449e-01,\n",
      "           6.3170e-01,  3.2810e+00],\n",
      "         [ 4.1122e-01, -2.4297e-01, -3.9615e-01,  ...,  3.1840e-01,\n",
      "           5.1082e-01, -7.7559e-02],\n",
      "         [-8.1073e-01, -4.5008e-01, -8.1223e-01,  ...,  1.2990e+00,\n",
      "           3.7404e-01,  9.0462e-02]],\n",
      "\n",
      "        [[ 2.6268e-01, -7.7104e-01, -1.4651e+00,  ..., -6.4443e-01,\n",
      "          -4.4353e-01, -9.2719e-01],\n",
      "         [ 1.1406e+00, -3.4269e-01, -6.9491e-01,  ..., -1.7102e-01,\n",
      "           6.8366e-01,  2.0607e+00],\n",
      "         [ 1.1106e+00,  1.1755e+00, -6.2576e-01,  ..., -4.4802e-01,\n",
      "           2.6767e-02, -9.2523e-01],\n",
      "         [-3.8614e-01,  3.0415e-01, -9.1265e-01,  ...,  1.8638e+00,\n",
      "           6.4222e-01,  2.9281e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dummy sample\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim = 0)\n",
    "print(\"Input token embeddings:\")\n",
    "print(batch)\n",
    "print()\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Logits:\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39f60",
   "metadata": {},
   "source": [
    "#### _9.2 Layernorm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f850829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[-1.4771,  0.3162, -0.1837, -0.8028, -1.2379],\n",
      "        [-1.2232,  0.3065, -0.4733, -0.1332, -0.5370]])\n",
      "\n",
      "Output:\n",
      "tensor([[0.0000, 0.2986, 0.2986, 0.5947, 1.2578, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3003, 0.4595, 0.9096, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple feedforward\n",
    "batch_sample = torch.randn(2, 5)\n",
    "print(\"Input:\")\n",
    "print(batch_sample)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_sample)\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "181ab561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4083],\n",
      "        [0.2782]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.2228],\n",
      "        [0.1328]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim = -1, keepdim = True)\n",
    "var = out.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e1ffc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8650, -0.2323, -0.2324,  0.3950,  1.7998, -0.8650],\n",
      "        [-0.7634, -0.7634,  0.0605,  0.4974,  1.7322, -0.7634]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layernorm = (out - mean) / (var ** 0.5)\n",
    "print(layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c6e7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4769e-08],\n",
      "        [ 4.4703e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = layernorm.mean(dim = -1, keepdim = True)\n",
    "var = layernorm.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a38ab2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "011ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2219e-07],\n",
      "        [-9.5367e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_sample)\n",
    "mean = out_ln.mean(dim = -1, keepdim = True)\n",
    "var = out_ln.var(dim = -1, keepdim = True, unbiased = False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f45fe",
   "metadata": {},
   "source": [
    "#### _9.3 GELU_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d37162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a23c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b0ce1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afb9f8",
   "metadata": {},
   "source": [
    "#### _9.4 Skip Connections_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30c63ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut): \n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), \n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x) # output of linear layer x\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87eab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_wo_shortut = ExampleDNN(layer_sizes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "59b36e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grads(model, x):\n",
    "    output = model(x) # normal output from nn\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "794b7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_wo_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cace1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_w_shortut = ExampleDNN(layer_sizes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1291623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_w_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def0a06",
   "metadata": {},
   "source": [
    "Gradient vanishing reduced..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a05c5",
   "metadata": {},
   "source": [
    "#### _9.5 Coding Attention & Linear Layers in a Transformer Block_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5cf83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733201",
   "metadata": {},
   "source": [
    "Requirements for building the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50ef554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4ee8ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # MHA\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # shape: [batch size, num tokens, emb size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        # FCL\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cadd4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[0.2961, 0.5166, 0.2517,  ..., 0.9541, 0.8567, 0.4604],\n",
      "         [0.2238, 0.3047, 0.3019,  ..., 0.5465, 0.4532, 0.7598],\n",
      "         [0.6945, 0.2478, 0.4111,  ..., 0.8838, 0.4898, 0.5963],\n",
      "         [0.0890, 0.7804, 0.9223,  ..., 0.4507, 0.6357, 0.5833]],\n",
      "\n",
      "        [[0.5716, 0.9297, 0.3396,  ..., 0.0477, 0.4564, 0.2797],\n",
      "         [0.0936, 0.2211, 0.3806,  ..., 0.3948, 0.4545, 0.4536],\n",
      "         [0.6788, 0.1741, 0.2084,  ..., 0.5557, 0.5930, 0.0959],\n",
      "         [0.3894, 0.4083, 0.0662,  ..., 0.9861, 0.9341, 0.1319]]])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Output:\n",
      "tensor([[[-0.0055,  0.0972, -0.1122,  ...,  1.2889,  0.2623,  0.6685],\n",
      "         [ 0.0023, -0.2369,  0.1720,  ...,  0.5952,  0.2497,  0.7447],\n",
      "         [ 0.4673,  0.4472,  0.1791,  ...,  1.2525,  0.3045,  0.7750],\n",
      "         [ 0.0662,  0.7224,  0.9206,  ...,  0.4790,  0.7428,  0.7015]],\n",
      "\n",
      "        [[ 0.3622,  1.2144,  0.5221,  ...,  0.1854,  0.0111, -0.5034],\n",
      "         [-0.0225,  0.7789,  0.2770,  ...,  0.1734,  0.5419,  0.1143],\n",
      "         [ 0.7425,  0.4013,  0.3211,  ...,  0.3268,  0.7523, -0.1642],\n",
      "         [ 0.5745,  0.6241,  0.4410,  ...,  1.1963,  1.2650,  0.2243]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24eb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896df3d9",
   "metadata": {},
   "source": [
    "### 10. CODING GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "548d0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b2055",
   "metadata": {},
   "source": [
    "#### _10.1 The GPT Class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74dea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_len\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx): # input batch\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "33845b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output batch:\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\")\n",
    "print(batch)\n",
    "print(\"Output batch:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "71656dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e817112",
   "metadata": {},
   "source": [
    "_Note: We are not using weight tying, which was performed in the original GPT-2 model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c9e73",
   "metadata": {},
   "source": [
    "#### _10.2 Generate Text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9f72ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx is the input batch\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop current context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # get predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # batch_size x tokens_num x vocab_size\n",
    "        # get the last time step (last set of logits)\n",
    "        logits = logits[:, -1, :]\n",
    "        # apply softmax\n",
    "        probs = torch.softmax(logits, dim = -1)\n",
    "        # get id of max\n",
    "        idx_next = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "        # append id to running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c37518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 314, 716, 407, 1016, 284, 4483, 11311]\n",
      "Encoded tensor: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311]])\n",
      "Encoded tensor shape: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am not going to eat chocolate\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded tensor: {encoded_tensor}\")\n",
    "print(f\"Encoded tensor shape: {encoded_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "50371a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311, 20656,\n",
      "         30719, 44035, 23338, 24151, 10835, 42731, 35799, 46215,   371]])\n",
      "Output shape: 19\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Output shape: {len(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d9b986d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am not going to eat chocolate AmbassadorOptional touredGirl malesGradorea R\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4bf9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68ec82",
   "metadata": {},
   "source": [
    "### 11. The LLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "17e66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 512, # can be reduced for training simplicity\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88e7cef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d000af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "Every effort moves youublishedstein municipal sushi abnormal Payment contemporths politely\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "\n",
    "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "41ebb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feet wet\n",
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\"]\n",
    "                       [40, 1107, 588]])     # [\"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],   # [\" effort moves you\"]\n",
    "                        [1107, 588, 11311]]) # [\" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50fea205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[2.9209e-05, 1.0527e-05, 1.6121e-05,  ..., 2.6603e-05,\n",
      "          2.6384e-05, 4.3995e-05],\n",
      "         [2.6438e-05, 9.8570e-06, 1.3430e-05,  ..., 7.6308e-06,\n",
      "          1.2208e-05, 4.2206e-05],\n",
      "         [2.0652e-05, 3.0963e-05, 7.3546e-05,  ..., 3.5792e-05,\n",
      "          1.5129e-05, 1.7849e-05]],\n",
      "\n",
      "        [[9.4693e-06, 2.0345e-05, 1.8425e-05,  ..., 8.6429e-06,\n",
      "          1.5247e-05, 3.9934e-05],\n",
      "         [2.3645e-05, 8.8373e-06, 2.1052e-05,  ..., 7.2422e-06,\n",
      "          1.4043e-05, 1.4942e-05],\n",
      "         [1.0545e-05, 1.4763e-05, 2.7852e-05,  ..., 2.5073e-05,\n",
      "          1.3072e-05, 3.0611e-05]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim = -1)\n",
    "print(probs.shape)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e751eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[40822],\n",
      "         [27614],\n",
      "         [20921]],\n",
      "\n",
      "        [[ 8807],\n",
      "         [34094],\n",
      "         [45958]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b7781e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  effort moves you\n",
      "Predicted: ovychrequisite Buddh\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Predicted: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe710892",
   "metadata": {},
   "source": [
    "#### _11.1 Cross Entropy Loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2790cdf",
   "metadata": {},
   "source": [
    "Basically find the target probabilities of the target ids. We need to get these as close to 1 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3a599343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3003e-05, 1.7775e-05, 1.4705e-05])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e579b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1463e-05, 1.5585e-05, 8.4811e-06])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5a962f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.6723, -10.9377, -11.1273, -11.3764, -11.0692, -11.6777])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "be67a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.9768)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e02985cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9768)\n"
     ]
    }
   ],
   "source": [
    "neg_log_likelihood = -1 * avg_log_probs\n",
    "print(neg_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6eaf5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler method\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "# above, we have merged the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6aab6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9768)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91714488",
   "metadata": {},
   "source": [
    "#### _11.2 Perplexity_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cea0b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58498.9844)\n"
     ]
    }
   ],
   "source": [
    "perp = torch.exp(loss)\n",
    "print(perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ece60",
   "metadata": {},
   "source": [
    "That's horrible...for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d066a05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dec17",
   "metadata": {},
   "source": [
    "### 12. INITIAL ERROR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c34ddffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 110806\n",
      "The Early Foundations and Academic Journey\n",
      "\n",
      "When one looks at the trajectory of Mohammed Asif Sahad\n"
     ]
    }
   ],
   "source": [
    "with open(\"me.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b38052cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 22066\n"
     ]
    }
   ],
   "source": [
    "total_tokens = len(tokenizer.encode(raw_text))\n",
    "print(f\"Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4f4810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = True,\n",
    "    shuffle = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "865bda38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens * (1 - train_ratio) < GPT_CONFIG_124M[\"context_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c291830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "\n",
      "Test Loader:\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print()\n",
    "print(\"Test Loader:\")\n",
    "for x, y in test_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8b12ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # this does all the softmax & everything\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None): # this will show the loss of the LM\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches # mean loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "081e60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cac25d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.974997771413703\n",
      "Testing loss: 10.976041316986084\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): # disable for now\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Testing loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eff95",
   "metadata": {},
   "source": [
    "### 13. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0fdb1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function\n",
    "def evaluate_model(model, train_loader, test_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches = eval_iter)\n",
    "        test_loss = calc_loss_loader(test_loader, model, device, num_batches = eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4ed39a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see text generation during training\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model, idx = encoded, max_new_tokens = 50, context_size = context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    print()\n",
    "    model.train() # set it back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2aa2c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, test_loader, optimizer, device, \n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients from previous batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel() # return number of tokens seen\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation (optional)\n",
    "            if global_step % eval_freq == 0: # only after a set of batches is used for training\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_loader, test_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch + 1} (Step {global_step:04d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Test loss {test_loss:.3f}\")\n",
    "            \n",
    "        # print sample text from each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        \n",
    "    return train_losses, test_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db83d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 0000): Train loss 9.729, Test loss 9.769\n",
      "Epoch 1 (Step 0005): Train loss 7.952, Test loss 7.916\n",
      "Epoch 1 (Step 0010): Train loss 6.682, Test loss 6.568\n",
      "Epoch 1 (Step 0015): Train loss 6.008, Test loss 6.002\n",
      "Asif likes to the a the to the a to the a of a the a of a to the a.                               \n",
      "\n",
      "Epoch 2 (Step 0020): Train loss 5.790, Test loss 5.883\n",
      "Epoch 2 (Step 0025): Train loss 5.445, Test loss 5.693\n",
      "Epoch 2 (Step 0030): Train loss 4.748, Test loss 5.331\n",
      "Epoch 2 (Step 0035): Train loss 4.392, Test loss 5.074\n",
      "Asif likes to the to the, and the to the, and the, and the of AI, and his ability to the to the to the, and the to the to an of AI, and the to the to.        \n",
      "\n",
      "Epoch 3 (Step 0040): Train loss 4.214, Test loss 4.748\n",
      "Epoch 3 (Step 0045): Train loss 3.791, Test loss 4.406\n",
      "Epoch 3 (Step 0050): Train loss 3.623, Test loss 4.200\n",
      "Epoch 3 (Step 0055): Train loss 3.084, Test loss 4.023\n",
      "Asif likes to the time, and the ability to code.                                         \n",
      "\n",
      "Epoch 4 (Step 0060): Train loss 2.891, Test loss 3.799\n",
      "Epoch 4 (Step 0065): Train loss 2.662, Test loss 3.724\n",
      "Epoch 4 (Step 0070): Train loss 2.153, Test loss 3.461\n",
      "Epoch 4 (Step 0075): Train loss 1.954, Test loss 3.150\n",
      "Asif likes to the time at VIT, and the world of a practitioner of a practitioner, and the best, and the Machine Learning Team.                        \n",
      "\n",
      "Epoch 5 (Step 0080): Train loss 1.898, Test loss 2.898\n",
      "Epoch 5 (Step 0085): Train loss 1.589, Test loss 2.755\n",
      "Epoch 5 (Step 0090): Train loss 1.562, Test loss 2.780\n",
      "Asif likes to the time of AI.       Asifsolving, Asifsolving, and the surface of his ability to the time Asifsolving. He was not only a project development.\n",
      "\n",
      "Epoch 6 (Step 0095): Train loss 1.299, Test loss 2.790\n",
      "Epoch 6 (Step 0100): Train loss 1.186, Test loss 2.840\n",
      "Epoch 6 (Step 0105): Train loss 1.077, Test loss 2.717\n",
      "Epoch 6 (Step 0110): Train loss 0.874, Test loss 2.696\n",
      "Asif likes to the competing demands of AI, and the tools for global tech hubs but as an early age, Asifsolving. He developed reliable agentic frameworks and the time Asifs academic and the field of AI today.  \n",
      "\n",
      "Epoch 7 (Step 0115): Train loss 0.824, Test loss 2.706\n",
      "Epoch 7 (Step 0120): Train loss 0.628, Test loss 2.690\n",
      "Epoch 7 (Step 0125): Train loss 0.630, Test loss 2.653\n",
      "Epoch 7 (Step 0130): Train loss 0.578, Test loss 2.735\n",
      "Asif likes to the example of AI.    Living in a Righteous Society  Equally significant is Asifs aspiration to live within a Muslim society, and the result of his Bachelors degreewith a respectable CGPA\n",
      "\n",
      "Epoch 8 (Step 0135): Train loss 0.545, Test loss 2.902\n",
      "Epoch 8 (Step 0140): Train loss 0.444, Test loss 2.886\n",
      "Epoch 8 (Step 0145): Train loss 0.377, Test loss 2.840\n",
      "Epoch 8 (Step 0150): Train loss 0.330, Test loss 2.795\n",
      "Asif likes to his ambitionis his identity as a Muslim, his attachment to both lead and his aspiration to live as a man of balance: physically strong, spiritually grounded, and socially responsible.    But Asifs professional experiences were not\n",
      "\n",
      "Epoch 9 (Step 0155): Train loss 0.321, Test loss 2.828\n",
      "Epoch 9 (Step 0160): Train loss 0.314, Test loss 2.990\n",
      "Epoch 9 (Step 0165): Train loss 0.263, Test loss 2.959\n",
      "Epoch 9 (Step 0170): Train loss 0.228, Test loss 2.972\n",
      "Asif likes to the example of generations before him.  Living in a Righteous Society  Equally significant is Asifs aspiration to live within a Muslim society, in a country and community where faith, tradition, and social norms align with his\n",
      "\n",
      "Epoch 10 (Step 0175): Train loss 0.212, Test loss 2.966\n",
      "Epoch 10 (Step 0180): Train loss 0.197, Test loss 2.955\n",
      "Epoch 10 (Step 0185): Train loss 0.188, Test loss 2.952\n",
      "Asif likes to a mobile application presentation to an audience of over twenty team members. In essence, this project is a form of self-directed research, one that mimics the rigor of graduate-level experimentation and could, in the future, evolve into publishable\n",
      "\n",
      "Training completed in 1.34 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, test_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, test_loader, optimizer, device, \n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5, # after every 5 batches, training and validation loss will be printed\n",
    "    start_context = \"Asif likes to\", tokenizer = tokenizer\n",
    ") \n",
    "\n",
    "end = time.time()\n",
    "training_time = (end - start) / 60\n",
    "print(f\"Training completed in {training_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a9c87ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 6144, 11264, 16384, 21504, 26624, 31744, 36864, 41984, 47104, 52224, 57344, 62464, 67584, 72704, 77824, 82944, 88064]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_seen) # number of tokens seen by the model at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e03c1a",
   "metadata": {},
   "source": [
    "Awesome, but there is some overfitting. This means the model is taking text directly from the book...word to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e4c17d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (7, 4))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, label = 'Training Loss', color='#2E86AB', linewidth = 1)\n",
    "plt.plot(epochs, test_losses, label = 'Testing Loss', color='#F24236', linewidth = 1, linestyle='--')\n",
    "\n",
    "plt.title(\"Training & Testing Loss\", fontsize = 14, pad = 10)\n",
    "plt.xlabel(\"Epoch\", fontsize = 10)\n",
    "plt.ylabel(\"Loss\", fontsize = 10)\n",
    "\n",
    "plt.legend(frameon = True, fancybox = True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef895f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d367a73",
   "metadata": {},
   "source": [
    "### 14. LLM DECODING STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ba413d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae7d8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling + top-k sampling\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature = 0.0, top_k = None, eos_id = None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "        \n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "        \n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c3196",
   "metadata": {},
   "source": [
    "Time to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7a7c2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Asif likes to engage deploymenting, resource management into the notoriously difficult challenge of learning, but also ensured his best form of two, and in optimizing its deeply versatile.\n",
      "\n",
      "It was him, he emphasized a mosaic of architecture (HIT, an agentic Systems that could evaluate contribution with market and howsolving engineering\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(246)\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model, \n",
    "    idx = text_to_token_ids(\"Asif likes to\", tokenizer).to(device),\n",
    "    max_new_tokens = 65,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"],\n",
    "    top_k = 25,\n",
    "    temperature = 1.4\n",
    ")\n",
    "\n",
    "print(\"Output:\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d74b19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ef147",
   "metadata": {},
   "source": [
    "### 15. SAVING & LOADING MODEL WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "94b8fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0cfe646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19280a6",
   "metadata": {},
   "source": [
    "It is also required to store the optimizer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35fa3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d788fb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-4, weight_decay = 0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0309510",
   "metadata": {},
   "source": [
    "_Next: Lecture 32_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
