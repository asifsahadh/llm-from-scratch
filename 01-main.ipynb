{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6c38b",
   "metadata": {},
   "source": [
    "### 1. TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35d5c2",
   "metadata": {},
   "source": [
    "#### _1.1 Load text file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24bbc17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab034f1",
   "metadata": {},
   "source": [
    "#### _1.2 RE tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c072e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', '', ' ', 'what', \"'\", 's', ' ', 'good', '?', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample = \"Hey, what's good?\"\n",
    "result = re.split(r'([,.:;?!\"()\\'/]|--|\\s)', sample)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d13985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', ',', 'what', \"'\", 's', 'good', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # retunrs false for whitespaces / no spaces\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03138869",
   "metadata": {},
   "source": [
    "Now, apply RE tokenizer to main text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5277b7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 4654\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([.,:;?!\"()\\'/]|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item.split()]\n",
    "\n",
    "print(f\"Number of tokens: {len(preprocessed)}\")\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51066469",
   "metadata": {},
   "source": [
    "#### _1.3 Token ID creation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3c8005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary: 1139\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"Length of vocabulary: {len(all_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6806f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! 0\n",
      "\" 1\n",
      "' 2\n",
      "( 3\n",
      ") 4\n",
      ", 5\n",
      "-- 6\n",
      ". 7\n",
      ": 8\n",
      "; 9\n",
      "? 10\n",
      "A 11\n",
      "Ah 12\n",
      "Among 13\n",
      "And 14\n",
      "Are 15\n",
      "Arrt 16\n",
      "As 17\n",
      "At 18\n",
      "Be 19\n",
      "Begin 20\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for t, i in vocab.items():\n",
    "    print(t, i)\n",
    "    if i >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f41849",
   "metadata": {},
   "source": [
    "#### _1.4 Tokenizer class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c814a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07da86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "s2i = tokenizer.encode(\"I HAD always thought.\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15042de9",
   "metadata": {},
   "source": [
    "#### _1.5 Special Context Tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5422a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary with special context tokens: 1141\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(f\"Length of vocabulary with special context tokens: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8ca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "for k, v in enumerate(vocab.keys()):\n",
    "    keys.append(v)\n",
    "print(keys[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c6a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\'/]|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip() # remove white spaces\n",
    "        ]\n",
    "        # if item not in vocab, replace it with <|unk|> token\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        preprocessed.append(\"<|endoftext|>\")\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids) # int_to_str gives back the text in a list. \" \".join joins them into a normal sentence\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # fixes space before punctuations\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b72c372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, how are you doing? <|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "s2i = tokenizer.encode(\"Hello, how are you doing?\")\n",
    "tokenizer.decode(s2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fbc60",
   "metadata": {},
   "source": [
    "#### _1.6 Byte Pair Encoding_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d87e9f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84994a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d2bf5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 13778, 2763, 13, 220, 50256, 10928, 345, 588, 257, 6508, 1659, 660, 64, 30]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, Ilham. <|endoftext|> Would you like a cupoftea?\"\n",
    "integers = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b08a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Ilham. <|endoftext|> Would you like a cupoftea?\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe5f90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151db66",
   "metadata": {},
   "source": [
    "### 2. INPUT-TARGET PAIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "826950f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens from byte pair encoding: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens from byte pair encoding:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b564426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [40, 367, 2885, 1464, 1807]\n",
      "y:     [367, 2885, 1464, 1807, 3619]\n"
     ]
    }
   ],
   "source": [
    "context_size = 5 # input will have 5 tokens\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size + 1]\n",
    "print(f\"X: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991f59e",
   "metadata": {},
   "source": [
    "#### _2.1 Using Dataloader_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c2795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride): # max_len is context size\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        # sliding window to create overlapping sequences\n",
    "        for i in range(0, len(token_ids) - max_len, stride):\n",
    "            input_chunk = token_ids[i:i + max_len]\n",
    "            target_chunk = token_ids[i + 1:i + max_len + 1]\n",
    "            self.input_ids.append(input_chunk)\n",
    "            self.target_ids.append(target_chunk)\n",
    "    \n",
    "    # the below 2 methods is required for Dataloader to be used\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): # we are basically saying that if the input is the 50th tensor, then the output is the 50th tensor\n",
    "        return (\n",
    "            torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            torch.tensor(self.target_ids[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291c8d7",
   "metadata": {},
   "source": [
    "The idea is to form something like as follows:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[5, 6, 7, 8],<br>\n",
    "[9, 10, 11, 12]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[6, 7, 8, 9],<br>\n",
    "[10, 11, 12, 13]]<br><br>\n",
    "...where the first matrix is X and the second matrix is y. Note that in the above example, the stride as well as the max length is 4. If the stride was 2:<br><br>\n",
    "[[1, 2, 3, 4],<br>\n",
    "[3, 4, 5, 6],<br>\n",
    "[5, 6, 7, 8]]<br><br>\n",
    "[[2, 3, 4, 5],<br>\n",
    "[4, 5, 6, 7],<br>\n",
    "[6, 7, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b7d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    # drop last if last tensor is shorter than max_len\n",
    "    # batch size is the number of training ip-op data pairs to be used for training by whcih the parameters are updated\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a398d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f91f4208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464, 12556,  4062,   602]]), tensor([[12556,  4062,   602,   290]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_len = 4, stride = 1, shuffle = False) # looking into how the function will work\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00fb31",
   "metadata": {},
   "source": [
    "Using a batch size of 1 is not preferred as this leads to noisy updates, even though good for memory.<br>\n",
    "Note that a higher overlap (lower stride) can lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147d962",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51c802",
   "metadata": {},
   "source": [
    "### 3. VECTOR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60811e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3 # embedding dimention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # intialize mebedding matrix randomly\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2974b366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e240f8",
   "metadata": {},
   "source": [
    "The embedding weight matrix is basically used for lookup operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f791e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids)) # looking up vector embeddings for the sample input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb66cb",
   "metadata": {},
   "source": [
    "Note that this is essencially a one hot encoded represenation of the input IDs passed into a linear layer to get the output embeddings where the weights of the neural net are randomly initialized. But we dom't use this because it's not efficient due to the sparsity of the one hot encoded input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5e6f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample two\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1f16f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size = 8, max_len = max_len,\n",
    "    stride = max_len, shuffle = False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87ee1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  464, 12556,  4062,   602],\n",
      "        [  290, 31421, 15120,   198],\n",
      "        [  198,  2215,   530,  3073],\n",
      "        [  379,   262, 22942,   286],\n",
      "        [19773,  1081,   361, 22982],\n",
      "        [24411,    11,   340,  4329],\n",
      "        [ 1598,   326,   465,  3108],\n",
      "        [  656,   262,   995,   286]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "147640a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15a3873",
   "metadata": {},
   "source": [
    "This is basically a batch of 8 with 4 tokens each, and each token is converted to a vector of dimention 256. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b54ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967bad5",
   "metadata": {},
   "source": [
    "### 4. POSITIONAL EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16957c",
   "metadata": {},
   "source": [
    "Now we create positional embedding the same way as we did for the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c80d41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_len = max_len # 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_len, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_len)) # arange creates ids from 0 to max_len - 1 and pos_embedding_layer converts them to embedding matrix where each row corresponds to the positional embedding for that position id\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36c780",
   "metadata": {},
   "source": [
    "So the first vector embedding of a 4 token sentence will always be added by the vector [1.7375, -0.5620, ..., 1.0345]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9485424",
   "metadata": {},
   "source": [
    "It must also be noted that each row will have the same set of positional embedding values. In other words, the PE value repeats for each row. So the final embedding matrix will only be 4x256 and not 8x4x256. We only care about the position in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a685f7",
   "metadata": {},
   "source": [
    "We can directly add the token and position embeddings, even though the dimentions don't match exactly via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83f6463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d22a91",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8b1b2",
   "metadata": {},
   "source": [
    "### 5. SIMPLIFIED ATTENTION MECHANISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0cb7797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99401b4a",
   "metadata": {},
   "source": [
    "We know that attention scores are calculated by taking the dot product between the query token and all the other input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c4b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # let the query token be journey\n",
    "\n",
    "attention_scores_x_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_x_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attention_scores_x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec621eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b35b4e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# now normalize the scores\n",
    "\n",
    "attention_weights_x_2 = attention_scores_x_2 / attention_scores_x_2.sum()\n",
    "print(attention_weights_x_2)\n",
    "print(attention_weights_x_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f000ce",
   "metadata": {},
   "source": [
    "Note that attention __scores__ are not normalized, but attention __weights__ are, and they sum up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff9c1a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# softmax normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
    "\n",
    "attention_weights_x_2_naive_sm = softmax_naive(attention_scores_x_2) # sm: softmax\n",
    "print(attention_weights_x_2_naive_sm)\n",
    "print(attention_weights_x_2_naive_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6e463",
   "metadata": {},
   "source": [
    "PyTorch implementation of Softmax is preffered to control instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "433ce8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# pytorch softmax operation\n",
    "\n",
    "attention_weights_x_2_pt_sm = torch.softmax(attention_scores_x_2, dim = 0) # pt: pytorch\n",
    "print(attention_weights_x_2_pt_sm)\n",
    "print(attention_weights_x_2_pt_sm.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8713a1",
   "metadata": {},
   "source": [
    "#### _5.1 Context vector calculation for 'journey'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c95e352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "\n",
    "context_vector_x2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vector_x2 += attention_weights_x_2_pt_sm[i] * x_i\n",
    "\n",
    "print(context_vector_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af65b2",
   "metadata": {},
   "source": [
    "#### _5.2 Calculate attention matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09b7c6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad44d1f3",
   "metadata": {},
   "source": [
    "This can be done using 2 for loops but that's computationally very expensive. Rather, we can do the above transpose operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd949977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(attention_scores, dim = -1) \n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080d6e",
   "metadata": {},
   "source": [
    "Setting dimention to -1 means it will normalize accross the columns. This is because the matrix dimention is n_row x n_col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53a8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# context vectors calculation (z_i)\n",
    "\n",
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb33fa7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a780a1",
   "metadata": {},
   "source": [
    "### 6. SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27e2b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb2fa1",
   "metadata": {},
   "source": [
    "Now we randomly initialize W_q, W_k & W_v. Each of them will have dimentiones were the number of row count will be eqaul to the input vector dimention (column count of input matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44f666e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be working with the sample word 'journey' again\n",
    "\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # this will be the number of columns in the key, quey and value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c259d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# set requires_grad to True later for model training\n",
    "W_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "print(W_q)\n",
    "print(W_k)\n",
    "print(W_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ff8bc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# now we calculate the query, key and value for the sample input word 'journey'\n",
    "\n",
    "q_2 = x_2 @ W_q\n",
    "k_2 = x_2 @ W_k\n",
    "v_2 = x_2 @ W_v\n",
    "\n",
    "print(q_2)\n",
    "print(k_2)\n",
    "print(v_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d166cc5",
   "metadata": {},
   "source": [
    "Note that conventionally, just like how things were implemented in section 5, the output from these dot product operations must have the same dimention as the input vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e18afa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]])\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "# get the overall query, key and value\n",
    "\n",
    "query = inputs @ W_q\n",
    "key = inputs @ W_k\n",
    "value = inputs @ W_v\n",
    "\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38158409",
   "metadata": {},
   "source": [
    "Now we compute the attention scores. In self attention, this is essencially the dot product between the query and the key vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2b0bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# for 'journey':\n",
    "\n",
    "query_2 = query[1]\n",
    "key_2 = key[1]\n",
    "\n",
    "attention_scores_2 = query_2 @ key.T\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1ce3e",
   "metadata": {},
   "source": [
    "This is basically saying how much the word __journey__ attends to all the other words. Obviously, this will be highest for the second word (itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7406e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "# overall attention\n",
    "\n",
    "attention_scores = query @ key.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ea712",
   "metadata": {},
   "source": [
    "For now, these don't mean anything because they are not trained. Next, we normalize these scores. We normalize by first scaling the scores by square root of d_out or embedding dimention of each word of the key matrix (number of columns). Next, we apply softmax over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77eeecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "# normalize to get attention weights (this is just for 'journey')\n",
    "\n",
    "d_k = key.shape[1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61831e12",
   "metadata": {},
   "source": [
    "Why take square root? Multiply any 2 numbers (here, we are multiplying the key and query) increases the variance. So to stabilize it back, we take the root. Another reason is for bringing stability to the softmax outputs and to have an even distribution. If not, the scores can get overly confident for a single input word. (Refer lecture 15, 46th minute for more detail).<br><br>\n",
    "This is why self attention is also called __sclaed dot product attention__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ae49dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
      "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
      "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
      "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
      "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])\n"
     ]
    }
   ],
   "source": [
    "# get attention weights for enitre sentence\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores / d_k ** 0.5, dim = -1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42196d7a",
   "metadata": {},
   "source": [
    "These attentions weights are now multiplied with the _value_ matrix to get the __context vectors__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4251f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]])\n"
     ]
    }
   ],
   "source": [
    "context = attention_weights @ value\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac068759",
   "metadata": {},
   "source": [
    "Now we make a self attention calss for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d68f1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_k = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_v = torch.nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_k\n",
    "        queries = x @ self.W_q\n",
    "        values = x @ self.W_v\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80c39c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "378ea687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2, which is more optimized due to the Linear class from PyTorch\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_q = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = torch.nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    \n",
    "    # change here compared to v1\n",
    "    def forward(self, x): \n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "\n",
    "        context = attention_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5c61440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test class\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd29b258",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ec3580",
   "metadata": {},
   "source": [
    "### 7. CAUSAL ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8e047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55681081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_q(inputs)\n",
    "keys = sa_v2.W_k(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f16ae",
   "metadata": {},
   "source": [
    "Now we apply causal masking using tril."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "001d9023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_len = attention_weights.shape[0]\n",
    "mask_sample = torch.tril(torch.ones(context_len, context_len))\n",
    "print(mask_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d705d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attention = attention_weights * mask_sample\n",
    "print(masked_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788ce1",
   "metadata": {},
   "source": [
    "Now perform renormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0bfb33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_attention.sum(dim = 1, keepdim = True)\n",
    "masked_attention_norm = masked_attention / row_sums\n",
    "print(masked_attention_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d886285d",
   "metadata": {},
   "source": [
    "However, there is a problem with this method. Even though we have performed masking, the attention scores after applying softmax leads to data leakag due to data redistribution which occurs based on future values as well. A simple solution is to perform masking over the attention scores, and then perform softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9660fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf) # this basically takes the attention scores matrix, looks at positions where the value is True, and gves it -ve inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61290adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim = 1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aae77",
   "metadata": {},
   "source": [
    "_(not exactly correct it seems)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412bba0",
   "metadata": {},
   "source": [
    "Dropout is applied here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d13e1117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f273f7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988cba8",
   "metadata": {},
   "source": [
    "Rescaling based on droupout percentage also occurs.<br>\n",
    "Next, we also introduce batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c2196b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n",
      "\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch)\n",
    "print()\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72632a48",
   "metadata": {},
   "source": [
    "Think of it as 2 input matrices. One sentence can be \"your journey starts with one step\" and the other can be \"my name is mohammed asif sahadh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1eca4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # new\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_len, context_len), diagonal = 1)) # new (register buffer i think ensures that the non trainable stuff will be moved to appropriate device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # new batch dim b\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2) # one coma two because we don't need to transpose the batch dimention (idx 0)\n",
    "        attention_scores.masked_fill_( # _ ops makes it inplace\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # :num_tokens is to ensure if the sequence is less than the context length\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1] ** 0.5, dim = -1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights) # new      \n",
    " \n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "422bb0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_len, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(context_vecs)\n",
    "print()\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f200a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a54ab2",
   "metadata": {},
   "source": [
    "### 8. MULTI HEAD ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1552de7",
   "metadata": {},
   "source": [
    "For MHA, we simply have to create a wrapper for causal attention that stacks the outputs of multiple of thier outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eed8cf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( # creates an instance of causal attention class\n",
    "            [CausalAttention(d_in, d_out, context_len, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f655078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your\n",
    "     [0.55, 0.87, 0.66], # journey\n",
    "     [0.57, 0.85, 0.64], # starts\n",
    "     [0.22, 0.58, 0.33], # with\n",
    "     [0.77, 0.25, 0.10], # one \n",
    "     [0.05, 0.80, 0.55]] # step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "391277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0cbd3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063,  0.4566,  0.2729],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257,  0.5792,  0.3011],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860,  0.6249,  0.3102],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589,  0.5691,  0.2785],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428,  0.5543,  0.2520],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493,  0.5337,  0.2499]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_len = inputs.shape[0]\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_len, 0.0, 3)\n",
    "mha.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b17cb",
   "metadata": {},
   "source": [
    "_(output matches here though)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b194d7",
   "metadata": {},
   "source": [
    "To solve the inefficiency casued by performing matrix multiplations over multiple head, we implement weight splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e319612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_out = x.shape # s1\n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # s5\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights) # s8\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2) # s9 & s10\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # s11\n",
    "        context_vec = self.out_proj(context_vec) # optional\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a43c13",
   "metadata": {},
   "source": [
    "s1 to s11 are steps to implement MHA, in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87274ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 6])\n",
      "tensor([[[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]],\n",
      "\n",
      "        [[ 0.1570, -0.0864,  0.0213,  0.0216, -0.3244, -0.2521],\n",
      "         [ 0.1118, -0.0543,  0.0409, -0.0212, -0.3252, -0.2995],\n",
      "         [ 0.1196, -0.0488,  0.0319, -0.0635, -0.2789, -0.2579]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.90, 0.55, 0.87, 0.66],\n",
    "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],\n",
    "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)\n",
    "\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 6\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02e987",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39e8b5",
   "metadata": {},
   "source": [
    "### 9. IMPLEMENTING GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c5f2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2425",
   "metadata": {},
   "source": [
    "#### _9.1 Dummy class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f92aa2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]) # this is the entire lookup matrix to get the embedding for a token\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]) # this depends on context length of course (also a lookup table)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # make a placeholder for transformer block\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )        \n",
    "        \n",
    "        # make a placeholder for layernorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx): # in_idx is a batch of input tokens\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device)) # arange creates ids from 0 to max_len - 1 \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) # lots of things happen here\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # only a placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7af49492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token embeddings:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Logits:\n",
      "tensor([[[-5.4645e-03, -7.1054e-01, -1.0149e+00,  ..., -4.1823e-01,\n",
      "           2.2412e-02, -1.0233e+00],\n",
      "         [ 1.7904e+00, -3.2025e-03, -1.1346e-01,  ..., -6.4449e-01,\n",
      "           6.3170e-01,  3.2810e+00],\n",
      "         [ 4.1122e-01, -2.4297e-01, -3.9615e-01,  ...,  3.1840e-01,\n",
      "           5.1082e-01, -7.7559e-02],\n",
      "         [-8.1073e-01, -4.5008e-01, -8.1223e-01,  ...,  1.2990e+00,\n",
      "           3.7404e-01,  9.0462e-02]],\n",
      "\n",
      "        [[ 2.6268e-01, -7.7104e-01, -1.4651e+00,  ..., -6.4443e-01,\n",
      "          -4.4353e-01, -9.2719e-01],\n",
      "         [ 1.1406e+00, -3.4269e-01, -6.9491e-01,  ..., -1.7102e-01,\n",
      "           6.8366e-01,  2.0607e+00],\n",
      "         [ 1.1106e+00,  1.1755e+00, -6.2576e-01,  ..., -4.4802e-01,\n",
      "           2.6767e-02, -9.2523e-01],\n",
      "         [-3.8614e-01,  3.0415e-01, -9.1265e-01,  ...,  1.8638e+00,\n",
      "           6.4222e-01,  2.9281e-01]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# dummy sample\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim = 0)\n",
    "print(\"Input token embeddings:\")\n",
    "print(batch)\n",
    "print()\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Logits:\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef39f60",
   "metadata": {},
   "source": [
    "#### _9.2 Layernorm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f850829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[-1.4771,  0.3162, -0.1837, -0.8028, -1.2379],\n",
      "        [-1.2232,  0.3065, -0.4733, -0.1332, -0.5370]])\n",
      "\n",
      "Output:\n",
      "tensor([[0.0000, 0.2986, 0.2986, 0.5947, 1.2578, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3003, 0.4595, 0.9096, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple feedforward\n",
    "batch_sample = torch.randn(2, 5)\n",
    "print(\"Input:\")\n",
    "print(batch_sample)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_sample)\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "181ab561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4083],\n",
      "        [0.2782]], grad_fn=<MeanBackward1>)\n",
      "tensor([[0.2228],\n",
      "        [0.1328]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim = -1, keepdim = True)\n",
    "var = out.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e1ffc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8650, -0.2323, -0.2324,  0.3950,  1.7998, -0.8650],\n",
      "        [-0.7634, -0.7634,  0.0605,  0.4974,  1.7322, -0.7634]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layernorm = (out - mean) / (var ** 0.5)\n",
    "print(layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c6e7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4769e-08],\n",
      "        [ 4.4703e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = layernorm.mean(dim = -1, keepdim = True)\n",
    "var = layernorm.var(dim = -1, keepdim = True)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a38ab2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "011ec518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2219e-07],\n",
      "        [-9.5367e-08]], grad_fn=<MeanBackward1>)\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_sample)\n",
    "mean = out_ln.mean(dim = -1, keepdim = True)\n",
    "var = out_ln.var(dim = -1, keepdim = True, unbiased = False)\n",
    "print(mean)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f45fe",
   "metadata": {},
   "source": [
    "#### _9.3 GELU_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "35d37162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a23c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b0ce1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afb9f8",
   "metadata": {},
   "source": [
    "#### _9.4 Skip Connections_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30c63ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDNN(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut): \n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()), \n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x) # output of linear layer x\n",
    "            if self.use_shortcut and x.shape == layer_out.shape:\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87eab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_wo_shortut = ExampleDNN(layer_sizes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "59b36e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grads(model, x):\n",
    "    output = model(x) # normal output from nn\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "794b7e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_wo_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cace1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_w_shortut = ExampleDNN(layer_sizes, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1291623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "print_grads(model_w_shortut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def0a06",
   "metadata": {},
   "source": [
    "Gradient vanishing reduced..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a05c5",
   "metadata": {},
   "source": [
    "#### _9.5 Coding Attention & Linear Layers in a Transformer Block_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c5cf83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52733201",
   "metadata": {},
   "source": [
    "Requirements for building the transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "50ef554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ee8ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # MHA\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # shape: [batch size, num tokens, emb size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        # FCL\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cadd4868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[[0.2961, 0.5166, 0.2517,  ..., 0.9541, 0.8567, 0.4604],\n",
      "         [0.2238, 0.3047, 0.3019,  ..., 0.5465, 0.4532, 0.7598],\n",
      "         [0.6945, 0.2478, 0.4111,  ..., 0.8838, 0.4898, 0.5963],\n",
      "         [0.0890, 0.7804, 0.9223,  ..., 0.4507, 0.6357, 0.5833]],\n",
      "\n",
      "        [[0.5716, 0.9297, 0.3396,  ..., 0.0477, 0.4564, 0.2797],\n",
      "         [0.0936, 0.2211, 0.3806,  ..., 0.3948, 0.4545, 0.4536],\n",
      "         [0.6788, 0.1741, 0.2084,  ..., 0.5557, 0.5930, 0.0959],\n",
      "         [0.3894, 0.4083, 0.0662,  ..., 0.9861, 0.9341, 0.1319]]])\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Output:\n",
      "tensor([[[-0.0055,  0.0972, -0.1122,  ...,  1.2889,  0.2623,  0.6685],\n",
      "         [ 0.0023, -0.2369,  0.1720,  ...,  0.5952,  0.2497,  0.7447],\n",
      "         [ 0.4673,  0.4472,  0.1791,  ...,  1.2525,  0.3045,  0.7750],\n",
      "         [ 0.0662,  0.7224,  0.9206,  ...,  0.4790,  0.7428,  0.7015]],\n",
      "\n",
      "        [[ 0.3622,  1.2144,  0.5221,  ...,  0.1854,  0.0111, -0.5034],\n",
      "         [-0.0225,  0.7789,  0.2770,  ...,  0.1734,  0.5419,  0.1143],\n",
      "         [ 0.7425,  0.4013,  0.3211,  ...,  0.3268,  0.7523, -0.1642],\n",
      "         [ 0.5745,  0.6241,  0.4410,  ...,  1.1963,  1.2650,  0.2243]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print()\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc24eb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896df3d9",
   "metadata": {},
   "source": [
    "### 10. CODING GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "548d0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 1024,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b2055",
   "metadata": {},
   "source": [
    "#### _10.1 The GPT Class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74dea935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_len\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "        \n",
    "    def forward(self, in_idx): # input batch\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "33845b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output batch:\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\")\n",
    "print(batch)\n",
    "print(\"Output batch:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71656dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e817112",
   "metadata": {},
   "source": [
    "_Note: We are not using weight tying, which was performed in the original GPT-2 model_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c9e73",
   "metadata": {},
   "source": [
    "#### _10.2 Generate Text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f72ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx is the input batch\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop current context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # get predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # batch_size x tokens_num x vocab_size\n",
    "        # get the last time step (last set of logits)\n",
    "        logits = logits[:, -1, :]\n",
    "        # apply softmax\n",
    "        probs = torch.softmax(logits, dim = -1)\n",
    "        # get id of max\n",
    "        idx_next = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "        # append id to running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c37518e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 314, 716, 407, 1016, 284, 4483, 11311]\n",
      "Encoded tensor: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311]])\n",
      "Encoded tensor shape: torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am not going to eat chocolate\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(f\"Encoded tensor: {encoded_tensor}\")\n",
    "print(f\"Encoded tensor shape: {encoded_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "50371a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,   407,  1016,   284,  4483, 11311, 20656,\n",
      "         30719, 44035, 23338, 24151, 10835, 42731, 35799, 46215,   371]])\n",
      "Output shape: 19\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = encoded_tensor,\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "print(f\"Output: {out}\")\n",
    "print(f\"Output shape: {len(out[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9b986d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am not going to eat chocolate AmbassadorOptional touredنGirl malesGradットorea R\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4bf9a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed68ec82",
   "metadata": {},
   "source": [
    "### 11. The LLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "17e66e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 512, # can be reduced for training simplicity\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 12,\n",
    "    \"n_layers\" : 12,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88e7cef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d000af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      "Every effort moves youublishedstein municipal sushi abnormal Payment contemporthsα politely\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special = {'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"]\n",
    ")\n",
    "\n",
    "print(f\"Output text:\\n{token_ids_to_text(token_ids, tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "41ebb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feet wet\n",
    "inputs = torch.tensor([[16833, 3626, 6100],  # [\"every effort moves\"]\n",
    "                       [40, 1107, 588]])     # [\"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345],   # [\" effort moves you\"]\n",
    "                        [1107, 588, 11311]]) # [\" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "50fea205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[2.9209e-05, 1.0527e-05, 1.6121e-05,  ..., 2.6603e-05,\n",
      "          2.6384e-05, 4.3995e-05],\n",
      "         [2.6438e-05, 9.8570e-06, 1.3430e-05,  ..., 7.6308e-06,\n",
      "          1.2208e-05, 4.2206e-05],\n",
      "         [2.0652e-05, 3.0963e-05, 7.3546e-05,  ..., 3.5792e-05,\n",
      "          1.5129e-05, 1.7849e-05]],\n",
      "\n",
      "        [[9.4693e-06, 2.0345e-05, 1.8425e-05,  ..., 8.6429e-06,\n",
      "          1.5247e-05, 3.9934e-05],\n",
      "         [2.3645e-05, 8.8373e-06, 2.1052e-05,  ..., 7.2422e-06,\n",
      "          1.4043e-05, 1.4942e-05],\n",
      "         [1.0545e-05, 1.4763e-05, 2.7852e-05,  ..., 2.5073e-05,\n",
      "          1.3072e-05, 3.0611e-05]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probs = torch.softmax(logits, dim = -1)\n",
    "print(probs.shape)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e751eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[40822],\n",
      "         [27614],\n",
      "         [20921]],\n",
      "\n",
      "        [[ 8807],\n",
      "         [34094],\n",
      "         [45958]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b7781e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:  effort moves you\n",
      "Predicted: ovychrequisite Buddh\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Predicted: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe710892",
   "metadata": {},
   "source": [
    "#### _11.1 Cross Entropy Loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2790cdf",
   "metadata": {},
   "source": [
    "Basically find the target probabilities of the target ids. We need to get these as close to 1 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a599343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3003e-05, 1.7775e-05, 1.4705e-05])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e579b30f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1463e-05, 1.5585e-05, 8.4811e-06])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "target_probs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5a962f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.6723, -10.9377, -11.1273, -11.3764, -11.0692, -11.6777])\n"
     ]
    }
   ],
   "source": [
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "be67a724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.9768)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e02985cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9768)\n"
     ]
    }
   ],
   "source": [
    "neg_log_likelihood = -1 * avg_log_probs\n",
    "print(neg_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6eaf5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simpler method\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "# above, we have merged the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6aab6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.9768)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91714488",
   "metadata": {},
   "source": [
    "#### _11.2 Perplexity_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cea0b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(58498.9844)\n"
     ]
    }
   ],
   "source": [
    "perp = torch.exp(loss)\n",
    "print(perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ece60",
   "metadata": {},
   "source": [
    "That's horrible...for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d066a05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dec17",
   "metadata": {},
   "source": [
    "### 12. INITIAL ERROR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c34ddffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 110806\n",
      "The Early Foundations and Academic Journey\n",
      "\n",
      "When one looks at the trajectory of Mohammed Asif Sahad\n"
     ]
    }
   ],
   "source": [
    "with open(\"me.txt\", \"r\", encoding = \"utf-8\") as t:\n",
    "    raw_text = t.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b38052cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 22066\n"
     ]
    }
   ],
   "source": [
    "total_tokens = len(tokenizer.encode(raw_text))\n",
    "print(f\"Tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e4f4810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = True,\n",
    "    shuffle = True,\n",
    "    num_workers = 0\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_v1(\n",
    "    test_data,\n",
    "    batch_size = 2,\n",
    "    max_len = GPT_CONFIG_124M[\"context_len\"],\n",
    "    stride = GPT_CONFIG_124M[\"context_len\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False,\n",
    "    num_workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "865bda38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens * (1 - train_ratio) < GPT_CONFIG_124M[\"context_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c291830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "\n",
      "Test Loader:\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print()\n",
    "print(\"Test Loader:\")\n",
    "for x, y in test_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b12ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # this does all the softmax & everything\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None): # this will show the loss of the LM\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches # mean loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "081e60d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cac25d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.974997771413703\n",
      "Testing loss: 10.976041316986084\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): # disable for now\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Testing loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db941cff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97eff95",
   "metadata": {},
   "source": [
    "### 13. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0fdb1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function\n",
    "def evaluate_model(model, train_loader, test_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches = eval_iter)\n",
    "        test_loss = calc_loss_loader(test_loader, model, device, num_batches = eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4ed39a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see text generation during training\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model, idx = encoded, max_new_tokens = 50, context_size = context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    print()\n",
    "    model.train() # set it back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2aa2c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, test_loader, optimizer, device, \n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # reset gradients from previous batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel() # return number of tokens seen\n",
    "            global_step += 1\n",
    "\n",
    "            # evaluation (optional)\n",
    "            if global_step % eval_freq == 0: # only after a set of batches is used for training\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_loader, test_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch + 1} (Step {global_step:04d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Test loss {test_loss:.3f}\")\n",
    "            \n",
    "        # print sample text from each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "        \n",
    "    return train_losses, test_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "db83d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 0000): Train loss 9.729, Test loss 9.769\n",
      "Epoch 1 (Step 0005): Train loss 7.952, Test loss 7.916\n",
      "Epoch 1 (Step 0010): Train loss 6.682, Test loss 6.568\n",
      "Epoch 1 (Step 0015): Train loss 6.008, Test loss 6.002\n",
      "Asif likes to the a the to the a to the a of a the a of a to the a.                               \n",
      "\n",
      "Epoch 2 (Step 0020): Train loss 5.790, Test loss 5.883\n",
      "Epoch 2 (Step 0025): Train loss 5.445, Test loss 5.693\n",
      "Epoch 2 (Step 0030): Train loss 4.748, Test loss 5.331\n",
      "Epoch 2 (Step 0035): Train loss 4.392, Test loss 5.074\n",
      "Asif likes to the to the, and the to the, and the, and the of AI, and his ability to the to the to the, and the to the to an of AI, and the to the to.        \n",
      "\n",
      "Epoch 3 (Step 0040): Train loss 4.214, Test loss 4.748\n",
      "Epoch 3 (Step 0045): Train loss 3.791, Test loss 4.406\n",
      "Epoch 3 (Step 0050): Train loss 3.623, Test loss 4.200\n",
      "Epoch 3 (Step 0055): Train loss 3.084, Test loss 4.023\n",
      "Asif likes to the time, and the ability to code.                                         \n",
      "\n",
      "Epoch 4 (Step 0060): Train loss 2.891, Test loss 3.799\n",
      "Epoch 4 (Step 0065): Train loss 2.662, Test loss 3.724\n",
      "Epoch 4 (Step 0070): Train loss 2.153, Test loss 3.461\n",
      "Epoch 4 (Step 0075): Train loss 1.954, Test loss 3.150\n",
      "Asif likes to the time at VIT, and the world of a practitioner of a practitioner, and the best, and the Machine Learning Team.                        \n",
      "\n",
      "Epoch 5 (Step 0080): Train loss 1.898, Test loss 2.898\n",
      "Epoch 5 (Step 0085): Train loss 1.589, Test loss 2.755\n",
      "Epoch 5 (Step 0090): Train loss 1.562, Test loss 2.780\n",
      "Asif likes to the time of AI.       Asif’solving, Asif’solving, and the surface of his ability to the time Asif’solving. He was not only a project development.\n",
      "\n",
      "Epoch 6 (Step 0095): Train loss 1.299, Test loss 2.790\n",
      "Epoch 6 (Step 0100): Train loss 1.186, Test loss 2.840\n",
      "Epoch 6 (Step 0105): Train loss 1.077, Test loss 2.717\n",
      "Epoch 6 (Step 0110): Train loss 0.874, Test loss 2.696\n",
      "Asif likes to the competing demands of AI, and the tools for global tech hubs but as an early age, Asif’solving. He developed reliable agentic frameworks and the time Asif’s academic and the field of AI today.  \n",
      "\n",
      "Epoch 7 (Step 0115): Train loss 0.824, Test loss 2.706\n",
      "Epoch 7 (Step 0120): Train loss 0.628, Test loss 2.690\n",
      "Epoch 7 (Step 0125): Train loss 0.630, Test loss 2.653\n",
      "Epoch 7 (Step 0130): Train loss 0.578, Test loss 2.735\n",
      "Asif likes to the example of AI.    Living in a Righteous Society  Equally significant is Asif’s aspiration to live within a Muslim society, and the result of his Bachelor’s degree—with a respectable CGPA\n",
      "\n",
      "Epoch 8 (Step 0135): Train loss 0.545, Test loss 2.902\n",
      "Epoch 8 (Step 0140): Train loss 0.444, Test loss 2.886\n",
      "Epoch 8 (Step 0145): Train loss 0.377, Test loss 2.840\n",
      "Epoch 8 (Step 0150): Train loss 0.330, Test loss 2.795\n",
      "Asif likes to his ambition—is his identity as a Muslim, his attachment to both lead and his aspiration to live as a man of balance: physically strong, spiritually grounded, and socially responsible.    But Asif’s professional experiences were not\n",
      "\n",
      "Epoch 9 (Step 0155): Train loss 0.321, Test loss 2.828\n",
      "Epoch 9 (Step 0160): Train loss 0.314, Test loss 2.990\n",
      "Epoch 9 (Step 0165): Train loss 0.263, Test loss 2.959\n",
      "Epoch 9 (Step 0170): Train loss 0.228, Test loss 2.972\n",
      "Asif likes to the example of generations before him.  Living in a Righteous Society  Equally significant is Asif’s aspiration to live within a Muslim society, in a country and community where faith, tradition, and social norms align with his\n",
      "\n",
      "Epoch 10 (Step 0175): Train loss 0.212, Test loss 2.966\n",
      "Epoch 10 (Step 0180): Train loss 0.197, Test loss 2.955\n",
      "Epoch 10 (Step 0185): Train loss 0.188, Test loss 2.952\n",
      "Asif likes to a mobile application presentation to an audience of over twenty team members. In essence, this project is a form of self-directed research, one that mimics the rigor of graduate-level experimentation and could, in the future, evolve into publishable\n",
      "\n",
      "Training completed in 1.34 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, test_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, test_loader, optimizer, device, \n",
    "    num_epochs = num_epochs, eval_freq = 5, eval_iter = 5, # after every 5 batches, training and validation loss will be printed\n",
    "    start_context = \"Asif likes to\", tokenizer = tokenizer\n",
    ") \n",
    "\n",
    "end = time.time()\n",
    "training_time = (end - start) / 60\n",
    "print(f\"Training completed in {training_time:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a9c87ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024, 6144, 11264, 16384, 21504, 26624, 31744, 36864, 41984, 47104, 52224, 57344, 62464, 67584, 72704, 77824, 82944, 88064, 93184, 98304, 103424, 108544, 113664, 118784, 123904, 129024, 134144, 139264, 144384, 149504, 154624, 159744, 164864, 169984, 175104, 180224, 185344, 190464]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_seen) # number of tokens seen by the model at each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e03c1a",
   "metadata": {},
   "source": [
    "Awesome, but there is some overfitting. This means the model is taking text directly from the book...word to word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e4c17d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArEAAAGGCAYAAABsTdmlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbLpJREFUeJzt3Xd4VGXexvHvmZLeA2mU0HvvTYqggIA0C8qugIqroqhYcVVA1xdZFVl0xbpgxwqiICoIKEUp0ksoAgFCCDW9zcx5/0gYjRQDJJlJuD/XNVdmTv3NmSPe88xznmOYpmkiIiIiIlKOWDxdgIiIiIjIhVKIFREREZFyRyFWRERERModhVgRERERKXcUYkVERESk3FGIFREREZFyRyFWRERERModhVgRERERKXcUYkVERESk3FGIFRGPMQyD7t27X9I2li5dimEYTJw4sURqkgtXEp+jiMiFUogVucwZhnFBDym+zz77jE6dOhEWFkZoaCgtW7bkiSeeICsrq9jbOB3Si/sojTDZvXv3cvXZjxw5EsMw+Pnnnz1dioiUIpunCxARz5owYcIZ06ZNm0ZqaupZ55Wk7du3ExAQcEnbaNeuHdu3b6dSpUolVFXJeO2117jrrrsIDg7mhhtuIDQ0lG3btvH8889z++23U6NGjWJtp0aNGmd8DqdOneI///kP8fHxjBw58ozly1pJfI4iIhfKME3T9HQRIuJdatSowf79+9E/DxevZcuWbNiwgZ9//pn27du7p588eZKAgAB8fX0vetv79u2jZs2adOvWjaVLl5ZAtefXvXt3li1bVm7Oh5EjR/LOO++watUqOnTo4OlyRKSUqDuBiBTLvn37MAyDkSNHsn37dgYPHkxkZCSGYbBv3z4A5syZw0033USdOnUICAggNDSUK664gs8///ys2zzbz9+nfwreu3cv06dPp0GDBvj6+hIfH8+kSZNwuVxFlj9Xn9gaNWpQo0YNMjIyuO+++4iLi8PX15dmzZrx2WefnfM93njjjURERBAUFES3bt348ccfmThxIoZhXFBgDAoKwjAMmjZtWmR6eHj4JQXYv5KSksIDDzxAnTp18PX1pVKlSgwdOpQtW7acseyuXbsYNWoUNWvWxNfXl4iICJo3b87999/vDqyGYbBs2TL389OPP7YAl8TnCJCVlcUjjzxCtWrV8PPzo0mTJrz55pul3u955syZtG/fnqCgIIKCgmjfvj2zZs0667Kff/453bp1IyoqCj8/P+Li4ujVq9cZ5/iSJUvo27ev+7yLjo7miiuu4I033iiV9yByOVJ3AhG5ILt376ZDhw40bdqUkSNHcvz4cXx8fAAYP348Pj4+dOnShdjYWI4ePcq8efO47rrrmD59Ovfee2+x9/Pwww+zbNky+vfvT+/evZk7dy4TJ04kLy+PZ599tljbyM/P5+qrr+bkyZMMHTqUrKwsZs+ezQ033MDChQu5+uqr3cseOnSITp06cfjwYfr06UPLli1JSEjgqquu4sorr7ywgwSMHTuW5cuXM2HCBJ5//vkLXv9i7Nmzh+7du3Pw4EGuvvpqBg0aREpKCp9//jnffvstixcvdrcKJyUl0a5dOzIzM+nXrx833ngjmZmZ7Nq1i1dffZUXXngBm83GhAkTmDVrFvv37y/SraFFixbFqqm4n6PT6aR///4sWbKEpk2bcvPNN3PixAkefPDBUr1obOzYsbz88stUqVKF2267DSgIqqNGjWL9+vX85z//cS87Y8YM7r77bmJjY91f4pKTk1m9ejVz5sxh6NChAMyfP58BAwYQFhbGwIED3f8tbNy4kffee4877rij1N6PyGXFFBH5k/j4ePPP/zzs3bvXBEzAfOqpp8663p49e86Ylp6ebjZt2tQMDQ01MzMzi8wDzG7duhWZNmLECBMwa9asaSYlJbmnHz161AwLCzODg4PN3Nxc9/QlS5aYgDlhwoSzvoeBAwcWWX7RokUmYPbu3bvI8n/7299MwHz22WeLTH/77bfd73vJkiVnfd9n8/rrr5uGYZiAOWnSpGKvVxynP4s/H7tOnTqZVqvVXLhwYZHpCQkJZnBwsNm0aVP3tOnTp5uAOW3atDO2f/z48SKvu3Xrdsb58Ecl8Tm+9dZbJmD27dvXdDgc7ulbt241/fz8zvoZn8vpfa9ateq8yy1btswEzIYNG5qnTp1yTz9x4oRZr149EzB//PFH9/RWrVqZPj4+5pEjR87Y1rFjx9zPhwwZYgLmhg0bzruciFwadScQkQsSExPDP//5z7POq1Wr1hnTgoKCGDlyJKmpqaxZs6bY+3nyySeJjY11v65UqRIDBw4kPT2dhISEYm/npZdecrcUA/Ts2ZP4+PgiteTm5vLpp58SFRXFgw8+WGT9UaNGUb9+/WLvD+Ctt97iH//4B6NHj+a+++5jwoQJPPLII2csd8cdd2AYxgW9n3NZv349K1euZMSIEfTu3bvIvHr16jF69Gg2b958RrcCf3//M7YVERFxyfWcVtzP8f333wfg2WefxWq1uqc3atSIW265pcTq+aN33nkHgIkTJxIaGuqeHh4e7m51/nO3Arvdjt1uP2NbkZGRZ0w727E923IicnHUnUBELkjz5s2LhMI/SklJ4bnnnuObb75h//79ZGdnF5mflJRU7P20bt36jGlVq1YFCq7OL46wsDBq1qx51u2sWrXK/TohIYHc3FzatGlzRn9VwzDo1KlTsYPm8ePHGTt2LI0bN+aVV17BbreTk5PD888/T0ZGBv/973/dw1Xt2rWLsLAw6tSpU6xtn8/p4aSOHDly1r6jO3bscP9t0qQJAwYMYPz48YwZM4bFixfTp08funXrdtYvIpeiuJ/jxo0bCQwMpGXLlmcs37lz51LpS7p+/XqAs3ZX6NGjBwAbNmxwTxs2bBiPPPIITZo04eabb6ZHjx506dKFkJCQIusOGzaML774gg4dOnDzzTfTs2dPrrjiCq8bQUOkvFOIFZELEh0dfdbpJ06coG3btiQmJtK5c2d69epFWFgYVquVDRs28OWXX5Kbm1vs/fw5GADYbAX/ZDmdzmJt44+ta3/ezh8vLEpLSwMgKirqrMuf6z2fzVdffUV2djYjR450t9jNmDGD7OxsZsyYQUZGBjNnzuTEiROsXLmSm2++uUjL48U6ceIEUNAfc/78+edcLjMzEyi48O3nn39m4sSJLFiwgE8++QSABg0a8PTTT3P99ddfck1Q/M8xLS2NatWqnXUbF3L8L0RaWhoWi4XKlSufdZ+GYbjPDYCHHnqIyMhIZsyYwYsvvujuN9yvXz9eeukl9xem66+/nrlz5zJ16lRee+019xeXHj168OKLLxa7P7GInJ9CrIhckHMNev/222+TmJjIM888wxNPPFFk3nPPPceXX35ZFuVdlNNBKyUl5azzjxw5UuxtHT58GIDg4GD3NMMw+N///kdOTg7vvfceGRkZVK1aFYfDcUb3hYt1+j28/PLL3HPPPcVap0mTJnz22Wfk5+ezbt06vvnmG6ZPn86NN95IXFwcnTt3LpHaiiMkJISjR4+edd6FHP8L3afL5eLo0aNnfIFJSUnBNM0iIdwwDG699VZuvfVWjh8/zk8//cRHH33EJ598wq5du9i0aZP7C8nAgQPd3SZWrFjBF198wdtvv02fPn3YsWMHYWFhpfKeRC4n6hMrIiViz549QMH/vP/sp59+KutyLkj9+vXx9fVl3bp1Z7QWm6ZZpOvBXzl9s4E/D8dltVr54IMPGDhwIHPmzOHll1/m7rvvpkmTJpdaPoB71IELqfU0u91Ohw4dmDRpEtOnT8c0Tb7++usitUPxW8AvRvPmzcnMzCzy8/1pK1euLJV9nu66cLah005PO1eraWRkJIMGDeLjjz/myiuvZNu2bezevfuM5YKDg+nTpw9vvPEGI0eO5MiRI/zyyy8l9RZELmsKsSJSIuLj4wFYvnx5kekffvghCxYs8ERJxebr68t1113HkSNHmDZtWpF57777rrs/aXH069ePypUrM3v2bN5+++0i82w2G9ddd5379eHDh886XurFaNeuHe3bt+ejjz7i448/PmO+y+Vyj/cKsG7duiI/lZ92utXTz8/PPe30hV4HDhwokVrPZvjw4QA88cQTRY7Jjh073BdglbQRI0YAMGnSpCLHIjU1lUmTJhVZBgqCrfmnGz7k5+e7u3KcPmY//vjjWQP/6Zb+Px5bEbl46k4gIiXi73//O1OmTOHee+9lyZIlxMfHs3HjRhYvXsyQIUP44osvPF3ieU2ePJlFixbx2GOPsWzZMvc4sV9//TV9+vRh4cKFWCx//b0/JCSE2bNnc+2113L77bfz5ptv0qFDB/dNA9avX0/Hjh2x2Wx8/vnnPPDAA0XGIr0UH330ET169GDYsGFMmzaNVq1a4e/vT2JiIqtWreLo0aPk5OQA8N577/H666/TtWtXateuTUhICNu2bWPBggVEREQwatQo93avvPJKPvvsM4YOHUrfvn3x8/OjefPmDBgwoETqhoJRIN577z3mz59Py5Yt6du3LydOnGD27NlcddVVfPXVV8U6/n/0zDPPnLW/K8Bjjz1G165duffee3n55Zdp0qQJQ4cOxTRNPv/8cw4ePMjYsWPp2rWre51BgwYREhJChw4diI+PJz8/n++//55t27Zx3XXXub/IjR07lqSkJLp06UKNGjUwDIPly5ezevVqOnToQJcuXS7+QImIm0KsiJSIqlWrsmzZMh555BEWLVqEw+GgVatWfPfddxw4cMDrQ2y1atVYtWoVjz76KN999x3Lli2jdevWfPfdd3z66afA2S9SOpsrr7ySDRs2MGXKFL799lteffVVgoKCaNmyJe+++y4333wzqampdOjQgenTp1OlSpWzDsF1oWrWrMn69euZOnUqc+fOZebMmVitVmJjY+natWuRVuCbbrqJnJwcVqxYwerVq8nNzaVq1arcddddPPzww1SvXt297OjRo9m3bx+zZ89mypQpOBwORowYUaIh1mq1smDBAiZMmMBHH33EtGnTqF27Ni+++CIRERF89dVXxT7+p53vF4CRI0fSoEEDpk+fTsuWLZkxY4Z7BITGjRvz9NNPFwnyUPBFZ+HChaxevZqvvvqKwMBAateuzYwZM9w3SoCCm3588cUXrFu3jm+//Ra73U6NGjWYMmUKd999d4lcyCciYJh//m1ERESK6NKlC6tWrSI1NZWgoCBPl3PZeeKJJ3j22WdZsGABffv29XQ5IuIl1CdWRKTQ6ZEF/uj9999nxYoV9OrVSwG2lJ3t+G/bto3p06cTFhZWqrefFZHyR90JREQKNWnShJYtW9KoUSP3+LZLly4lODiYF154wdPlVXh33XUX+/bto127doSHh7Nnzx6++uor8vPzefvtt896BywRuXypO4GISKF//vOffPXVVyQmJpKZmUnlypXp0aMHTz75JA0aNPB0eRXeBx98wGuvvcb27dvdXTfatm3Lgw8+eMatdEVEFGJFREREpNxRn1gRERERKXcUYkVERESk3FGIFREREZFyRyFWRERERModhVgRERERKXcUYkVERESk3FGIFREREZFyRyFWRERERMqdCn/bWZfLRVJSEsHBwRiG4elyREREROQ8TNMkPT2duLg4LJZzt7dW+BCblJREtWrVPF2GiIiIiFyAAwcOULVq1XPOr/AhNjg4GCg4ECEhIR6uRkRERETOJy0tjWrVqrkz3Ll4NMT++OOPPP/886xbt47Dhw8zZ84cBg0a5J5vmiYTJkzgzTff5NSpU3Tu3JkZM2ZQt27dYu/jdBeCkJAQhVgRERGRcuKvuoF69MKuzMxMmjdvzn//+9+zzv/3v//N9OnTee211/jll18IDAykd+/e5OTklHGlIiIiIuJNPNoS27dvX/r27XvWeaZpMm3aNJ544gkGDhwIwLvvvkt0dDRz585l2LBhZVmqiIiIiHgRrx1ia+/evSQnJ9OrVy/3tNDQUNq3b8+qVas8WJmIiIiIeJrXXtiVnJwMQHR0dJHp0dHR7nlnk5ubS25urvt1Wlpa6RQoIiIiZ+V0OsnPz/d0GeKl7HY7Vqv1krfjtSH2Yk2ePJlJkyZ5ugwREZHLjmmaJCcnc+rUKU+XIl4uLCyMmJiYSxrD32tDbExMDABHjhwhNjbWPf3IkSO0aNHinOuNHz+ecePGuV+fHqZBREREStfpABsVFUVAQIBuMiRnME2TrKwsUlJSAIpkvAvltSG2Zs2axMTEsHjxYndoTUtL45dffuGuu+4653q+vr74+vqWUZUiIiICBV0ITgfYyMhIT5cjXszf3x+AlJQUoqKiLrprgUdDbEZGBrt373a/3rt3Lxs2bCAiIoLq1atz//33869//Yu6detSs2ZNnnzySeLi4oqMJSsiIiKed7oPbEBAgIcrkfLg9HmSn59fPkPs2rVr6dGjh/v16W4AI0aMYNasWTzyyCNkZmZyxx13cOrUKbp06cLChQvx8/PzVMkiIiJyHupCIMVREueJR4fY6t69O6ZpnvGYNWsWUPAGn376aZKTk8nJyWHRokXUq1fPkyWLiIiI/KUaNWowbdq0Yi+/dOlSDMPQRXEXwGvHiS2vcnft5OjExzE1tIiIiIjXMwzjvI+JEyde1HbXrFnDHXfcUezlO3XqxOHDhwkNDb2o/RVXRQrLXnthV3n1xfo99P9pMXkbB+Lbpr2nyxEREZHzOHz4sPv5xx9/zFNPPUVCQoJ7WlBQkPu5aZo4nU5str+OT5UrV76gOnx8fNwjM0nxqCW2hDXp3I6jvkEc+WGxp0sRERGRvxATE+N+hIaGYhiG+/WOHTsIDg7mm2++oXXr1vj6+rJ8+XL27NnDwIEDiY6OJigoiLZt27Jo0aIi2/1zdwLDMHjrrbcYPHgwAQEB1K1bl3nz5rnn/7mFdNasWYSFhfHtt9/SsGFDgoKC6NOnT5HQ7XA4GDt2LGFhYURGRvLoo48yYsSIS7oA/uTJk9xyyy2Eh4cTEBBA37592bVrl3v+/v37GTBgAOHh4QQGBtK4cWMWLFjgXnf48OFUrlwZf39/6taty8yZMy+6lr+iEFvCGsdFsD6mLsbaXzxdioiIiJSAxx57jOeee47t27fTrFkzMjIyuOaaa1i8eDHr16+nT58+DBgwgMTExPNuZ9KkSdxwww1s2rSJa665huHDh3PixIlzLp+VlcULL7zAe++9x48//khiYiIPPfSQe/6UKVP44IMPmDlzJitWrCAtLY25c+de0nsdOXIka9euZd68eaxatQrTNLnmmmvco0+MGTOG3NxcfvzxRzZv3syUKVPcrdVPPvkk27Zt45tvvmH79u3MmDGDSpUqXVI956PuBCXMYhjktGhL2Jdv4Eo6iCWuqqdLEhERkUvw9NNPc9VVV7lfR0RE0Lx5c/frZ555hjlz5jBv3jzuueeec25n5MiR3HTTTQD83//9H9OnT2f16tX06dPnrMvn5+fz2muvUbt2bQDuuecenn76aff8l19+mfHjxzN48GAAXnnlFXer6MXYtWsX8+bNY8WKFXTq1AmADz74gGrVqjF37lyuv/56EhMTGTp0KE2bNgWgVq1a7vUTExNp2bIlbdq0AQpao0uTQmwpqN6jO88n7OW2fAvxni5GRETEg7LzHew7nl7m+60RGYy/vWRizulQdlpGRgYTJ05k/vz5HD58GIfDQXZ29l+2xDZr1sz9PDAwkJCQEPedq84mICDAHWCh4O5Wp5dPTU3lyJEjtGvXzj3farXSunVrXC7XBb2/07Zv347NZqN9+9+v6YmMjKR+/fps374dgLFjx3LXXXfx3Xff0atXL4YOHep+X3fddRdDhw7l119/5eqrr2bQoEHuMFwaFGJLQdsGNXioRmtqHU5npFKsiIhcxvYdT2fYzLK/TmT2qJ40jAkvkW0FBgYWef3QQw/x/fff88ILL1CnTh38/f257rrryMvLO+927HZ7kdeGYZw3cJ5tedM0L7D6knX77bfTu3dv5s+fz3fffcfkyZN58cUXuffee+nbty/79+9nwYIFfP/99/Ts2ZMxY8bwwgsvlEotCrGlwM9upW+kncj3X8Ns/iyGv+5eIiIil6cakcHMHtXTI/stLStWrGDkyJHun/EzMjLYt29fqe3vbEJDQ4mOjmbNmjV07doVKLj176+//kqLFi0uapsNGzbE4XDwyy+/uFtQjx8/TkJCAo0aNXIvV61aNe68807uvPNOxo8fz5tvvsm9994LFIzKMGLECEaMGMEVV1zBww8/rBBb3nSoHknnD1ZyatVKwq/s5elyREREPMLfbiuxFlFvUbduXb744gsGDBiAYRg8+eSTF/0T/qW49957mTx5MnXq1KFBgwa8/PLLnDx5slh3w9q8eTPBwb8HfcMwaN68OQMHDmT06NG8/vrrBAcH89hjj1GlShUGDhwIwP3330/fvn2pV68eJ0+eZMmSJTRs2BCAp556itatW9O4cWNyc3P5+uuv3fNKg0JsKWnbsSWJAeFYF/+gECsiIlKBTJ06lVtvvZVOnTpRqVIlHn30UdLS0sq8jkcffZTk5GRuueUWrFYrd9xxB71798Zqtf7luqdbb0+zWq04HA5mzpzJfffdR//+/cnLy6Nr164sWLDA3bXB6XQyZswYDh48SEhICH369OGll14CCsa6HT9+PPv27cPf358rrriC2bNnl/wbL2SYnu5cUcrS0tIIDQ0lNTWVkJCQMt33l/eMo3XiFqp8+a3uJS0iIhVaTk4Oe/fupWbNmvj5+Xm6nMuSy+WiYcOG3HDDDTzzzDOeLue8zne+FDe7aZzYUmRp35nQzFRydib89cIiIiIiF2D//v28+eab7Ny5k82bN3PXXXexd+9ebr75Zk+XViYUYktRo6u682atLmxMy/d0KSIiIlLBWCwWZs2aRdu2bencuTObN29m0aJFpdoP1ZuoT2wpqhUdwQ+tepF5PJ8Oni5GREREKpRq1aqxYsUKT5fhMWqJLUWGYdC7SjBRX83GWXgvZBERERG5dAqxpaxzfCVu3rGMxEWLPF2KiIiISIWhEFvKmjWtx67QGNJ/+tHTpYiIiIhUGAqxpcxmsXCkfguidm7CdDo8XY6IiIhIhaAQWwZCr+hGYF42KWvWeboUERERkQpBIbYMNO3eiY/j2/LLiVxPlyIiIiJSISjEloHgAD/WXn0j81M9XYmIiIh40sSJE2nRooWny6gQFGLLSM9qYcSu+J70pMOeLkVEREQKGYZx3sfEiRMvadtz584tMu2hhx5i8eLFl1Z0MVwOYVk3OygjXWpE0WvH9+yZX4MWo2/zdDkiIiICHD78e+PSxx9/zFNPPUVCwu+3iw8KCirR/QUFBZX4Ni9XaoktI7FVovmtUjzOX1Z6uhQREREpFBMT436EhoZiGEaRabNnz6Zhw4b4+fnRoEEDXn31Vfe6eXl53HPPPcTGxuLn50d8fDyTJ08GoEaNGgAMHjwYwzDcr//cQjpy5EgGDRrECy+8QGxsLJGRkYwZM4b8/N9vWX/48GH69euHv78/NWvW5MMPP6RGjRpMmzbtot/35s2bufLKK/H39ycyMpI77riDjIwM9/ylS5fSrl07AgMDCQsLo3Pnzuzfvx+AjRs30qNHD4KDgwkJCaF169asXbv2omu5WGqJLUMZzdrQaOmX5GVn4+Pv7+lyRERE5Dw++OADnnrqKV555RVatmzJ+vXrGT16NIGBgYwYMYLp06czb948PvnkE6pXr86BAwc4cOAAAGvWrCEqKoqZM2fSp08frFbrOfezZMkSYmNjWbJkCbt37+bGG2+kRYsWjB49GoBbbrmFY8eOsXTpUux2O+PGjSMlJeWi31dmZia9e/emY8eOrFmzhpSUFG6//XbuueceZs2ahcPhYNCgQYwePZqPPvqIvLw8Vq9ejWEYAAwfPpyWLVsyY8YMrFYrGzZswG63X3Q9F0shtgzF9eqF3+LP2L14KY369/V0OSIiImXCdfwY5vFjRaYZwSFYYuMw83Jx7dt7xjrWeg0K1j2wHzM7u8g8S0wsRkgorlMnMVOOFN1uQACWqtVLpO4JEybw4osvMmTIEABq1qzJtm3beP311xkxYgSJiYnUrVuXLl26YBgG8fHx7nUrV64MQFhYGDExMefdT3h4OK+88gpWq5UGDRrQr18/Fi9ezOjRo9mxYweLFi1izZo1tGnTBoC33nqLunXrXvT7+vDDD8nJyeHdd98lMDAQgFdeeYUBAwYwZcoU7HY7qamp9O/fn9q1awPQsGFD9/qJiYk8/PDDNGhQ8BldSi2XQiG2DNVu05zPa7Qm63gOjTxdjIiISBnJ/3oOee++VWSarWcf/B+fhHk0hay7RpyxTvDiXwDInvI0ru1biszze2wi9qv64li6iNyXXygyz9qmPQFTpl9yzZmZmezZs4fbbrvN3SIK4HA4CA0NBQq6Alx11VXUr1+fPn360L9/f66++uoL3lfjxo2LtNTGxsayefNmABISErDZbLRq1co9v06dOoSHh1/sW2P79u00b97cHWABOnfujMvlIiEhga5duzJy5Eh69+7NVVddRa9evbjhhhuIjY0FYNy4cdx+++2899579OrVi+uvv94ddsuSQmwZslgsJFx/O6v2HuEW03Q3y4uIiFRk9v6DsXW8osg0Izik4G/lKAJmvHPOdf0ffeqsLbEAtu69sDZqWnS7AQElUbK7f+ibb75J+/bti8w7HThbtWrF3r17+eabb1i0aBE33HADvXr14rPPPrugff35p3jDMHC5XJdQ/aWbOXMmY8eOZeHChXz88cc88cQTfP/993To0IGJEydy8803M3/+fL755hsmTJjA7NmzGTx4cJnWqBBbxnrUqMSRbxeyd0cdajX0TPO7iIhIWbJEVoLISmedZ/j4ursOnHXdavHnnhcWDmEX3yJ5PtHR0cTFxfHbb78xfPjwcy4XEhLCjTfeyI033sh1111Hnz59OHHiBBEREdjtdpxO5yXVUb9+fRwOB+vXr6d169YA7N69m5MnT170Nhs2bMisWbPIzMx0t8auWLECi8VC/fr13cu1bNmSli1bMn78eDp27MiHH35Ihw4dAKhXrx716tXjgQce4KabbmLmzJkKsRVd66qR1N8yn+1f+1Or4QOeLkdERETOYdKkSYwdO5bQ0FD69OlDbm4ua9eu5eTJk4wbN46pU6cSGxtLy5YtsVgsfPrpp8TExBAWFgYUjFCwePFiOnfujK+v70V1AWjQoAG9evXijjvuYMaMGdjtdh588EH8/f3/8hfd7OxsNmzYUGRacHAww4cPZ8KECYwYMYKJEydy9OhR7r33Xv7+978THR3N3r17eeONN7j22muJi4sjISGBXbt2ccstt5Cdnc3DDz/MddddR82aNTl48CBr1qxh6NChF/zeLpVCbBnzCwpka9W6+G9Y7elSRERE5Dxuv/12AgICeP7553n44YcJDAykadOm3H///UBBIPz3v//Nrl27sFqttG3blgULFmCxFIxg+uKLLzJu3DjefPNNqlSpwr59+y6qjnfffZfbbruNrl27EhMTw+TJk9m6dSt+fn7nXW/nzp20bNmyyLSePXuyaNEivv32W+677z7atm1LQEAAQ4cOZerUqQAEBASwY8cO3nnnHY4fP05sbCxjxozhH//4Bw6Hg+PHj3PLLbdw5MgRKlWqxJAhQ5g0adJFvbdLYZimaZb5XstQWloaoaGhpKamEhIS4ulyAFj36uvU+mIWzg++olL02X9eERERKU9ycnLYu3cvNWvW/MtwJZfm4MGDVKtWjUWLFtGzZ09Pl3NRzne+FDe76WYHHlCzz9XYTBc7vv3e06WIiIiIl/vhhx+YN28ee/fuZeXKlQwbNowaNWrQtWtXT5fmUQqxHhBRqyY/12nNhmOZni5FREREvFx+fj6PP/44jRs3ZvDgwVSuXNl944PLmfrEesihUffx3k/buC3fgb9dH4OIiIicXe/evendu7eny/A6aon1kO61Y6h3bD/r123564VFREREpAiFWA+JDw/k+Y1zSJ//ladLERERESl3FGI9xLDZOFKnMZW2/YqrYg8QISIil5EKPuiRlJCSOE8UYj0ooHNX6pxKYtuO3zxdioiIyCU5fZFRVlaWhyuR8uD0eXIpF6fpiiIPqnn1VWS/9RKJ3y+iScPani5HRETkolmtVsLCwkhJSQEKBsz/qztKyeXHNE2ysrJISUkhLCwMq9V60dtSiPUgW2Qk2xq0YVtKGtd4uhgREZFLFBMTA+AOsiLnEhYW5j5fLpZCrIelj32c975YxbCTGVQND/J0OSIiIhfNMAxiY2OJiooiPz/f0+WIl7Lb7ZfUAnuaQqyHdaoZTc2cU6xeu5mqV3X0dDkiIiKXzGq1lkhIETkfXdjlYf42C/9dNxvr/DmeLkVERESk3FCI9TDDYiGtSStq/raFtOw8T5cjIiIiUi4oxHqByj16UDPzOGvXbPJ0KSIiIiLlgleHWKfTyZNPPknNmjXx9/endu3aPPPMMxVuIOXILlfgsFg5tnSJp0sRERERKRe8+sKuKVOmMGPGDN555x0aN27M2rVrGTVqFKGhoYwdO9bT5ZUYIzCIg43akHj0FPlOF3arV3+3EBEREfE4r05LK1euZODAgfTr148aNWpw3XXXcfXVV7N69WpPl1bi7I8/zXtxLVmXeNTTpYiIiIh4Pa8OsZ06dWLx4sXs3LkTgI0bN7J8+XL69u17znVyc3NJS0sr8igP6keFUsfXZM2v2zxdioiIiIjX8+ruBI899hhpaWk0aNAAq9WK0+nk2WefZfjw4edcZ/LkyUyaNKkMqywZhmHw0sp3Wb29OuaQ7rpVn4iIiMh5eHVL7CeffMIHH3zAhx9+yK+//so777zDCy+8wDvvvHPOdcaPH09qaqr7ceDAgTKs+NI4W7aledJOdiSf9HQpIiIiIl7Nq1tiH374YR577DGGDRsGQNOmTdm/fz+TJ09mxIgRZ13H19cXX1/fsiyzxMT27En+D/N599ufaDhyoKfLEREREfFaXt0Sm5WVhcVStESr1YrL5fJQRaXLt3U7cgJDCPrxW1J14wMRERGRc/LqEDtgwACeffZZ5s+fz759+5gzZw5Tp05l8ODBni6tVBh2O/Z+g/Bz5DFn415PlyMiIiLitQzTi+8ckJ6ezpNPPsmcOXNISUkhLi6Om266iaeeegofH59ibSMtLY3Q0FBSU1MJCQkp5YovnWmaPPn1WtYdOMrXd/bFatEFXiIiInL5KG528+oQWxLKW4gF2HrgCC/9512G3zGcHvXiPF2OiIiISJkpbnbz6u4El6t6+7bx0vrPWPLdj54uRURERMQrKcR6IVvHK8gLDqXWmqXsPprq6XJEREREvI5CrBcybDb8+w2kT/I2PlulO3iJiIiI/JlCrJfy6z+YAEce2Yu/JS1Hw22JiIiI/JFCrJeyxMbhHDCUkzY/vty0z9PliIiIiHgVhVgvFn7/wwR27cHsdXtwuir0IBIiIiIiF0Qh1suNqGzQcMcalu857OlSRERERLyGQqyXq7HjVx5JWMTnP+sCLxEREZHTFGK9nE+/gfg58gj+5Uf2Hk/zdDkiIiIiXkEh1stZomOxtu3A0MOb+GjtHk+XIyIiIuIVFGLLAd8BQ6h76jDbV/xCRm6+p8sRERER8TiF2HLA2r4TecNGctSq4bZEREREQCG2XDCsNiJH30Wz5g2YvW4PLlPDbYmIiMjlTSG2nDBNk7uOrKfOjrWs+u2Ip8sRERER8SiF2HLCMAyiE3dx26F1fLh2t6fLEREREfEohdhyxKf/EOJPJHF0wwb2n0j3dDkiIiIiHqMQW45Y23WEytHckLyZj9dpuC0RERG5fCnEliOG1YrPNddy5eHtfP9rAll5Dk+XJCIiIuIRCrHljL3/YBxPPMsJl5WvNu/3dDkiIiIiHqEQW85YIiKJ6nIF3etX4aN1uzE13JaIiIhchhRiyyHXieM8svht/H9L4Od9KZ4uR0RERKTMKcSWQ0ZoGIEnj/L349v5SMNtiYiIyGVIIbYcMqxW7P0G0enAFtbt2MfBkxmeLklERESkTCnEllP23v2xOp1ce2wHH/+q4bZERETk8qIQW05ZKlXG1ukKhp7aw5xN+zTcloiIiFxWFGLLMd8x4wiYMp3M3Hzmb030dDkiIiIiZUYhthyzREUTF1OJK2tF8dFaDbclIiIilw+F2HLOsWo5T3zwNCmHU1iTeNTT5YiIiIiUCYXYcs5SvyG27CyGp+/hQw23JSIiIpcJhdhyzhIRia1TVwYe2siynYdISs30dEkiIiIipU4htgKw9x9M8JFDtMs6wn9/3Kq+sSIiIlLhKcRWANZWbTGqVOOuSiZfb0nUuLEiIiJS4dk8XYBcOsNiIfCN92nh58fw7zfw/KKN1K0cSuvqlT1dmoiIiEipUEtsBWH4+WE6HYwNyaJltUo8OOdnktOyPF2WiIiISKlQiK1AHMsWk//4A7zgewR/u5UHPl9FTr7T02WJiIiIlDiF2ArE1uNq7INvwDpjKq9VzuS3Y2k8/c06XeglIiIiFY5CbAViGAa+dz+AvXd/It6YystxBbej/WCNxo8VERGRikUXdlUwhsWC77jxmHm5NDu6lxHt+zL1h03UjQqlfY0oT5cnIiIiUiIMs4L/1pyWlkZoaCipqamEhIR4upwyYzodYFhwAuPeW8TGkzl8OLInVcICPV2aiIiIyDkVN7upO0EFZVhtGBYLxvYt/GvuC7RKP8wDn68kO9/h6dJERERELplCbAVnqV0PW83aTPxlNj77djNxvi70EhERkfJPIbaCM/z88P/XC9hq1GT6hs9IWPMr767e6emyRERERC6JQuxlwAgIJGDyNHzj4nhp/xKm/bCJlb8le7osERERkYumEHuZMIJD8J8ynapTXqJTrVge/fIXDpzM8HRZIiIiIhdFIfYyYgkLx16lCpN7NuSfG79i4nsLycrThV4iIiJS/nh9iD106BB/+9vfiIyMxN/fn6ZNm7J27VpPl1WuBTnz6JCVzLhF/+O5T3/QhV4iIiJS7nh1iD158iSdO3fGbrfzzTffsG3bNl588UXCw8M9XVq5ZomOJWTqq8RYnQz6ZDrvL17n6ZJERERELohX3+zgscceY8WKFfz0008XvY3L9WYHxeHc9xsn7hnNb/Zg8p57mSvqV/N0SSIiInKZqxA3O5g3bx5t2rTh+uuvJyoqipYtW/Lmm296uqwKw1qjFmFTX2FX886Mn/8r+46ne7okERERkWLx6pZYPz8/AMaNG8f111/PmjVruO+++3jttdcYMWLEWdfJzc0lNzfX/TotLY1q1aqpJfY80nPy+ds7P3Dd2vnUiQ6n9hUdqNSmLZbISp4uTURERC4zxW2J9eoQ6+PjQ5s2bVi5cqV72tixY1mzZg2rVq066zoTJ05k0qRJZ0xXiD2/5LQs9j35OFG7t1A5p6BFNjeiMiFPT8GnYWNcJ09gBAZh+Ph4uFIRERGpyIobYm1lWNMFi42NpVGjRkWmNWzYkM8///yc64wfP55x48a5X59uiZXziwkJIOY/08jKc7Bk5Qa2LVtByP5dfLdgKz2Pwk1LPsC26kcsdephbdik4NGiNZaISE+XLiIiIpchrw6xnTt3JiEhoci0nTt3Eh8ff851fH198fX1Le3SKqwAHxv9urehX/c2bE8+yZH1v/Heml0szqvGkPb96ZJ3nMifV5D/xcf4PvAYPv0H49y6Ccfmjdiv6I6lir4wiIiISOnz6u4Ea9asoVOnTkyaNIkbbriB1atXM3r0aN544w2GDx9erG1odIJLl5mbz4JtB/hs/W/sOHKKmBB/htWJ5JrmtYmOqUzeV1+Q+9p/ICcHS4PG2K+8GluPq9RKKyIiIhesQvSJBfj6668ZP348u3btombNmowbN47Ro0cXe32F2JJjmiZbD5/k8w17WbAtkTyHk651YrmuRS06xoVhrl6BY/G3OFavxOfvt+H7t1sxU1PBasUICvJ0+SIiIlIOVJgQe6kUYktHRm4+C7Ym8tn630hISSUuNICHezXnynpVMNPTwDQxQkLJnfkaeR9/gK1DJ2xX9sbWoTOGj7p7iIiIyNkpxBZSiC1dpmmyOekEb6zYzk97kulZvwqPXdWCqGB/AFxHU3AsXUT+D9/i2rkDAgPxu/ch7Fdd4+HKRURExBspxBZSiC0bpmny7faDTPl+A/lOF/f3aMqQFjWxGIZ7GdeB/eT/8B22Dl2w1m9I/qKFOHdsxd6zN5YGjTH+sKyIiIhcnhRiCynElq3U7Dym/rCJuZv20apaJZ7q24qakWc/7nlffkbeBzMxjx/D2qodfg89jiU6towrFhEREW+iEFtIIdYzVu9P4ZlvfuVwWhajOzXg1o4NsFvPvMux6XTi+Hk5uS+/gJmZQcB/3sRaq44HKhYRERFvoBBbSCHWc3LynbyxYhuzft5JjchgJvRtTfOqZx92y8zIIO+rL/C5YTiG1YqZmYERqBENRERELjfFzW5nNo2JlBA/u5Wx3Zvy0aie+NmtjHhvCZO/W09mbv4ZyxpBQfjedAuG1Ypjy0YybhpI3vy5VPDvWCIiInKRFGKl1NWPDuO9W67koZ7N+XLTPga/+R1LdyWdc3lrjdrYu/Ukd+pksh8di+tIchlWKyIiIuWBuhNImUpKzeTZhetZ/lsyVzesyqO9WlApyO+syzrW/EzOi89iZmbg/8wL2Fq0LuNqRUREpKypT2whhVjvY5om32w7wL8XbcThcjHuymYMblbjrENsmRkZ5L7/Nr5/uxUjKBgzPx/DbvdA1SIiIlIW1CdWvJZhGFzTuDpzR19Nj7pxTFqwjucXbTz7skFB+N15H0ZQMK6jR8j8+xDyvp6jvrIiIiKXOYVY8ZiwAF+e6d+WR3o154O1u/lma+J5lzf8A7G17UjuS8+R/chYXEcOl1GlIiIi4m0uKsQeOHCAgwcPul+vXr2a+++/nzfeeKPECpPLx81t6nBNo2pM/GYdu4+mnnM5IygIvwcfx/+5/+A6sI/M228mf8WyMqxUREREvMVFhdibb76ZJUuWAJCcnMxVV13F6tWr+ec//8nTTz9dogVKxWcYBk/2bU3VsEDGfbGKjLMMwfVHtrYdCHzrI+xX9sZSpRoArhPH1cVARETkMnJRIXbLli20a9cOgE8++YQmTZqwcuVKPvjgA2bNmlWS9cllIsDHxouDO3I8M4en5q/9y0BqBAXh98BjWGvUwnQ6yR53F1l3jyT/pyWYLlcZVS0iIiKeclEhNj8/H19fXwAWLVrEtddeC0CDBg04fFj9FOXi1IgM5ul+bVmccIh3V+8s/ooWC773PIjhH0DOxMfIuu0m8r+dj+l0lF6xIiIi4lEXFWIbN27Ma6+9xk8//cT3339Pnz59AEhKSiIy8uy3FRUpjp71qzCyQz3+s2QLaxOPFmsdwzCwtWlPwNQZBEx/C6NKNXLfe9s9X2FWRESk4rmoEDtlyhRef/11unfvzk033UTz5s0BmDdvnrubgcjFurdbE1pVr8Qjc38mJT37gta1Nm5KwL9eIPCN9zCsNlwHE8m8aSC5H72DmZFRShWLiIhIWbvomx04nU7S0tIIDw93T9u3bx8BAQFERUWVWIGXSjc7KJ+OZ+Zw4/8WUSUskLdu7obdenGjwbmOppD3/v/I//Zr8PHBZ+D12IfciCU8ooQrFhERkZJQqjc7yM7OJjc31x1g9+/fz7Rp00hISPCqACvlV2SgHy8O7siWpBO89MOmi96OpXIUfg88RuD7c7D3G0TeFx+TN/P1EqxUREREPOGiQuzAgQN59913ATh16hTt27fnxRdfZNCgQcyYMaNEC5TLV/OqkTzUs/BGCNsOXNK2LJUq4/ePsQR9+CU+I+8AIG/BPLKffwbXseL1vRURERHvcVEh9tdff+WKK64A4LPPPiM6Opr9+/fz7rvvMn369BItUC5vw1rX5ppG1Zi0YC17jqVd8vaM0FAsEQUXHxpWK86fV5A56gbyPvsI06ELwERERMqLiwqxWVlZBAcHA/Ddd98xZMgQLBYLHTp0YP/+/SVaoFzeTt8IoUpYIOM+/+sbIVwIe+9+BM76BPtV15D7+nSy7rwF19GUEtu+iIiIlJ6LCrF16tRh7ty5HDhwgG+//Zarr74agJSUFF08JSXu9I0QjmVmM6EYN0K4EEZwCH5jHybg1VlYGzXBKGylNbOzSmwfIiIiUvIuKsQ+9dRTPPTQQ9SoUYN27drRsWNHoKBVtmXLliVaoAj8fiOERQmHeHf1rhLfvrVuffzGPV7QxWD7FjKGXUve57M1xqyIiIiXuughtpKTkzl8+DDNmzfHYinIwqtXryYkJIQGDRqUaJGXQkNsVSwvLdnEe7/s4s2bu9K6euVS2YeZlkruzNfJ/+oLLDVq4zv2IWzN9OVMRESkLBQ3u110iD3t4MGDAFStWvVSNlNqFGIrFofLxZ2zf+K3Y2nMHtWLqGD/UtuXM2E7OdOfx7VjK36PPIW9d79S25eIiIgUKNVxYl0uF08//TShoaHEx8cTHx9PWFgYzzzzDC6X66KLFvkrNouFKQPbYzEMHp77M/nO0jvfrPUbEvDyW/g99E9sHbsA4Pxtt7oYiIiIeIGLCrH//Oc/eeWVV3juuedYv34969ev5//+7/94+eWXefLJJ0u6RpEiIgP9eGFwB7YknWDakou/EUJxGBYL9r7XYoSEYmZnkzXuLrLuHoVza+nuV0RERM7voroTxMXF8dprr3HttdcWmf7ll19y9913c+jQoRIr8FKpO0HF9eHa3Uz5fgPPDmhL/ybxZbJP545t5Ez/N66E7dh698P3jnuxhIX/9YoiIiJSLKXaneDEiRNnvXirQYMGnDhx4mI2KXLBbmpdm/5NqvPPr9bw9DfrSnQM2XOxNmhEwMtv4/vAYzhW/kTOv58u9X2KiIjImS4qxDZv3pxXXnnljOmvvPIKzZo1u+SiRIrDMAye6d+Wf/ZuyTfbDjDkze9YsSe59PdrteLTfzCBsz7B754HAXDu3onrQGKp71tEREQKXFR3gmXLltGvXz+qV6/uHiN21apVHDhwgAULFrhvSesN1J3g8pCUmsmkBev4eV8K1zaN5+GezQnx9ymz/Wf980Gc61bj87dR+Nz4dwy7vcz2LSIiUpGUaneCbt26sXPnTgYPHsypU6c4deoUQ4YMYevWrbz33nsXXbTIxYoLDeS1YVcw4ZrW/LDzEEPe+o6lu5LKbP/+Tz6Lz9Bh5L3zFln/+DuOLRvLbN8iIiKXo0seJ/aPNm7cSKtWrXA6nSW1yUumltjLz5G0LJ5Z+Cs/7UnmmkbVeOSqFoQH+JbJvp17dpHz0mTMlCMEvv8Fhk/Z7FdERKSiKNWWWBFvFh0SwMvXd+Zf/duy/Ldkhrz5Hd/vOFgm+7bWrkvAf94k4KXXMHx8cR07Sv6yxZTgd0URERFBIVYqKMMwGNA0njmje9OiaiQPzfmZh+as4nhmTunv22rFUqUaAI4fviPn6cfJ/ueDuI4cLvV9i4iIXC4UYqVCqxTkx9QhHfn3oPas3X+UIW9+x4KtiWXWMupzw3D8Jv0b156dZN56E3mffqg7fomIiJQA24UsPGTIkPPOP3Xq1KXUIlIqDMOgd8NqtI2PYsp3Gxg/bzULtx3giT6tiAr2L/X927t0w9ayNbn/e43ct2dg69AZo1rZ3JxBRESkorqgC7tGjRpVrOVmzpx50QWVNF3YJX/2w85D/Gvhr+Q5XYzr0YxrGlfHz24tk327jh/DElkJMy+PvHfexNalG5YGjTEMo0z2LyIi4u2Km91KdHQCb6QQK2eTmp3H84s28tWW/fjbrXStE8tVDarSuVYMAT4X9APFRXFu30L2pPGYR1OwxNfE3qc/tl59sURElvq+RUREvJlCbCGFWDmffcfTWZRwkEUJh9iefAo/m5XOtWO4qn4VrqgTS5Bv6d20wHQ6cf66hvyFX+FYsQxr/YYE/OfNgv66TieGrfTDtIiIiLdRiC2kECvFdfBkBosSDvH9joNsOXwSH6uFTrWi6VW/Kt3qxhLiV3p3ADPTUnGdOI61Ri2cWzeRPeFRbFf1xd67P9YatUptvyIiIt5GIbaQQqxcjKTUTBYnHGLRjkNsOHQcm8WgQ41oejWoQo+6cYSV4s0TXEmHyPtiNvmLFkJ6GpaGTfC57ibs3XuV2j5FRES8hUJsIYVYuVRH0rP5IeEQixIOsi7xGBbDoG18Za5qUJV+Tarjby+dn/3NvDwcq34if+FXWBs0xnfEaFzHjuI6mIi1WUsMi0bIExGRikchtpBCrJSkYxk5/LDzEIsSDrF2/1FqVQph6pCOVI8IKtX9mqaJYRjkffYRuTOmYcTEYut6JfYrehSMbqBAKyIiFYRCbCGFWCktu4+mMu6LVZzIzOVfA9rSvW5cqe/TNE2cmzfgWLQQx4plmKdOYr/uZvzuug8zPx8sFgxr2QwXJiIiUhqKm93KVfPNc889h2EY3H///Z4uRYQ6lUP5YERP2sRX5r7PVvLKsi04XaX7ndAwDGzNWuI3bjyBn8zHf+oM7H0HAJD//Tdk3tifnGlTcKxbrTuDiYhIhVZuQuyaNWt4/fXXadasmadLEXEL9rPz0pCO3Ne9CW+v2sGYT5ZzKiu3TPZtWK3Ymrdyj15gbdQEW68+ONasIvuRe8kY2pe8uZ+WSS0iIiJlrVyE2IyMDIYPH86bb75JeHi4p8sRKcIwDG7t2IDXhl3BjuST3DRrMVsPnyjzOqw1auF3530Evj+HgBmz8Ok/GEtUNACOtb+QPXkC+cuXYebmlHltIiIiJa1chNgxY8bQr18/evX66yGGcnNzSUtLK/IQKQvta0Qz+9ZeRAT4MuK9pXyxYa9H6jAMA2u9hvjefje2Tl0BMHNzcO3ZRc6ER8gY0oecF/8P52+7PVKfiIhISfD6WwLNnj2bX3/9lTVr1hRr+cmTJzNp0qRSrkrk7GJCApj5t+5MWbSRSd+sY1PSccZf3RJfm2cvtrJ37oa9czdcB/aTv+R78ufPxVK1GtZadTAz0sHHB8On9Ma+FRERKWlePTrBgQMHaNOmDd9//727L2z37t1p0aIF06ZNO+s6ubm55Ob+3icxLS2NatWqaXQCKXNzN+3j2YW/UqdyKC8O6UBcaKCnS3IznQ5wODB8/ch5dRqORd9g73st9v6DscSW/igLIiIi51IhhtiaO3cugwcPxvqHIYOcTieGYWCxWMjNzS0y72w0xJZ40vbkk4z7YhWZuQ6eG9iOTrViPF3SGVwHEsn76nPyF34NWZlY23fG99Z/YK1dz9OliYjIZahChNj09HT2799fZNqoUaNo0KABjz76KE2aNPnLbSjEiqelZucxft4vrPztCGO6Nua2Tg2wGIanyzqDmZ1N/g/fkT/vM/wefhJrnXo4dyVgiYrBCA31dHkiInKZKG528+o+scHBwWcE1cDAQCIjI4sVYEW8Qai/Dy9f34U3VmzjlR+3sjnpBP8a0JYQPx9Pl1aE4e+PT7+B2K+5FqMwZOdM/T9ce3/D1uMqfAYOxdqgsYerFBERKVAuRicQKe+sFoO7rmjMy9d35teDx7h55mLWJR7F5YU/hBh/aCX2nzwNnxGjcW76lawxt5J590jMjAwAXIeTMFNT8eIfc0REpALz6u4EJUHdCcTbHDyZwbg5P5Nw5BSVg/zoVjeOK+vG0Ta+Mj4eHsXgXEynE+eaVeR9PRf/ic9h2Gxk3f8PnJs3QGAQlrgqWOKq4jN0GNbGzXClnoLcXIxKlTEs+q4sIiLFVyH6xJYEhVjxRg6Xi40Hj/PDziSW7kri4KlMAn1sdK4dQ4+6cXSpHeN13Q3+zLl3D67EfbiSDmEmHcSVdBCfv92KrWUbcj96h7y3XgW7D5bYOIy4qtg6dMZnwBBPly0iIl5OIbaQQqx4O9M02X0sjSU7k1iyM4ltySexWQzaVK9Mj3pxdK8bR0xIgKfLvCCuY0dx7U7AlXQIV9JBXEmHsDZsjO/fb8N15DCO5Uux9xuM4efn6VJFRMTLKMQWUoiV8iY5LYuluwoC7drEozhcJo1iwulRL44edeOoUzmkSL/V8ib/uwXkPP8vjJAQ7NcPx+faIRgB3jOGroiIeJZCbCGFWCnP0nLyWLEnmSW7kli+J5nMPAdVwwLp26ga/+jSCLu1fPY3dSUdIm/2u+R/+zX4B+D30BPYu3TzdFkiIuIFFGILKcRKRZHncLIm8Sg/7Exi7sa9dK8bx3MD25fbIAvgSjlC3ifvY+8/GGuNWjh37sASHatxaUVELmMKsYUUYqUiWroriQe/WEW3unFMKedB9o8y77wF14FE7NcOxef6m7FERHq6JBERKWPFzW4V4/98IpeZ7nXjmDqkIz/uPswjc38m3+nydEklwn/KdHyG3Ej+11+QOXwwOa+8iJmV6emyRETECynEipRT3QqD7E97knm4ggRZS2gYvrfdRdCHX+Jz0y04t24CH18AzNRUD1cnIiLeRCFWpBzrWieWl4Z0ZPmeZB6eUzGCLIARHILvLbcT8OosDJsN18FEMm7sX9Aym6YwKyIiCrEi5d4VdWKZNrQjK35L5qE5qypMkIXfb4FrVIrCZ8Ro8r/9moxbriNv7qeYDoeHqxMREU9SiBWpALrUjmXa0E6s/O0ID36xijyH09MllSjDzw/fm24h8J3PsF/Rg9xXXiTvf695uiwREfEgjU4gUoGs+C2Z+z9bSfsaUUwd0hEfm9XTJZUK5+6dGGHhWCpVxrFuNZaoaCzV4j1dloiIlACNTiByGepcK4b/XNeJ1ftTGPfFKnIrWIvsadY69bBUqoxpmuTOeoPM224i59VpmOlpni5NRETKiEKsSAXTqVYM/7muc0GQ/bziBlko6DMb8OJ/8Rl5B/nz55J5y3Xkzfsc01lx37OIiBRQiBWpgDrWjGb6dZ1Zk5jCA5+vrNhB1scX35tHEvjuZ1g7diF//lxPlyQiImVAIVakgupQM5qXr+/MusRj3P9ZxQ6yAJbISvg/8hQBL7+FYbXi3LmD7CcfxnUw0dOliYhIKVCIFanA2teIZvr1nfn1wDHu+2wlOfkVO8hCQcssgJmZgXP3zoL+sq9Px8xI93BlIiJSkhRiRSq49jWiePmGzqw/cIz7PltxWQRZAFvLNgTO+hifv91K/rzPybjpWhyrV3m6LBERKSEKsSKXgXbxUbxyQ2c2HDrOfZ+t4GhGtqdLKhOGrx++f7+NwPe+wGfg9Vjq1APAsfInXAf2e7g6ERG5FBonVuQysjbxKPd+uoKcfAft4qO4pnF1etavQpCv3dOllRnTNMm642+49u7B1rkbPsP+jrVhE0+XJSIihYqb3RRiRS4zadl5LEo4xPytiaxNPIqvzUK3unH0a1ydzrVisFsr/g80Zl4e+d9/Q94n72MeTMTavBX+Tz+PERTk6dJERC57CrGFFGJFzi05LYtvth1gwdZEdqakEurnw1UNq9KvcTVaVK2ExTA8XWKpMp1OHCt/xLnmZ3wfeAwA5+qVWNu0x7DaPFydiJQk0+nAuW0Lzp9X4Dp8CCMgEN97HsTw8yN/xTJIS4WAQIzAQIzAIIy4qlhCwzBdLjAMjGL8e2iaJricYLFiGAZmehpmdjY48jHz88GRjxFRCUt4BGZWJubx42C1gMUCFivY7VjCIwq2lZEOhlEw3Vo432otVh3lnUJsIYVYkeLZdTSVBVsT+WbrAQ6nZREXGkDfRtW4pnF16lQO9XR5ZcK5ZxdZd/wNIyYWn+uHY+8zAMPPz9NlichFMh0ODJsNMzuLzOGDMVNPFdyyulYdzOwsAl56HcNuJ+vRsTjX/lJkXd9x4/HpN4j8hV+R8+JkCCwMuH7+WOo3xP+RpzBzcsi8eSCmIx/yCkIqpkngJ/OxRFYie8KjOJYvLbJdn9H34Dvs7+T/tISciY8VmWepWZvAtz4EIL1fN8jJKTI/4I33sNauR84rL5L/7ddgtWHYbGCzYR8wFN/hI3HuSiDnhX+BtWC6YbNhhIXj/9T/AZD9/DOYp05hWAwwLGAY+NxyO9badclf8n1BvYZREKwNA2vTFvj0H1yyH8xfKG52U1ODiABQt3Io93Vvyr3dmrD+wDHmb03k0/W/8faqBOpHhXJN4+r0bVSN6JAAT5daaqy16xLw+nvkffweuf+dSt67b+EzfBQ+Q4d5ujSR83Lu3VPQWujjA3Yf8PHBCAq6LH9RcB1IxPHzchw/L8eVnETg+3Mw/AMKglr9hljqN8KwFO02FTBlOqbTAVlZBS2kmZkYkZUAsDRqiu/YhzAzMiArEzM7C0u1+IIV7Xbs192EYbOD3V4QGu0+GAGBAPgMH4m9/2Cw2cBux7DZMSpHAWBt1hL/qTPA5QKns+DvH740+41/uiAUu5zgdIHLhSUqBgBbp65YomMxHQ5wOsDhwFqvAQBGQADWRk3B6SiY73Bi+Pu7t2sYFkzDwHSZYOaDy4TT7Zm5OZhpqQW1FE4z01JL+BMqOWqJFZFzyne6WL4nmQVbE1m2O4k8h4sONaMZ1ro2V9SOxWqpuD9ruQ4nkffpBxiVo/C9aQSug4nkf78Aa6t2WBs2KQgLIh5g5ubgXLcaZ8J2fEf9A4CMG/phHj9WZDn/qTOwNW9F7tszyPtiNth9CsZR9vHB3vNqfEfdietwEjnT/40RFlHQQhkejhEegf2qawBwnTxR0PpYOP6yN3OdOknW2NGYhw6A3QdrqzbYOnTBfs3AgtZKKTfUnaCQQqxIycjIzWdRwiE+/XUPWw6fpEpYIDe2qsWgZjUJ9a/4gc7x83Jypjxd0Crh54e1aUvs3Xti7zPA06VdMtPpgMxMjJBQTJcLx/ffYGZnYa1bH0vd+uUiwFR0Zl4ejh++w7HyRxzrfoGcHCzV4gl4dSZGQCDOPTshJwczLw/y8jDz87A2bYElNAzHlo24ErYVzMvPh7xcLPUaYu96Ja7kJHJfnYbr1EnMwgcWC8Fzvwcgc9SNuBL3QUCAO+j63XUf1kZNcWzegGvnDozwcIzQcIywMCyVojFCS6/7kWmakJONmZaKc9MGHD8vxzx5goCpMzBNk9w3XsHWtAXWlm2KtD5K+aIQW0ghVqTkbU46wex1u/l2+0GshsE1jaszrHVt6keHebq0UmW6XLj27MK5fg2OX9dgqVodv3sexHXiOLn/nYq1VVtsrdphiY3zdKmYuTmYGRmYJ09gnjiOefIElpq1sdZrgHPrJnJnvfH7vLRULLXrEvj6e5imSUb/7gVhx+ksuNCkbgP8J0zGUqkyZl6uQm0ZcSUdxLkrAXu3npgOBxnX9cVSLR5b527YO3f9/SftEmbm52PYC4bdc6xfi3k0BfPUCcxTp3CdOoHPDX/DWqMWuR/MJO/9mZCX617X1rsf/o88hevQAbLGP1DQuhsahhFW0MLrM+ofGIaBc8dWwCg8T9MhIwNrq7ZYKkfhWLWc/OVLICMDMyMdMzMDW5v2+N4+Buf+vWTd+nv3Hkv9Rtg6dMbnb7ee0UVAyi+F2EIKsSKl53hmDl9s2Msn638jJT2bVtUqMax1ba6sV+WyGKrrNOeeXeS89ByuhG3gcmHEVsHWpRt+d96H6XJhnjwB2VmY2dmY2VmQnY21TTsMq4385UsLWrqysjBzCubbe1yNrU17HL+sJPfdt+AP/d4sNWrhP/E5TNMkc2if3/vEOZ3gcBD4zqdYqlQje/IEHIsWFqnTZ+Qd+P79Npw7d5A3+12MiEiM8AiM8Egs0THYWrcDCkIMhoHrt104t27GuX0rfo88iWGzkTXuLlwpR7A2boq1SXOsjZpiqVELw2r1wJGvWEzTxJWwvaC1deWPuPbugYAAgj5fiOHji5mdheHvXX3S3S2jp05hpp4E/wCs8TVxHU0h74vZmCdPYqaexEw9VXB+vvE+ABm3XFfws/8f+P/rRWwdu5A3fy7533yFERRc0K83KBhr42bYr+qLmZ2FY9VyjMAgLHXqYSnstyoVi0JsIYVYkdLncLlYujOJj9btZm3iMSoH+XF9y1oMbVGLSkGXz9X9ZkY6jo2/4vx1DQB+9z6EmZ5GxqCrzlg28POFWMLCyZ40Huem9eDvj+EXAP7++Fx3E/ZuPXHu2Er+/C8LLgqxWsFqwxIbh8+g6wHI/eidgtYnm819JbK9ey+MoGCc27dgnjz5h6AaUSL9ePOXL8O5cR3OrZtx7U4ApxP/Z1/E1qFLwT4zM7A2aKIxd4vBTE3FdTARMz0VW4cumBkZZAztDf4B2Dp0wda5K7Y27b0uuJYE15FkzLRUDD9/CAoqGNJK/cylkEJsIYVYkbK1KyWV2b/u4est+3E4XVzdoCo3talD07iIy2J8wz8znU6ca1aBfwCGv39BIPHzLwiX5bz10szJwblzO9badTECg8h+/hkcC78umOkfgOHri8+Nf8fnhuE4t28h9/WXwdcXw9cP/PywREXje/sYAPI++QAwwce3ILA7HNi69cQSEYlj9Sqc27cUtkg7MR0OrM1aYO/SHVfSQXLffdvdUo3pwggKwe+hfwKQ+9G7kJlesF27D4aPHVvnblhi4nDu+w3XwUSM01fz+/hgVKqMJTq25I5RXi6upEMYVhuWatVx7t1DztTJmAcT3Vd9G2HhBH1e0Gru3L0TS81al+WoAiKnaYgtEfGIulGhPNmnFfd1a8KXm/fx8bo9LHh3CY1iwhnZoR69G1bzdIllyrBasXXo4ukySoXh54etWUv3a7+HnsAcdgvObVsKBnnPzcFSv2HBTB8fjJhYyM3FzM2BY+m4nE73unnz5xZ0u8jNKRjex2oruB1wRCTOLRvJX/h1QYuzzYphtWGEhQEFFzyZhw+6W6KxWAqGDCrk3LAW16GDkJcH+XmYeXlY4mtiiYnDsWwxee++VeQ92Xr2xv/xp3EdTiLztmEYQSEYISEFP20Hh+A3aQqGxUL+9wsws7IwgkMKH8FYqtfACAgk/8cfyF/wZUEra/JhME13X1EjKAhLXBUs7TthqVodS7XqWOJ+/2/CWqdeKXxSIhWTWmJFpFS5TJMVe5L5cO1uVu49wp1dGnJnl0aXZauseBczL6+gP+fpK/rzcjF8/bDExmGmpZL//TcFFxalp2Gmp0FODv6TpgCQNe4unFs2FvRFLuT31P9h79aT/MXf4lj6PUbV+N+DavUaWMLCPfVWRcoVdScopBAr4j3eXrWD6Uu3MLxNHR7q1bzC39ZWKjbTNAsu2EsvCLpGpcoKqiIlQN0JRMTr3NaxAcG+dv7v2/Vk5Dl4qm8rbBoWR8opwzAgILDg7kzRMZ4uR+SyoxArImXqhla1CfSx8+TXa8jMzWfyte3wsZXvC5xERKTsqQlERMpcvybVmTq0Iz/uPsx9n60kK8/h6ZJERKScUYgVEY/oXjeOV27owvqDx7jr459Iz8n3dEkiIlKOKMSKiMe0rxHFGzd15bdjadz+4TKOZ+Z4uiQRESknFGJFxKOaVYnkf8O7czQjm1vfX8aRtCxPlyQiIuWAQqyIeFzdqFBm/a0HeU4nI99fyv4T6Z4uSUREvJxCrIh4heoRQcz6W3d8bVZGvb+UnSmnPF2SiIh4MYVYEfEa0SEB/O9v3akc5M+tHyxj06Hjni5JRES8lEKsiHiViABf3rq5G3UqhXDHRz/yy74jni5JRES8kEKsiHidYD87M4ZdQcuqlRjzyQqW7EzydEkiIuJlFGJFxCv5221Mv74z3evG8uAXq/h6y35PlyQiIl7Eq0Ps5MmTadu2LcHBwURFRTFo0CASEhI8XZaIlBG71cJzA9vTv2k8//xqDaM/XMa8TfvIzNWNEURELneGaZqmp4s4lz59+jBs2DDatm2Lw+Hg8ccfZ8uWLWzbto3AwMBibSMtLY3Q0FBSU1MJCQkp5YpFpDS4TJP5WxKZt3kfa/YfxddupWe9KgxoGk+7+CisFsPTJYqISAkpbnbz6hD7Z0ePHiUqKoply5bRtWvXYq2jECtSsRxOzWLB1kTmbd7PvhPpVA7yo1+T6vRvEk/dyqGeLk9ERC5RcbObrQxrumSpqakAREREeLgSEfGU2NAAbuvUgFs71mfr4ZN8tWU/czfuY9bPO2kYE8aAJvH0aVSNyEA/T5cqIiKlqNy0xLpcLq699lpOnTrF8uXLz7lcbm4uubm57tdpaWlUq1ZNLbEiFVi+08XyPcl8tWU/y3YlYZrQqVY0A5rG071uHL42q6dLFBGRYqpwLbFjxoxhy5Yt5w2wUHAx2KRJk8qoKhHxBnarhR714uhRL45TWbl8t+MgX23ezyNzfyHY187VDavSqlolKgf5Ex3sT+UgPwJ97Z4uW0RELkG5aIm95557+PLLL/nxxx+pWbPmeZdVS6yInLbveDpfb9nPgm0HOHQqs8i8AB8bUUH+VA72o3KQP1HBfgWvC59XDioIuz5qxRURKVMV4sIu0zS59957mTNnDkuXLqVu3boXvA1d2CUiANn5Do6m53A0I5uUjBxS0rMLnqdnczQjh6Pp2aRkZJPrcBVZL9zfh/rRYYzt3oTGseqPLyJS2ipEd4IxY8bw4Ycf8uWXXxIcHExycjIAoaGh+Pv7e7g6ESlP/O02qkcEUT0i6JzLmKZJek4+KX8Itynp2SzcfoCbZ/3AtU3jubdbE6KC9e+PiIineXVLrGGcfezHmTNnMnLkyGJtQy2xInKpHC4XX2zYy6s/biXH4eTWjg24pV09/OzqaiAiUtIqRHeCkqAQKyIlJS0njzdXbOfDtbupHOTPfT2a0qdh1XN+4RYRkQunEFtIIVZEStr+E+m89MNmluxKokWVSB7q1ZymceovKyJSEoqb3SxlWJOISIUQHxHMtOs68cZNV5CZ5+Bv7/zAP79azZH0bE+XJiJy2VBLrIjIJXC6TOZs3MsrP24lO9/BrR3qc0v7evjbvfq6WRERr6XuBIUUYkWkLKTn5PPWqu18sGY3kYG+3N+9KX0aVVN/WRGRC6QQW0ghVkTK0oGTGby0ZDOLEw7RLC6CB65sSp3KoQT42LBZ1INLROSvKMQWUogVEU9YvT+FFxZtJCEl1T3Nz2YlwMdGoI+NAF87gaef+9gI9LG75wX62gjwsRPkY6NNfGUqB2lcWhG5fCjEFlKIFRFPcbpM1h04yqmsPDLz8snKc5CZ5yh4nlvwPOv0a/fz36e5TPCxWhjUrAajOtYnLjTQ029JRKTUVYg7domIlGdWi0G7+KiLWtc0TVKz8/h8w17eW72TLzbupV+TeG7rWJ/4iOASrlREpPxRS6yIiJfLynPw+YbfeOeXnRzPzKF3w2rc1qkBdSuHero0EZESp+4EhRRiRaSiyHU4+XLTPv63KoHDaVlcWS+O0Z0a0ig23NOliYiUGIXYQgqxIlLR5DtdzN+ayNsrd5B4MoMutWIY3bkBLapW8nRpIiKXTCG2kEKsiFRUTpfJd9sP8ObKHew5lkbb+MqM7tSQdvGVNT6tiJRbCrGFFGJFpKJzmSZLdibx5srtbE8+RfMqEdzeqSFX1I5RmBWRckchtpBCrIhcLkzTZMVvybyxYjsbD52gSlggzeMiaBwbQeO4cBpEh+l2uCLi9TTElojIZcYwDLrUjqVzrRjWJB5lyc4ktiSdYFHCIfKcLqyGQZ3KITSJi6BxbDiNYyOoUzlEdxITkXJJIVZEpIIxjILxaU+PUZvvdLH7aCpbDp9kS9IJNh06wZyNe3GZBXcRaxAdRpO4glDbJDaCauGB6oYgIl5P3QlERC5DWXkOdhw56Q62Ww+f5OCpTABC/Ow0igmncWw4jWIjaBwTTkyIv4KtiJQJ9YktpBArIlI8J7Ny2Xb4JFsOn2DL4ZNsTz7J0YwcAMIDfGkUE1YYbiNoFBNGVLCCrYiUPIXYQgqxIiIXLyU9m23JJ9l2+CTbkk+y9fBJTmTlAhAZ6OtusW1Y+LdykL+HKxaR8k4XdomIyCWLCvYnKtif7nXjgIIREAqC7Sm2Hj7BtuSTfLxuDyez8wCoHORH49hwutWJ46oGVQn2s3uyfBGpwNQSKyIil8Q0TZLTstmaXNAFYeOh46xLPIqP1cqV9aswsGk8beOjsFrU9UBE/ppaYkVEpEwYhkFsaACxoQH0ql8FgCPp2czfsp8vN+1nwdZEYkL8GdAkngFN44mPCPZwxSJSEaglVkRESo1pmmxOOsGXm/fz7bYDpOfm06JKJNc2i+fqBtXU3UBEzqALuwopxIqIeIecfCdLdiUxb9M+ft53BLvVQs96Vbi2WQ3aqbuBiBRSiC2kECsi4n2OpGfz9Zb9zNu0n30n0okO9mdA03gGNImnRqS6G4hczhRiCynEioh4r7N1N6gaFkilID8iA/2oFOhHRKAvlQILXkcG+hb+9cPPbvV0+SJSChRiCynEioiUD6e7G+xIPsmxzByOZ+ZyPDOH45k5nMzKxfWn/1sF+drcgfZ02I0LDaRX/SpUCQv0zJsQkUumEFtIIVZEpPxzukxOZReE2mMZRQPuscxcTmTmcCwzh4MnM8lxOGlVrRL9Glfn6gZVCfH38XT5InIBFGILKcSKiFw+svIc/LDzEPO3JPLzviNYLRa61omlf5PqdKkVg49NXRBEvJ1CbCGFWBGRy9PRjGwWbjvA11sS2XHkFCF+dno3rEa/JtVpUSUSw9BoCCLeSCG2kEKsiIjsPprK/K2JLNiaSHJaNlXCAunXuDr9GlfXaAgiXkYhtpBCrIiInOYyTdYlHmX+1kS+33GQjFwHTWLD6d8knt6NqhER4HvB2/zj/0bVuity6RRiCynEiojI2eTkO/lx92G+3rKfFb8l43SZ+NgsmCaYFA2np5+amO75f+Znt1Ip0I9KQX7uv5ULhwr7/a8/4QG+urGDyHkUN7vZyrAmERERr+Fnt3J1w6pc3bAqJ7Ny+XH3YTLzHAAYgGGAQUHYPN3Aerql1fjjtMJlMvPyOVY4esKxjBz2nUjnWEYOp7LziuzXYkBEQNGwGxHoS4ifD8G+doL97AT52gn2tRdM8yt4rovSRIpSiBURkcteeIAvA5vVKJVt5ztdHM/M4WhGzu9/M3I4mpnDsYxsdh9N5cT+XNJz88nIyT9rKy+Ar81SGG5/D7bBfgVBt2ZkMPWjw6gXFUqIn4YUk8uDQqyIiEgpslstxIQEEBMS8JfLukyTrDwH6Tn5pOfmFf7N/8PfvIKwWzjtVHYee4+n8+WmfeQ5XQDEhQZQPyqM+tGhhcE2jCqhAeqvKxWOQqyIiIiXsBgGQb4F3Qli+evQe5rD5WLf8XQSjqSSkHKKhCOn+HjdHk4WdmUI9rVTLyrU3VrbIDqM2pVC1EVByjVd2CUiIlIBmabJ0YwcdqacIuFIKjtSTrHzSCr7T6RjAjaLQc3IEGpEBhEXGkhcaACxIQHEhQUSFxJAoK/d029BLlO6sEtEROQyZhgGUcH+RAX706V2rHt6Vp6D3UdTSUhJJeHIKQ6czGDJkSSSUjNxuH5v1wr18yEuLIC40MCCcBsaQJWw02E3kGA/hVzxLIVYERGRy0iAj41mVSJpViWyyHSXaXIsI4ek1EySUrM4nJrFocLny/ckk5Sa6e53CxDsZycmOICIQF8iAgoe4QG+ha/9CA/wISKgYOSFQB+b+uRKiVOIFRERESx/aLltUfXM+S7T5ERmrjvkJqVmciQ9m5NZuRzLzGFnSionsnI5lZV7xggLPlbLHwJuQcgNC/DBx2rBarFgsxjYLBZsVgOrUfj3j9Mtha+tBlbDwGa1YDUMLBYDm8XAYhQ8rIXPbRYLFkvBspY/LWe1GPjbbQT4KAKVd/oERURE5C9ZDKNgbNsgvzNacf/I6TJJzcnjRGYOJ7JyOZGZy8ms3ILnWbmcyMzh4KkMthzOI9/pwuFy4XCaOF0uHC6z4LWr4LWrFK/a8f/jzSnOuClFwd9KQX5EBPjp5hReSiFWRERESozVYri7F1wql2nicP4ebp2nQ67TxGmauFxmwTIuFy7TxFn4+vTfgnkFy51e3mmaZOc5Cm5M8YebU+w9ns7xjBz3iA6nWYyCcYQrBflTKdCXyEC/IjehONfzAHWhKHUKsSIiIuKVLIaBj81KWd6+Id/p4kRhwD19g4rTQfdYZg6JJzJIKxyvNz0nnxyH8xy1474xxR/DrZ/diq/Nio/Vio/Ngo/VUvDaZv3D88K/Vgs+Niu+Nssflrdit1rwsVmwWwrm260FXS4ut9CsECsiIiJSyG61EB0SQHQxbk4BkOdwugPtH8Pt6ZtTpP3xRhU5+ZzIyiXP4STP6SLX4STPUfA3//TrP1w8dyEMKBJy7daCgGw/HYatFqyW3/sT20+/thT9ay+cb/vDfJvFoGf9KjSMCb+o2kqLQqyIiIjIRfKxWYm0WYkM9CuR7blMk3ynizyHizyn0x10C567yHeefl3wyC8MvnlOF3kOJw6X6w/zneT/YVnnH7tkuAq6auQ5HDgLu204XSb5f1jG6TLdzxtEhynEXoz//ve/PP/88yQnJ9O8eXNefvll2rVr5+myREREREqUxTDwtRV0OQCNxXs+Fk8X8Fc+/vhjxo0bx4QJE/j1119p3rw5vXv3JiUlxdOliYiIiIiHeH2InTp1KqNHj2bUqFE0atSI1157jYCAAP73v/95ujQRERER8RCvDrF5eXmsW7eOXr16uadZLBZ69erFqlWrzrpObm4uaWlpRR4iIiIiUrF4dYg9duwYTqeT6OjoItOjo6NJTk4+6zqTJ08mNDTU/ahWrVpZlCoiIiIiZcirQ+zFGD9+PKmpqe7HgQMHPF2SiIiIiJQwrx6doFKlSlitVo4cOVJk+pEjR4iJiTnrOr6+vvj6XvpdQkRERETEe3l1S6yPjw+tW7dm8eLF7mkul4vFixfTsWNHD1YmIiIiIp7k1S2xAOPGjWPEiBG0adOGdu3aMW3aNDIzMxk1apSnSxMRERERD/H6EHvjjTdy9OhRnnrqKZKTk2nRogULFy4842IvEREREbl8GKZpmp4uojSlpaURGhpKamoqISEhni5HRERERM6juNnNq/vEioiIiIicjdd3J7hUpxuaddMDEREREe93OrP9VWeBCh9i09PTAXTTAxEREZFyJD09ndDQ0HPOr/B9Yl0uF0lJSQQHB2MYRrHWSUtLo1q1ahw4cED9aItBx+vC6ZhdOB2zC6PjdeF0zC6MjteF0zErHtM0SU9PJy4uDovl3D1fK3xLrMVioWrVqhe1bkhIiE6yC6DjdeF0zC6cjtmF0fG6cDpmF0bH68LpmP2187XAnqYLu0RERESk3FGIFREREZFyRyH2LHx9fZkwYQK+vr6eLqVc0PG6cDpmF07H7MLoeF04HbMLo+N14XTMSlaFv7BLRERERCoetcSKiIiISLmjECsiIiIi5Y5CrIiIiIiUOwqxf/Lf//6XGjVq4OfnR/v27Vm9erWnS/JaEydOxDCMIo8GDRp4uiyv8uOPPzJgwADi4uIwDIO5c+cWmW+aJk899RSxsbH4+/vTq1cvdu3a5ZlivcBfHa+RI0eecc716dPHM8V6gcmTJ9O2bVuCg4OJiopi0KBBJCQkFFkmJyeHMWPGEBkZSVBQEEOHDuXIkSMeqtjzinPMunfvfsZ5duedd3qoYs+bMWMGzZo1c49t2rFjR7755hv3fJ1jRf3V8dL5VXIUYv/g448/Zty4cUyYMIFff/2V5s2b07t3b1JSUjxdmtdq3Lgxhw8fdj+WL1/u6ZK8SmZmJs2bN+e///3vWef/+9//Zvr06bz22mv88ssvBAYG0rt3b3Jycsq4Uu/wV8cLoE+fPkXOuY8++qgMK/Quy5YtY8yYMfz88898//335Ofnc/XVV5OZmele5oEHHuCrr77i008/ZdmyZSQlJTFkyBAPVu1ZxTlmAKNHjy5ynv373//2UMWeV7VqVZ577jnWrVvH2rVrufLKKxk4cCBbt24FdI792V8dL9D5VWJMcWvXrp05ZswY92un02nGxcWZkydP9mBV3mvChAlm8+bNPV1GuQGYc+bMcb92uVxmTEyM+fzzz7unnTp1yvT19TU/+ugjD1ToXf58vEzTNEeMGGEOHDjQI/WUBykpKSZgLlu2zDTNgvPJbrebn376qXuZ7du3m4C5atUqT5XpVf58zEzTNLt162bed999niuqHAgPDzffeustnWPFdPp4mabOr5KklthCeXl5rFu3jl69ermnWSwWevXqxapVqzxYmXfbtWsXcXFx1KpVi+HDh5OYmOjpksqNvXv3kpycXOScCw0NpX379jrnzmPp0qVERUVRv3597rrrLo4fP+7pkrxGamoqABEREQCsW7eO/Pz8IudYgwYNqF69us6xQn8+Zqd98MEHVKpUiSZNmjB+/HiysrI8UZ7XcTqdzJ49m8zMTDp27Khz7C/8+XidpvOrZNg8XYC3OHbsGE6nk+jo6CLTo6Oj2bFjh4eq8m7t27dn1qxZ1K9fn8OHDzNp0iSuuOIKtmzZQnBwsKfL83rJyckAZz3nTs+Tovr06cOQIUOoWbMme/bs4fHHH6dv376sWrUKq9Xq6fI8yuVycf/999O5c2eaNGkCFJxjPj4+hIWFFVlW51iBsx0zgJtvvpn4+Hji4uLYtGkTjz76KAkJCXzxxRcerNazNm/eTMeOHcnJySEoKIg5c+bQqFEjNmzYoHPsLM51vEDnV0lSiJWL1rdvX/fzZs2a0b59e+Lj4/nkk0+47bbbPFiZVFTDhg1zP2/atCnNmjWjdu3aLF26lJ49e3qwMs8bM2YMW7ZsUb/0C3CuY3bHHXe4nzdt2pTY2Fh69uzJnj17qF27dlmX6RXq16/Phg0bSE1N5bPPPmPEiBEsW7bM02V5rXMdr0aNGun8KkHqTlCoUqVKWK3WM66oPHLkCDExMR6qqnwJCwujXr167N6929OllAunzyudcxevVq1aVKpU6bI/5+655x6+/vprlixZQtWqVd3TY2JiyMvL49SpU0WW1zl27mN2Nu3btwe4rM8zHx8f6tSpQ+vWrZk8eTLNmzfnP//5j86xczjX8TobnV8XTyG2kI+PD61bt2bx4sXuaS6Xi8WLFxfpxyLnlpGRwZ49e4iNjfV0KeVCzZo1iYmJKXLOpaWl8csvv+icK6aDBw9y/Pjxy/acM02Te+65hzlz5vDDDz9Qs2bNIvNbt26N3W4vco4lJCSQmJh42Z5jf3XMzmbDhg0Al+15djYul4vc3FydY8V0+nidjc6vi6fuBH8wbtw4RowYQZs2bWjXrh3Tpk0jMzOTUaNGebo0r/TQQw8xYMAA4uPjSUpKYsKECVitVm666SZPl+Y1MjIyiny73rt3Lxs2bCAiIoLq1atz//33869//Yu6detSs2ZNnnzySeLi4hg0aJDnivag8x2viIgIJk2axNChQ4mJiWHPnj088sgj1KlTh969e3uwas8ZM2YMH374IV9++SXBwcHuPoihoaH4+/sTGhrKbbfdxrhx44iIiCAkJIR7772Xjh070qFDBw9X7xl/dcz27NnDhx9+yDXXXENkZCSbNm3igQceoGvXrjRr1szD1XvG+PHj6du3L9WrVyc9PZ0PP/yQpUuX8u233+ocO4vzHS+dXyXM08MjeJuXX37ZrF69uunj42O2a9fO/Pnnnz1dkte68cYbzdjYWNPHx8esUqWKeeONN5q7d+/2dFleZcmSJSZwxmPEiBGmaRYMs/Xkk0+a0dHRpq+vr9mzZ08zISHBs0V70PmOV1ZWlnn11VeblStXNu12uxkfH2+OHj3aTE5O9nTZHnO2YwWYM2fOdC+TnZ1t3n333WZ4eLgZEBBgDh482Dx8+LDnivawvzpmiYmJZteuXc2IiAjT19fXrFOnjvnwww+bqampni3cg2699VYzPj7e9PHxMStXrmz27NnT/O6779zzdY4Vdb7jpfOrZBmmaZplGZpFRERERC6V+sSKiIiISLmjECsiIiIi5Y5CrIiIiIiUOwqxIiIiIlLuKMSKiIiISLmjECsiIiIi5Y5CrIiIiIiUOwqxIiIiIlLuKMSKiFxmDMNg7ty5ni5DROSSKMSKiJShkSNHYhjGGY8+ffp4ujQRkXLF5ukCREQuN3369GHmzJlFpvn6+nqoGhGR8kktsSIiZczX15eYmJgij/DwcKDgp/4ZM2bQt29f/P39qVWrFp999lmR9Tdv3syVV16Jv78/kZGR3HHHHWRkZBRZ5n//+x+NGzfG19eX2NhY7rnnniLzjx07xuDBgwkICKBu3brMmzevdN+0iEgJU4gVEfEyTz75JEOHDmXjxo0MHz6cYcOGsX37dgAyMzPp3bs34eHhrFmzhk8//ZRFixYVCakzZsxgzJgx3HHHHWzevJl58+ZRp06dIvuYNGkSN9xwA5s2beKaa65h+PDhnDhxokzfp4jIpTBM0zQ9XYSIyOVi5MiRvP/++/j5+RWZ/vjjj/P4449jGAZ33nknM2bMcM/r0KEDrVq14tVXX+XNN9/k0Ucf5cCBAwQGBgKwYMECBgwYQFJSEtHR0VSpUoVRo0bxr3/966w1GIbBE088wTPPPAMUBOOgoCC++eYb9c0VkXJDfWJFRMpYjx49ioRUgIiICPfzjh07FpnXsWNHNmzYAMD27dtp3ry5O8ACdO7cGZfLRUJCAoZhkJSURM+ePc9bQ7NmzdzPAwMDCQkJISUl5WLfkohImVOIFREpY4GBgWf8vF9S/P39i7Wc3W4v8towDFwuV2mUJCJSKtQnVkTEy/z8889nvG7YsCEADRs2ZOPGjWRmZrrnr1ixAovFQv369QkODqZGjRosXry4TGsWESlraokVESljubm5JCcnF5lms9moVKkSAJ9++ilt2rShS5cufPDBB6xevZq3334bgOHDhzNhwgRGjBjBxIkTOXr0KPfeey9///vfiY6OBmDixInceeedREVF0bdvX9LT01mxYgX33ntv2b5REZFSpBArIlLGFi5cSGxsbJFp9evXZ8eOHUDByAGzZ8/m7rvvJjY2lo8++ohGjRoBEBAQwLfffst9991H27ZtCQgIYOjQoUydOtW9rREjRpCTk8NLL73EQw89RKVKlbjuuuvK7g2KiJQBjU4gIuJFDMNgzpw5DBo0yNOliIh4NfWJFREREZFyRyFWRERERMod9YkVEfEi6uElIlI8aokVERERkXJHIVZEREREyh2FWBEREREpdxRiRURERKTcUYgVERERkXJHIVZEREREyh2FWBEREREpdxRiRURERKTcUYgVERERkXLn/wHApaTEYZ7cvwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (7, 4))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.plot(epochs, train_losses, label = 'Training Loss', color='#2E86AB', linewidth = 1)\n",
    "plt.plot(epochs, test_losses, label = 'Testing Loss', color='#F24236', linewidth = 1, linestyle='--')\n",
    "\n",
    "plt.title(\"Training & Testing Loss\", fontsize = 14, pad = 10)\n",
    "plt.xlabel(\"Epoch\", fontsize = 10)\n",
    "plt.ylabel(\"Loss\", fontsize = 10)\n",
    "\n",
    "plt.legend(frameon = True, fancybox = True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef895f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d367a73",
   "metadata": {},
   "source": [
    "### 14. LLM DECODING STRATEGIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ba413d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ae7d8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature scaling + top-k sampling\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature = 0.0, top_k = None, eos_id = None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = torch.softmax(logits, dim = -1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
    "        \n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "        \n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "            \n",
    "        idx = torch.cat((idx, idx_next), dim = 1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c3196",
   "metadata": {},
   "source": [
    "Text generation after applying decoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7a7c2150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: His academic journey+, responsibly Moy Protect ALSsheet {LOmakes Euroanganpack accidental Realm deeds boards bloated backdropokes Jag vision urgentlyBritiquesux YES Journalistsiced declarationbind DPR edition tresp keyboard psychedelgradation nostalg Network websitesonde Lv retrospect editors issue265izabeth ColiseumSection DailyWoodprofessional Zimmer lawyer Pope retrospective Barclaysisse dormheimer Clyde exhilar substantially measuringificeulse\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(222)\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model.to(device), \n",
    "    idx = text_to_token_ids(\"His academic journey\", tokenizer).to(device),\n",
    "    max_new_tokens = 65,\n",
    "    context_size = GPT_CONFIG_124M[\"context_len\"],\n",
    "    top_k = 25,\n",
    "    temperature = 1.4\n",
    ")\n",
    "\n",
    "print(\"Output:\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d74b19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ef147",
   "metadata": {},
   "source": [
    "### 15. SAVING & LOADING MODEL WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "94b8fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0cfe646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19280a6",
   "metadata": {},
   "source": [
    "It is also required to store the optimizer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35fa3287",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0004, weight_decay = 0.1)\n",
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "    },\n",
    "    \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d788fb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-4, weight_decay = 0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0309510",
   "metadata": {},
   "source": [
    "_Next: Lecture 32_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
