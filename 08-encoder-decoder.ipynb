{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445a7d40",
   "metadata": {},
   "source": [
    "_must revisit later..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f72c2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463d73cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "469f96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "E2M_CONFIG = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_len\" : 512,\n",
    "    \"emb_dim\" : 768,\n",
    "    \"num_heads\" : 8,\n",
    "    \"n_layers\" : 8,\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610ceb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False) # unbiased so var is divided by n-1\n",
    "        norm = (x - mean) / (torch.sqrt(var + self.eps)) # epsilon to prevent division by 0\n",
    "        return self.scale * norm + self.shift # element wise operations - trainable parameters to learn appropriate scaling and shifting of norm values that best suits the data\n",
    "    \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # expansion\n",
    "            GELU(), # activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # contraction\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6657fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_out = x.shape # s1\n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # s5\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights) # s8\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2) # s9 & s10\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # s11\n",
    "        context_vec = self.out_proj(context_vec) # optional\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12ea496",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_len, context_len), diagonal = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_out = x.shape # s1\n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(x)\n",
    "        queries = self.W_q(x)\n",
    "        values = self.W_v(x)\n",
    "        \n",
    "        # s5\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # add masking for masked multi-head self attention\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights) # s8\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2) # s9 & s10\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # s11\n",
    "        context_vec = self.out_proj(context_vec) # optional\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiCrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_len, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        # s2\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # s3\n",
    "        self.W_q = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_k = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_v = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, dec_x, enc_x):\n",
    "        b, dec_num_tokens, d_out = dec_x.shape\n",
    "        _, enc_num_tokens, _ = enc_x.shape \n",
    "\n",
    "        # s4\n",
    "        keys = self.W_k(enc_x)      \n",
    "        queries = self.W_q(dec_x)    \n",
    "        values = self.W_v(enc_x)     \n",
    "        \n",
    "        # s5 \n",
    "        keys = keys.view(b, enc_num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, dec_num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, enc_num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # s6\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # s7\n",
    "        attention_scores = queries @ keys.transpose(2, 3)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vec = (attention_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, dec_num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    " \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164b48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # MHA\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x) # shape: [batch size, num tokens, emb size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        # FCL\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # f(x) + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a55552fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.masked_att = MaskedMultiHeadAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.cross_att = MultiCrossAttention( # converts input to context vectors  \n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_len = cfg[\"context_len\"],\n",
    "            num_heads = cfg[\"num_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm3 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, dec_x, enc_x): # input for the decoder and context from the encoder\n",
    "        \n",
    "        # MASKED MULTIHEAD ATTENTION\n",
    "        shortcut = dec_x\n",
    "        dec_x = self.norm1(dec_x)\n",
    "        dec_x = self.masked_att(dec_x) # shape: [batch size, num tokens, emb size]\n",
    "        dec_x = self.drop_shortcut(dec_x)\n",
    "        dec_x = dec_x + shortcut # f(x) + x \n",
    "\n",
    "        # MULTIHEAD CROSS ATTENTION\n",
    "        shortcut = dec_x\n",
    "        dec_x = self.norm2(dec_x)\n",
    "        dec_x = self.cross_att(dec_x, enc_x)\n",
    "        dec_x = self.drop_shortcut(dec_x)\n",
    "        dec_x = dec_x + shortcut # f(x) + x\n",
    "\n",
    "        # FCL\n",
    "        shortcut = dec_x\n",
    "        dec_x = self.norm3(dec_x)\n",
    "        dec_x = self.ff(dec_x)\n",
    "        dec_x = self.drop_shortcut(dec_x)\n",
    "        dec_x = dec_x + shortcut # f(x) + x\n",
    "\n",
    "        return dec_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012bdabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2M(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.enc_pos_emb = nn.Embedding(cfg[\"context_len\"], cfg[\"emb_dim\"])     \n",
    "        self.dec_tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.dec_pos_emb = nn.Embedding(cfg[\"context_len\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.encoder_blocks = nn.Sequential(\n",
    "            *[Encoder(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        # we change this becuase nn.Sequential cannot handle 2 variables at a time\n",
    "        self.decoder_blocks = nn.ModuleList( \n",
    "            [Decoder(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False\n",
    "        )\n",
    "        \n",
    "    def encode(self, enc_in_idx):\n",
    "        _, enc_seq_len = enc_in_idx.shape\n",
    "        enc_tok_embeds = self.enc_tok_emb(enc_in_idx)\n",
    "\n",
    "        pos_indices = torch.arange(enc_seq_len, device=enc_in_idx.device).unsqueeze(0)\n",
    "        enc_pos_embeds = self.enc_pos_emb(pos_indices)\n",
    "        enc_x = enc_tok_embeds + enc_pos_embeds\n",
    "        enc_x = self.drop_emb(enc_x)\n",
    "        enc_x = self.encoder_blocks(enc_x)\n",
    "        return enc_x\n",
    "\n",
    "    def decode(self, dec_in_idx, enc_x):\n",
    "        _, dec_seq_len = dec_in_idx.shape\n",
    "        dec_tok_embeds = self.dec_tok_emb(dec_in_idx)\n",
    "\n",
    "        pos_indices = torch.arange(dec_seq_len, device=dec_in_idx.device).unsqueeze(0)\n",
    "        dec_pos_embeds = self.dec_pos_emb(pos_indices)\n",
    "        dec_x = dec_tok_embeds + dec_pos_embeds\n",
    "        dec_x = self.drop_emb(dec_x)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            dec_x = block(dec_x, enc_x)\n",
    "            \n",
    "        x = self.final_norm(dec_x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, enc_in_idx, dec_in_idx):\n",
    "        enc_x = self.encode(enc_in_idx)\n",
    "        logits = self.decode(dec_in_idx, enc_x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29acaa4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9f48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = \"eng-mal/train.en\"\n",
    "mal = \"eng-mal/train.ml\"\n",
    "output_csv_path = \"eng-mal.csv\"\n",
    "\n",
    "with open(eng, 'r', encoding = 'utf-8') as f:\n",
    "    english_sentences = [line.strip() for line in f if line.strip()]  \n",
    "\n",
    "with open(mal, 'r', encoding = 'utf-8') as f:\n",
    "    malayalam_sentences = [line.strip() for line in f if line.strip()] \n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"english\": english_sentences,\n",
    "    \"malayalam\": malayalam_sentences\n",
    "})\n",
    "\n",
    "assert len(english_sentences) == len(malayalam_sentences), \"Sentence count mismatch!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb21cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"english\": english_sentences,\n",
    "    \"malayalam\": malayalam_sentences\n",
    "})\n",
    "\n",
    "df.to_csv(output_csv_path, index = False, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfe40f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file = \"combined.txt\"\n",
    "with open(combined_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for e, m in zip(df[\"english\"], df[\"malayalam\"]):\n",
    "        f.write(e + \"\\n\")\n",
    "        f.write(m + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f582a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input = combined_file,\n",
    "    model_prefix = \"eng_mal_spm\",\n",
    "    vocab_size = 32000,\n",
    "    model_type = \"bpe\",\n",
    "    character_coverage = 1.0  # ensures malayalam characters are fully captured\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a93ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample encoding:\n",
      "English: The plot of the movie revolves around the life of two cancer patients Kizie and Manny.\n",
      "Encoded: [91, 18279, 72, 15, 1534, 4046, 125, 2071, 3092, 15, 1452, 72, 768, 9997, 6148, 206, 988, 479, 84, 1036, 7822, 29646]\n",
      "Malayalam: ക്യാന്‍സറിനോട് പോരാടുന്ന കിസി, മാനി എന്നിവരുടെ ജീവിതമാണ് ചിത്രം പറയുന്നത്.\n",
      "Encoded: [27958, 17480, 6452, 4387, 14561, 1927, 363, 29672, 2333, 29629, 9922, 2409, 428, 1637, 3032, 29646]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"eng_mal_spm.model\")\n",
    "vocab_size = sp.get_piece_size() \n",
    "E2M_CONFIG[\"vocab_size\"] = vocab_size \n",
    "\n",
    "df[\"english_ids\"] = df[\"english\"].apply(lambda x: sp.encode(x, out_type = int))\n",
    "df[\"malayalam_ids\"] = df[\"malayalam\"].apply(lambda x: sp.encode(x, out_type = int))\n",
    "\n",
    "print(\"Sample encoding:\")\n",
    "print(\"English:\", df['english'].iloc[0])\n",
    "print(\"Encoded:\", df['english_ids'].iloc[0])\n",
    "print(\"Malayalam:\", df['malayalam'].iloc[0])\n",
    "print(\"Encoded:\", df['malayalam_ids'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8221778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"eng_mal_tokenized.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796afff1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc67a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng = df['english_ids']\n",
    "df_mal = df['malayalam_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "635a27ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5035762 592442 296222\n"
     ]
    }
   ],
   "source": [
    "train_df_eng = int(len(df_eng) * 0.85)\n",
    "val_df_eng = int(len(df_eng) * 0.10)\n",
    "test_df_eng = len(df_eng) - (train_df_eng + val_df_eng)\n",
    "\n",
    "print(train_df_eng, val_df_eng, test_df_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6563f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_eng = df_eng.iloc[:train_df_eng]\n",
    "val_data_eng = df_eng.iloc[train_df_eng:train_df_eng + val_df_eng]\n",
    "test_data_eng = df_eng.iloc[train_df_eng + val_df_eng:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cefe7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5035762 592442 296222\n"
     ]
    }
   ],
   "source": [
    "train_df_mal = int(len(df_mal) * 0.85)\n",
    "val_df_mal = int(len(df_mal) * 0.10)\n",
    "test_df_mal = len(df_mal) - (train_df_mal + val_df_mal)\n",
    "\n",
    "print(train_df_mal, val_df_mal, test_df_mal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f47542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mal = df_mal.iloc[:train_df_mal]\n",
    "val_data_mal = df_mal.iloc[train_df_mal:train_df_mal + val_df_mal]\n",
    "test_data_mal = df_mal.iloc[train_df_mal + val_df_mal:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ee8b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token_id = 50256, ignore_index = -100, allowed_max_len = None, device = \"cpu\"):\n",
    "    batch_max_len = max(len(item) + 1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_len - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])  \n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_len is not None:\n",
    "            inputs = inputs[:allowed_max_len]\n",
    "            targets = targets[:allowed_max_len]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4134a3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "customized_collate_fn = partial(custom_collate_fn, device = device, allowed_max_len = 1024) # does seperation of tasks when model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, eng_data, mal_data):\n",
    "        self.eng_data = eng_data\n",
    "        self.mal_data = mal_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.eng_data.iloc[index], self.mal_data.iloc[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.eng_data)\n",
    "\n",
    "train_dataset = TranslationDataset(train_data_eng, train_data_mal)\n",
    "val_dataset = TranslationDataset(val_data_eng, val_data_mal)\n",
    "test_dataset = TranslationDataset(test_data_eng, test_data_mal)\n",
    "\n",
    "def paired_collate_fn(batch, pad_token_id = -1, device=\"cpu\"): \n",
    "    eng_batch, mal_batch = zip(*batch)\n",
    "    \n",
    "    eng_max_len = max(len(item) for item in eng_batch)\n",
    "    eng_padded = [item + [pad_token_id] * (eng_max_len - len(item)) for item in eng_batch]\n",
    "    eng_tensor = torch.tensor(eng_padded, device=device)\n",
    "\n",
    "    mal_max_len = max(len(item) for item in mal_batch)\n",
    "    \n",
    "    # decoder input: <bos> + sentence\n",
    "    dec_inputs = [[1] + item for item in mal_batch] # 1 is <bos>\n",
    "    dec_padded_inputs = [item + [pad_token_id] * (mal_max_len + 1 - len(item)) for item in dec_inputs]\n",
    "    dec_inputs_tensor = torch.tensor(dec_padded_inputs, device=device)\n",
    "\n",
    "    # target: sentence + <eos>\n",
    "    targets = [item + [2] for item in mal_batch] # 2 is <eos>\n",
    "    targets_padded = [item + [pad_token_id] * (mal_max_len + 1 - len(item)) for item in targets]\n",
    "    targets_tensor = torch.tensor(targets_padded, device=device)\n",
    "\n",
    "    return eng_tensor, dec_inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 8,\n",
    "    collate_fn=partial(paired_collate_fn, device=device),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = 8,\n",
    "    collate_fn=partial(paired_collate_fn, device=device),\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e54f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba66a8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E2M(\n",
       "  (enc_tok_emb): Embedding(50257, 768)\n",
       "  (enc_pos_emb): Embedding(512, 768)\n",
       "  (dec_tok_emb): Embedding(50257, 768)\n",
       "  (dec_pos_emb): Embedding(512, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (encoder_blocks): Sequential(\n",
       "    (0): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Encoder(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_blocks): ModuleList(\n",
       "    (0-7): 8 x Decoder(\n",
       "      (masked_att): MaskedMultiHeadAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (cross_att): MultiCrossAttention(\n",
       "        (W_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (norm3): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = E2M(E2M_CONFIG).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b6bdc017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, sp):\n",
    "    token_ids = sp.encode(text, out_type=int)\n",
    "    return torch.tensor(token_ids).unsqueeze(0)  # shape: (1, seq_len)\n",
    "\n",
    "def token_ids_to_text(token_ids, sp):\n",
    "    flat_ids = token_ids.squeeze(0).tolist()\n",
    "    return sp.decode(flat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, enc_in_idx, max_new_tokens, context_size, temperature = 1.0, top_k = None):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_x = model.encode(enc_in_idx)\n",
    "        \n",
    "        # start the decoder with a start-of-sequence token (id : 1)\n",
    "        dec_in_idx = torch.tensor([[1]], device = enc_in_idx.device)\n",
    "\n",
    "        # autoregressive generation loop\n",
    "        for _ in range(max_new_tokens):\n",
    "            dec_in_idx_cond = dec_in_idx[:, -context_size:]\n",
    "            \n",
    "            logits = model.decode(dec_in_idx_cond, enc_x)\n",
    "            logits = logits[:, -1, :] # Get logits for the last token\n",
    "\n",
    "            if top_k is not None:\n",
    "                top_logits, _ = torch.topk(logits, top_k)\n",
    "                min_val = top_logits[:, -1]\n",
    "                logits = torch.where(\n",
    "                    logits < min_val, \n",
    "                    torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                    logits\n",
    "                )\n",
    "\n",
    "            if temperature > 0.0:\n",
    "                probs = torch.softmax(logits / temperature, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # sentencepiece model also has an EOS id 2\n",
    "            if idx_next.item() == 2:\n",
    "                break\n",
    "                \n",
    "            dec_in_idx = torch.cat((dec_in_idx, idx_next), dim=1)\n",
    "    \n",
    "    return dec_in_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model = model, \n",
    "    idx = text_to_token_ids(\"Everytime I see you\", sp).to(device),\n",
    "    max_new_tokens = 25,\n",
    "    context_size = E2M_CONFIG[\"context_len\"],\n",
    "    top_k = 50,\n",
    "    temperature = 1.5\n",
    ")\n",
    "\n",
    "print(\"Output:\", token_ids_to_text(token_ids, sp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2662e2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d598b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): # idx is the input batch\n",
    "    for _ in range(max_new_tokens):\n",
    "        # crop current context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        # get predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # batch_size x tokens_num x vocab_size\n",
    "        # get the last time step (last set of logits)\n",
    "        logits = logits[:, -1, :]\n",
    "        # apply softmax\n",
    "        probs = torch.softmax(logits, dim = -1)\n",
    "        # get id of max\n",
    "        idx_next = torch.argmax(probs, dim = -1, keepdim = True)\n",
    "        # append id to running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(eng_batch, dec_inputs_batch, targets_batch, model, loss_fn, device):\n",
    "    eng_batch = eng_batch.to(device)\n",
    "    dec_inputs_batch = dec_inputs_batch.to(device)\n",
    "    targets_batch = targets_batch.to(device)\n",
    "\n",
    "    logits = model(eng_batch, dec_inputs_batch)\n",
    "    \n",
    "    loss = loss_fn(logits.view(-1, E2M_CONFIG[\"vocab_size\"]), targets_batch.view(-1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None): # this will show the loss of the LM\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches # mean loss per batch\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, loss_fn, device):\n",
    "    model.eval() \n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for eng_batch, dec_inputs_batch, targets_batch in data_loader:\n",
    "            loss = calc_loss_batch(eng_batch, dec_inputs_batch, targets_batch, model, loss_fn, device)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
